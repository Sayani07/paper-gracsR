---
title: "Clustering four designs 500 iter"
output:
  bookdown::pdf_book:
    MonashEBSTemplates::workingpaper:
    #base_format: rticles::asa_article
    fig_height: 8
    fig_width: 12
    fig_caption: yes
    dev: "pdf"
    keep_tex: yes
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(knitr)
read_chunk("R/simdata01.R")
```

# How the time series plot of the designs look like for different sample sizes?

```{r sim-val, echo = TRUE}
set.seed(9999)
nx_val = 2 # number of x-axis levels
nfacet_val = 3 # number of facet levels
w1_val = 3 # increment in mean
w2_val = 0 # increment in sd
mean_val = 0 # mean of normal distribution of starting combination
sd_val = 1 # sd of normal distribution of starting combination
quantile_prob_val = seq(0.1, 0.9, 0.1)
```

```{r load-lib}
```


```{r designs}
```

## nobs per combination is 4

```{r, echo = TRUE}
ntimes_val = 4
```

```{r data}
```

```{r category-plots}
```


```{r}
change_index <- function(data){

index_new <- map(seq_len(ntimes_val), function(i){
  map((seq_len(nx_val*nfacet_val)), function(j)
    {
value = i + (j-1)*ntimes_val
})
  }) %>% unlist()

data_new = data %>%
  ungroup() %>%
  mutate(index_old = row_number(),
         index_new = index_new)

y = data_new[match(index_new, data_new$index_old),]

y <- y %>%
  mutate(time = row_number())

return(y)
}

endbreaks<- nrow(sim_panel_null)

p1 <- change_index(sim_panel_null) %>%
  ggplot(aes(x = time,
             y = sim_data)) +
  geom_line() +
  scale_x_continuous(breaks = seq(1, endbreaks, nfacet_val*nx_val))+
  theme_bw() +
  geom_point(alpha = 0.5, color = "blue") +
  theme(panel.grid.major.x =  element_line(colour = "#A9A9A9"),
        panel.grid.minor.x =  element_blank())+
  ylab("simulated response")+
  xlab("index")


p2 <- change_index(sim_panel_varf) %>%
  ggplot(aes(x = time,
             y = sim_data)) +
  geom_line() +
  scale_x_continuous(breaks = seq(1, endbreaks, nfacet_val*nx_val))+
  theme_bw() +
  geom_point(alpha = 0.5, color = "blue") +
  theme(panel.grid.major.x =  element_line(colour = "#A9A9A9"),
        panel.grid.minor.x =  element_blank())+
  ylab("simulated response")+
  xlab("index")

p3 <- change_index(sim_panel_varx) %>%
  ggplot(aes(x = time,
             y = sim_data)) +
  geom_line() +
  scale_x_continuous(breaks = seq(1, endbreaks, nfacet_val*nx_val))+
  theme_bw() +
  geom_point(alpha = 0.5, color = "blue")+
  theme(panel.grid.major.x =  element_line(colour = "#A9A9A9"),
        panel.grid.minor.x =  element_blank())+
  ylab("simulated response")+
  xlab("index")


p4 <- change_index(sim_panel_varall) %>%
  ggplot(aes(x = time,
             y = sim_data)) +
  geom_line() +
  scale_x_continuous(breaks = seq(1, endbreaks, nfacet_val*nx_val))+
  theme_bw() +
  geom_point(alpha = 0.5, color = "blue") +
  theme(panel.grid.major.x =  element_line(colour = "#A9A9A9"),
  panel.grid.minor.x =  element_blank())+
  ylab("simulated response")+
  xlab("index")


t1 <- sim_panel_varall %>% ungroup() %>%  select (-c(1, 2)) 

# %>% 
#   kable(format = "latex", caption = "Simulated data for categories format") 

t2 <- change_index(sim_panel_varall) %>% select (-c(1, 2)) 

# %>% 
#   kable(format = "latex",
#         caption = "Simulated data for time series format") 

kable(list(t1, t2), format = "latex", caption = "Simulated data across combination of categories (left) and manipulated time series format (right)") %>% 
  row_spec(1:24,  background =  "#d3d3d3")

# kable(t1) %>%
#   kable_styling(full_width = FALSE, position = "float_left")
# kable(t2) %>%
#   kable_styling(full_width = FALSE, position = "left")
```


```{r plot4}
(p1 + p_null)/( p2 + p_varf)/(p3 + p_varx)/(p4 + p_varall)
```

## nobs per combination is 10

```{r nobs10, echo = TRUE}
ntimes_val = 10 # nobs per combination 
```


```{r data}
```

```{r category-plots}
```

```{r line-graph}
```

```{r plot10}
(p1 + p_null)/( p2 + p_varf)/(p3 + p_varx)/(p4 + p_varall)
```


## nobs per combination is 50

```{r nobs50, echo = TRUE}
ntimes_val = 50 # nobs per combination 
```


```{r data}
```

```{r category-plots}
```

```{r line-graph}
```

```{r plot50}
(p1 + p_null)/( p2 + p_varf)/(p3 + p_varx)/(p4 + p_varall)

```

# Clustering on simulated datasets

```{r, echo=TRUE}
sample_seed <-  seq(1, 500, 1)
```

_DGP: Generate 10 time series from each designs_ 
Time series are simulated from each of these designs with $50$ observations in each group. So we have $50$ observations each for the six combination of categories $(1,1), (1, 2), (2, 1), (2, 2), (3, 1), (3, 2)$. Time series are simulated for ten different seeds (from `sample_seed`) for each design. `data_varall-1` represents a data set from a design $D_{var_{all}}$ (distributions change across both facet and x) with a seed $10$. `data_varall-2` represents a data set from a design $D_{var_{all}}$ with a seed $20$ and so on.
`data_null`, `data_varx` and `data_varf `corresponds to designs $D_{null}$, $D_{var_{x}}$ and $D_{var_{f}}$ designs respectively.   

_Compute quantiles of conditional distributions_
Conditional quantiles are obtained for each combination of categories.  

_JS Pairwise distances between datasets_
Distance between the data sets is computed as the sum of JS distances across different categories.  

_Hierarchical clustering with 4 clusters_
Hierarchical clustering is performed using k = 4 and dendogram observed

_Clusters obtained visualized using MDS_
Each cluster represents data sets from a separate design



```{r}
change_index_data <- function(ntimes_val = NULL,
                         nx_val = NULL,
                         nfacet_val = NULL,
                         sim_function = sim_varx_normal){

  data <- sim_panel(
    nx = nx_val, nfacet =  nfacet_val,
    ntimes = ntimes_val,
    # sim_dist = sim_varx_normal(2, 3, 5, 10, 5, -1.5)
    sim_dist = sim_function(nx_val, nfacet_val, mean_val, sd_val, w1_val, w2_val)
  ) %>% unnest(data)


  index_new <- map(seq_len(ntimes_val), function(i){
    map((seq_len(nx_val*nfacet_val)), function(j)
    {
      value = i + (j-1)*ntimes_val
    })
  }) %>% unlist()

  data_new = data %>%
    ungroup() %>%
    mutate(index_old = row_number(),
           index_new = index_new)

  y = data_new[match(index_new, data_new$index_old),]

  y <- y %>%
    mutate(time = row_number()) %>%
    select(-c(index_old, index_new))

  return(y)
}
```


```{r data-make}
data_null <- map(sample_seed, function(seed){
  set.seed(sample_seed[seed])
  change_index_data(5, 2, 3, sim_null_normal) %>%
    compute_quantiles(quantile_prob_val)
}) %>%
  bind_rows(.id = "seed_id") %>%
  mutate(data_type = "data_null")


data_varf <- map(sample_seed, function(seed){
  set.seed(sample_seed[seed])
  change_index_data(5, 2, 3, sim_varf_normal)%>%
    compute_quantiles(quantile_prob_val)
}) %>%
  bind_rows(.id = "seed_id") %>%
  mutate(data_type = "data_varf")

data_varx <- map(sample_seed, function(seed){
  set.seed(sample_seed[seed])
  change_index_data(5, 2, 3, sim_varx_normal)%>%
    compute_quantiles(quantile_prob_val)
}) %>%
  bind_rows(.id = "seed_id") %>%
  mutate(data_type = "data_varx")

data_varall <- map(sample_seed, function(seed){
  set.seed(sample_seed[seed])
  change_index_data(5, 2, 3, sim_varall_normal)%>%
    compute_quantiles(quantile_prob_val)
}) %>%
  bind_rows(.id = "seed_id") %>%
  mutate(data_type = "data_varall")

data_q <- bind_rows(data_null,
                      data_varf,
                      data_varx,
                      data_varall) %>%
  mutate(seed_id = sprintf("%02d", 
                              as.numeric(seed_id))) %>% 
  mutate(unique_data = paste(data_type, seed_id, sep = "-"))

data_q$data_type <- factor(data_q$data_type,
                                    levels = c("data_null" , "data_varf",
                                               "data_varx","data_varall"))

```



```{r JS}

```

```{r JS-mydata}
data_q_wide <- data_q %>%
  select(id_facet, id_x, unique_data, sim_data_quantile) %>%
  pivot_wider(names_from = unique_data,
              values_from = sim_data_quantile) %>%
  select(-c(1, 2))

ndata <- data_q %>% distinct(unique_data) %>% mutate(index = row_number())
ldata <- nrow(ndata)
lcomb <-  nx_val*nfacet_val

dist_data <- map(1:ldata, function(x){ # first data
  map(1:ldata, function(y){ # 2nd data
    map(1:lcomb, function(z){ # number of combinations nx*nfacet
      JS(
        prob = quantile_prob_val,
        unlist(data_q_wide[z,x]),
        unlist(data_q_wide[z,y])
      ) %>% as_tibble()
    })%>% bind_rows(.id = "combinations")
  })%>% bind_rows(.id = "data_type1")
}) %>% bind_rows(.id = "data_type2") 



dist_mat <- dist_data %>%
  group_by(data_type1, data_type2) %>%
  summarise(dist = sum(value)) %>%
  pivot_wider(names_from = data_type2,
              values_from = dist) %>%
  mutate(data_type1 = as.numeric(data_type1)) %>%
  left_join(ndata, by = c("data_type1" = "index"))


dist_mat_format <- dist_mat %>%
  ungroup() %>%
  select(-data_type1, -unique_data)


rownames(dist_mat_format) <- dist_mat$unique_data
```



```{r hier-clust}
d = stats::as.dist(dist_mat_format)
hc = stats::hclust(d,method="complete")
plot(hc)
```

```{r}
mds <- d %>%
  cmdscale() %>%
  as_tibble()

colnames(mds) <- c("Dim.1", "Dim.2")
rownames(mds) <- dist_mat$unique_data

groups<-cutree(hc, k=4)

all_data_cluster <- cbind(dist_mat_format, groups) %>%
  cbind(mds)%>%
  mutate(groups = as.factor(groups)) %>% as_tibble()


mds_plot_10 <- ggplot(all_data_cluster,
       aes(x = Dim.1,
           y = Dim.2,
       color = groups,
       label = rownames(mds))) +
  geom_point(size = 0.5) +
  geom_text(check_overlap = TRUE)  +
  theme_classic()+
  theme(legend.position = "bottom") +
  scale_color_brewer(palette = "Dark2")

mds_plot_10
```


```{r}
heatmap_raw <- data_q %>% 
  #select(unique_data, sim_data_quantile, id_facet, id_x) %>% 
  mutate(category = paste0("facet= ", id_facet, ",x= ", id_x)) %>% 
  unnest(sim_data_quantile) %>% 
  group_by(unique_data, category) %>% 
  mutate(D = row_number()) %>% 
  arrange(D, category) %>% 
  left_join(dist_mat, by = c("unique_data")) %>% 
  select(c(2:9))


data2 <- cbind(dist_mat_format, groups) 

heatmap_final <- bind_cols(serial_data = rownames(data2) %>%
                             as_tibble(),
          group = data2$groups) %>%
  left_join(heatmap_raw, by = c("value" = "unique_data")) %>% select(-id_facet, - id_x, - data_type1) %>% 
  arrange(D, category, group)


#heatmap_final$data_type <- factor(heatmap_final$data_type,
#                                    levels = c("data_null" , "data_varf",
#                                               "data_varx","data_varall"))
  
#heatmap_final$category <- reorder(heatmap_final$category, heatmap_final$data_type)
  
ggplot(heatmap_final,
       aes(x= reorder(value, data_type), y = as.factor(D))) +
  geom_tile(aes(fill = sim_data_quantile)) +
  scale_fill_distiller(palette = "YlGnBu", direction = 1)+ 
  facet_wrap(~category, ncol = 1) +
  theme(axis.text.x = element_text(angle = 90)) +
  ylab("Deciles") +
  xlab("data sets")
 

```

## Cluster validation

```{r}
library(fpc)
library(cluster)
seq_data = tibble(groups, name = names(groups)) %>% 
  mutate(seq_group  = row_number()) %>% 
  left_join(ndata, by = c("name" = "unique_data")) %>% 
  left_join(data_q %>% distinct(unique_data, data_type),
            by = c("name" = "unique_data"))

seq_data$data_type <- factor(seq_data$data_type,
                                    levels = c("data_null" , "data_varf",
                                               "data_varx","data_varall"))

v <- cluster.stats(d, as.numeric(seq_data$groups), as.numeric(seq_data$data_type), silhouette = TRUE)

v
```

The total average (mean of all individual silhouette widths) is `r round(v$avg.silwidth, 3)` and corrected Rand index is `r round(v$corrected.rand, 3)`
