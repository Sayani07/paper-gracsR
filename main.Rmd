---
title:  Clustering based on probability distributions with application on residential customers

# to produce blinded version set to 1
blinded: 0

authors:
- name: Sayani Gupta
  affiliation: Department of Econometrics and Business Statistics, Monash University
  thanks: "Email: Sayani.Gupta@monash.edu"

- name: Rob J Hyndman
  affiliation: Department of Econometrics and Business Statistics, Monash University

- name: Dianne Cook
  affiliation: Department of Econometrics and Business Statistics, Monash University
  
keywords:
- data visualization
- statistical distributions
- time granularities
- calendar algebra
- periodicties
- grammar of graphics
- R

abstract: |
 Clustering elements based on behavior across time granularities 
bibliography: [bibliography.bib]
preamble: >
  \setlength {\marginparwidth }{2cm}
  \usepackage{mathtools,amssymb,booktabs,longtable,todonotes,amsthm}
  \def\mod{~\text{mod}~}
output:
  bookdown::pdf_book:
    base_format: rticles::asa_article
    fig_height: 4
    fig_width: 6
    fig_caption: yes
    dev: "pdf" 
    keep_tex: yes
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache=TRUE, messages=FALSE, warning=FALSE)
# Make sure you have the latest version of rmarkdown and bookdown
#devtools::install_github("rstudio/rmarkdown")
#devtools::install_github("rstudio/bookdown")
library(ggplot2)
library(gravitas)
library(gracsr)
library(ggdendro)
library(dplyr)
library(readr)
library(visdat)
library(ggplot2)
library(tidyverse)
library(naniar)
library(here)
library(tsibble)
library(knitr)
library(patchwork)
```

```{r external, include = FALSE, cache = FALSE}
knitr::read_chunk(here('script/smart_meter.R'))
```


```{r mytheme}
theme_validation <- function() {
  theme_bw() +
      theme(
    strip.text = element_text(size = 8, margin = margin(b = 0, t = 1)),
    plot.margin = margin(0, 0, 0, 0, "cm"),
    axis.title.y = element_blank(),
    #axis.text.x = element_blank(),
  axis.text.y = element_blank(),
  axis.ticks = element_blank(),
      legend.position = "bottom"
    )
}

```


# Introduction

<!-- Description of data available -->
The Smart Grid, Smart City (SGSC) project, which rolled out Australia's first commercial-scale smart grid was implemented across eight local government areas in New South Wales (NSW). Data from more than 13,000 household electricity smart meters is obtained as part of that project. It provides half-hourly energy usage and demographic data for Australia, as well as detailed information on appliance use, climate, retail and distributor product offers, and other related factors. The trials were based in Newcastle, New South Wales, but also covered areas in Sydney CBD, Newington, Ku-Ring-Gai, and the rural township of Scone. The load time series is asynchronous as it is observed for these households for unequal time lengths and consists of missing observations.

\noindent The massive amount of data generated in such projects could be overwhelming for analysis. Electricity utilities can utilize the consumption patterns of customers to develop targeted tariffs for individual groups and alleviate the problem of volatility in production by capitalizing on the flexibility of consumers. Beyea (2010) has pointed out, there has been little discussion or exploration of the full potential of these data bases and their benefits can reach beyond the original intentions for collecting these data. Thus, there is a scope to investigate and analyze these data in various ways for a greater understanding of consumption patterns and how they correlate with other economic, physical or geographical factors. In this work, we are interested to see how we can utilize this dataset to group different customers with similar periodic behavior. Towards this goal, this chapter aims to: (a) describe the contents of the data set in SGSC database that we can utilize, and (b) propose a clustering algorithm to group customers with similar periodic behaviors. The distance metric introduced in Chapter 2 will be the inputs for this cluster analysis. One of the advantages of using our 
approach is that the technique is based on probability distributions instead of raw data. Many clustering approaches are limited by the type of noisy, patchy, and unequal time-series common in residential data sets. Since the distance measure considered is based on differences in probability distribution of time series, it is likely to be less sensitive to missing or noisy data.

_Themes_

- Dimension reduction: If each $P_{i, j, k}$ be considered to be a point in the space, key $i$ would have $mp$ dimensions as opposed to $n_i$ dimensions in case of considering raw data. Hence for a large number of observations ($n_i>>mp$), this approach benefits by transitioning to a lower dimension.
 
 - Avoid loss of information due to aggregation: This approach ensures key characteristic information of the data is not lost due to averaging or aggregation measures in an attempt to transition to a lower dimension. Hence, this approach could be thought to somehow balance the drawback of considering raw data or aggregated data.

- Robustness to outliers: This approach could be adapted to be robust to outliers and extreme behaviors by trimming the tails of the probability distributions.

- Non-synchronized observed time periods: Considering probability distribution would imply the clustering process can handle keys that are observed over periods of time that are overlapping but don't necessarily coincide.

- Similar periodic behavior: Since cyclic granularities are considered instead of linear granularities, clustering would group keys that have similar behavior across these cyclic granularities. This implies they will be grouped according to their periodic behavior and not on the linear stretch of time over which they are observed.



_Common load clustering techniques of smart meter data_

The foundation for this study is Tureczek2017-pb, which conducts a systematic review of the current state of the art in smart meter data analytics, which evaluates approximately 2100 peer-reviewed papers and summarizes the main findings. None of the 34 selected papers which focus on clustering consumption are based on Australian smart meter data. The clustering is frequently applied directly to the raw data without scrutinizing for auto correlation and periodicity. The algorithm most ubiquitously employed is K-Means. But the omission of the time series structure or correlation in the analysis while employing K-Means leads to inefficient clusters. Principal Component Analysis or Self-Organizing Maps removes correlation structures and transitions the data to a reduced feature space, but it comes at a cost of interpretability of the final results. @Tureczek2018-ha has shown that a transformation of data to incorporate autocorrelation before K-Means clustering can improve performance and enable K-Means to deliver smaller clusters with less within-cluster variance. However, it does not explain the cluster composition by combining it with external data. Some papers present pre-processing of the smart-meter data before clustering through principal component analysis or factor analysis for dimensionality reduction or self-organizing maps for 2-Dimensional representation of the data (@Ndiaye2011-pf). Other algorithms used in the literature include k-means variations, hierarchical methods and k-medoids based on a greedy algorithm have been designed to select typical periods in the time series. As the methods are often situation specific, 
it makes sense to compare them on the performance rather than any standard performance metric. A type of clustering based on information theory such as Shannon or Renyi entropy and their variants are addressed in , which differs from typical methods adopted for electricity consumer classification, based on the Euclidean distance notion. @Motlagh2019-yj presents strategy to address the problems on patchy, and unequal time-series common in residential data sets by converting load time series into map models. Most time-series clustering models are limited to handling time domain with same start and end date and time. Most of the solutions to handle this like longest common subsequence, dynamic time warping are prone to computational limit with increased length of the series.

The following contributions are made through the following chapter:

 * Present a cluster analysis of SGSC dataset to group households with similar periodic behavior
 * Cluster validation by relating to external data


<!-- Electricity smart meter technology is increasingly being deployed in residential and commercial buildings. For example, in the state of Victoria, Australia, it is a state government policy that all households have a smart meter fitted (Victoria State Government, 2015), which has resulted in 97% of the approximately 2.1 million households in Victoria having a smart meter installed in their home. These smart meters collect energy usage information at half-hourly intervals, resulting in over 35 billion half-hourly observations per year across all households in the state. Clustering electricity consumption patterns can enable electricity utilities to develop targeted tariffs for individual groups alleviating the problem of volatility in production by capitalizing on the flexibility of consumers. -->

<!-- Clustering households using only smart meter consumption data could also provide value in a societal setting by combining the findings and external data like weather conditions, socio-economic or other demographic factors of those households. -->

<!-- Background literature and shortcomings -->


<!-- # Preliminary exploratoration -->

<!-- ## Electricity demand data -->

<!-- _Electricity use interval reading data_   -->
<!-- The data from [SGSC consumer trial data](https://data.gov.au/data/dataset/smart-grid-smart-city-customer-trial-data) is available through [Department of the Environment and Energy](https://data.gov.au/data/organization/doee). -->


<!-- ### Data source -->

<!-- The entire data is procured from CSIRO. A subset of this data is also available from [SGSC consumer trial data](https://data.gov.au/data/dataset/smart-grid-smart-city-customer-trial-data) is available through [Department of the Environment and Energy](https://data.gov.au/data/organization/doee). It consists of the following data sets.  -->
<!-- _1. CustomerData:_ 78720 customers with 62 variables about them  -->
<!-- _2. EUDMData:_ 300 billion half-hourly consumption level data   -->
<!-- _3. OffersData:_ Method of contact to customer to join SGSC customer trial, either door-to-door (D2D) or via Telesales   -->
<!-- _4. PEResponseData:_ Peak Events response customer wise   -->
<!-- _5. PETimesData:_ Peak Events time stamps   -->

<!-- Only _CustomerData_ and _EUDMData_ are relevant for the clustering goals of this paper. _EUDMData_ contains half-hourly general supply in KwH for 13,735 customers, resulting in 344,518,791 observations in total. `CustomerData` provides demographic data for 78,720 customers with information about their Local Government Area amongst others. -->


<!-- ## LGA and weather data -->

<!-- Since the smart meters have been installed at different dates for each household, it is reasonable to assume that the records are obtainable for different time lengths -->
<!-- for each household. Since, general supply is available for only 13,735 customers, we will restrict ourselves to look at the LGA information for these customers only. We find that there are only 26 LGA that is covered for these customers. -->


<!-- _Weather data_   -->
<!-- This data is obtained through [Australian Government Bureau of Meteorology](BOM)(http://www.bom.gov.au/) and provides hourly data for nearest weather stations for all the LGAs -->


<!-- This section familiarizes the 13,000 SGSC households through visualization and provide a detailed layout of data structures (like missing observations/number of customers/number of observations) and also the external data that needs to be utilized for validating the clustering process. The [ABS TableBuilder](https://www.abs.gov.au/websitedbs/censushome.nsf/home/tablebuilder) has census data from 2011 and 2016. The data is at SA2 and LGA levels. However, some of the LGA in NSW changed between 2011 and 2016 and hence there would not be a one-to-one correspondence between the LGAs. Weather, notably temperature (and humidity) can be the main driver(s) for energy usage. In NSW many households have electric heaters so their use can impact winter energy use and air-conditioners can impact summer energy use. Relevant weather data could be obtained from the [Bureau of Meteorology](www.bom.gov.au). Some weather stations have 30 minute (sometimes even smaller interval) weather data. Potentially, there could be lag effects of weather on energy usage which should be considered. -->



# Clustering methodology


<!-- In contrast to models, a feature-based strategy is used to explicitly define or automatically extract the curves’ key time features, for instance by application of PCA on the daily curves [ -->




<!-- Most papers discussed in Tureczek2017-pb fail to accept smart meter readings as time series data, a data type which contains a temporal component. The omission of the essential time series features in the analysis leads to the application of methods that are not designed for handling temporal components. K-Means ignores autocorrelation, unless the input data is pre-processed. The clusters identified in the papers are validated by a variety of indices, with the most prevalent -->
<!-- being the cluster dispersion index (CDI) [22–24], the Davies–Bouldin index (DBI) [25,26] and the mean index adequacy (MIA) [8,13]. -->


The data set solely contains readings from smart meters and no information about the consumers' specific physical, geographical, or behavioural attributes. As a result, no attempt is made to explain why consumption varies. Instead, this work investigates how much energy usage heterogeneity can be found in smart meter data and what some of the most common electricity use patterns are. It is worth noting that when studying these dynamics, a variety of objectives may be pursued. One objective could be to group consumers with similar shapes over all relevant cyclic granularities. In this scenario, the variation in customers within each group is in magnitude rather than shape, while the variation between groups is only in shape. Most clustering algorithms offer only daily energy profiles throughout the hours of the day, but we suggest a broader approach to the problem, aiming to group consumers with similar shapes across all significant cyclic granularities. Another purpose of clustering could be to group customers that have similar differences in patterns across all major cyclic granularities, capturing similar jumps across categories regardless of the overall shape. For example, in the first goal, similar shapes across hours of the day will be grouped together, resulting in customers with similar behaviour across all hours of the day, whereas in the second goal, any similar big-enough jumps across hours of the day will be clubbed together, regardless of which hour of the day it is. Both of these objectives may be useful in a practical context and, depending on the data set, may or may not propose the same customer classification.

The proposed methodology aim to leverage the intrinsic temporal data structure hidden in time series data. The foundation of our method is unsupervised clustering algorithms based exclusively on time-series features. First, we study the underlying distributions that may have resulted in different patterns across temporal granularities in order to identify a mechanism to classify them based on the similarity of those distributions. Depending on the goal of clustering, the distance metric for defining similarity would be different. These distance metrics could be fed into a clustering algorithm to break large data sets into subgroups that can then be analyzed separately. These clusters may be commonly associated with real-world data segmentation. However, since the data is unlabeled a priori, more information is required to corroborate this. This section presents the work flow of the methodology: 


- _Data preparation_

@wang2020tsibble introduced the tidy "tsibble" data structure to support exploration and modeling of temporal data. A tsibble comprises an index, optional key(s), and measured variables. An index is a variable with inherent ordering from past to present and a key is a set of variables that define observational units over time. A linear granularity is a mapping of the index set to subsets of the time domain. For example, if the index of a tsibble is days, then a linear granularity might be weeks, months or years. For each key variable, the raw smart meter data is a sequence that is indexed by time and comprises values of several measurement variables at each time point. This sequence, though, could be depicted in a variety of ways. A shuffling of the raw sequence could reflect the distribution of hourly consumption over a single day, while another could indicate consumption over a week or a year. These temporal deconstructions of a time period into units such as hour-of-day, work-day/weekend are called cyclic temporal granularities. All cyclic granularities can be expressed in terms of the index set and could be augmented with the initial tsibble structure (index, key, measurements). It is worthwhile to note that the data structure changes while transporting from linear to cyclic scale of time as multiple observations of the measured variable would correspond to each category of the cyclic granularities. In this paper, quantiles are chosen to characterize the distributions for each category of the cyclic granularity. So, each category of a cyclic granularity corresponds to a list of numbers which is essentially few chosen quantiles of the multiple observations.


- _Finding significant cyclic granularities or harmonies_

\noindent These cyclic granularities are useful for exploring repetitive patterns in time series data that get lost in the linear representation of time. It is advantageous to consider only those cyclic granularities across which there is a significant repetitive pattern for the majority of customers or noteworthy in an electricity-behavior context. In that case, when the customers are grouped, we can expect to observe some interesting patterns across the categories of the cyclic granularities considered. [XXX reference 2nd chapter] proposes a way to select significant cyclic granularities and harmonies which is used for this paper.


- _Individual or combined categories of cyclic granularities as DGP_

The existing work on clustering probability distributions assumes we have an iid sample $f_1(v),\dots,f_n(v)$, where $f_i(v)$ denotes the distribution from observation $i$ over some random variable $v = \{v_t: t = 0, 1, 2, \dots, T-1\}$ observed across $T$ time points. In our work, we are using $i$ as denoting a customer and the underlying variable as the electricity demand. So $f_i(v)$ is the distribution of household $i$ and $v$ is electricity demand. In this work, instead of considering the probability distributions of the linear time series, we assume that the measured variables across different categories of any cyclic granularity are from data generating processes.
Hence, we want to be able to cluster distributions of the form $f_{i,A,B \dots, {N_C}}(v)$, where $A, B$ represent the cyclic granularities under consideration such that $A = \{a_j: j=1, 2, \dots J\}$,  $B = \{b_k: k  = 1, 2, \dots K\}$ and so on. We consider individual each category of a cyclic granularity ($A$) or combination of categories for interaction of cyclic granularities (for e.g. $A*B$) to have a distribution. For example, let us consider we have two cyclic  granularities of interest, $A = {0, 1, 2, \dots, 23}$ representing hour-of-day and  $B = \{Mon, Tue, Wed, \dots, Sun\}$ representing day-of-week. 
each customer $i$ consist of a collection of probability distributions. In case individual granularities ($A$ or $B$) are considered there are  $J = 24$ distributions of the form $f_{i,j}(v)$ or $K = 7$ distributions of the form $f_{i,k}(v)$ for each customer $i$. In case of interaction,  $J*K=168$ distributions of the form $f_{i,j, k}(v)$ could be conceived for each customer $i$. As a result, a distance between collections of these univariate probability distributions is required. Depending on the objective of the problem, there could be many approaches to considering such distances. This paper considers two approaches, which are explained in the next segment.


- _Distance metrics_

Considering each individual or combined categories of cyclic granularities as a data generating process lead to a collection of conditional distributions for each customer $i$. The (dis) similarity between each pair of observations should be obtained by combining the distances between these collections of conditional distributions such that the resulting metric is a distance metric, which could be fed into the clustering algorithm. Two types of distance metric is considered:

<!-- The choice of distance measures is a critical step in clustering. It defines how the similarity of two elements (x, y) is calculated. -->

**Inter-category distances**

This distance matrix considers two objects to be similar if every category of an individual cyclic granularity or combination of categories for interacting cyclic granularities have similar distributions. In this study, the distribution for each category is characterized using deciles and the distances between distributions are computed by using the Jensen-Shannon distance, which is symmetric and hence could be used as a distance measure. 


\noindent The total distance between two elements $x$ and $y$ is then defined as $$S^A_{x,y} = \sum_{j} D_{x,y}(A)$$ (sum of distances between each category $j$ of cyclic granularity A) or  $$S^{A*B}_{x,y} = \sum_j \sum_k D_{x,y}(A, B)$$ (sum of distances between each combination of categories $(j, k)$ of the harmony $(A, B)$. When combining distances from individual cyclic granularities $A$ and $B$, $$S^{A, B}_{x, y} = S^A_{x,y}/J + S^B_{x,y}/K$$ is used, which could also be shown to be a distance metric easily. This is shown for cyclic granularity $A$ and $B$, but could be practically extended to more granularities.

**Intra-category distances**

Compute weighted pairwise distances ($wpd$) (XXX reference) for all considered granularities for all objects. $wpd$ is designed to capture the maximum variation in the measured variable explained by an individual cyclic granularity or their interaction and is estimated by the maximum pairwise distances between consecutive between consecutive categories normalized by appropriate parameters. A higher value of $wpd$ indicates that some interesting pattern is expected, whereas a lower value would indicate otherwise. 


\noindent Distance between objects is then taken as the euclidean distances between them with the granularities being the variables and $wpd$ being the value under each variable. Since Euclidean distance is chosen, the observations with high values of features ($wpd$ values) will be clustered together. The same holds true for observations with low values of features. Thus this distance matrix would be useful to group customers that have similar significance of patterns across different granularities.



<!-- Consider a harmony table consisting of many harmonies, each of the form $(A, B)$, such that $A = \{ a_j: j = 1, 2, \dots, J\}$ and $B = \{ b_k: k = 1, 2, \dots, K\}$. Each household consists of a $J*K$ distributions one harmony. We compute the distributional difference -->
<!-- between $(A, B)$ for the $s^{th}$ household using $wpd_{{s}}(A,B)$. -->
<!-- $wpd_{{s}}(A,B)$ denotes the normalized weighted-pairwise distributional distances between $(A, B)$ and is a feature which measures distributional difference between harmonies. If we have $H_{N_C}$ harmonies in the harmony table, then for each household we have a vector of $wpd_{{s}}$ of $H_{N_C}$ elements with each element corresponding to one harmony. We aim to have pockets of households showing similar periodic behavior by conidering $wpd$ vlaues for different harmonies and some time series features. The features should also characterize probability distributions of different household. -->


<!-- grouping probability distributions across a harmony. This clustering algorithm is adopted to remove or appropriately adjust for auto correlation and unequal length in the data. The method could be further extended by clustering probability distributions conditional on one or more cyclic granularities. The following are some of the advantages of our proposed method. -->



- _Pre-processing steps_

    **Handling trend, seasonality, non-stationarity and auto-correlation:** Trend and seasonality are fundamental characteristics of time series data, and it is reasonable to define a time series according to its degree of trend and seasonality. These characteristics of the time series are lost or handled independently by considering probability distributions ( trend is lost) across categories of cyclic granularities (by independently modeling all seasonal fluctuations), and so there is no need to de-trend or de-seasonalize the data before conducting the clustering method. There is no need to omit holiday or weekend patterns for similar reasons.

    **Data transformation:** Robust scaling method is used before computing the Inter-category distances and NQT is built-in transformation used for computation of $wpd$, which forms the basis of Intra-category distances.


- _Clustering algorithm_

In the analysis of energy smart metre data, K-Means or hierarchical clustering are often employed. These are simple and effective techniques that work well in a range of scenarios. For clustering, both employ a distance measure, and the distance measure chosen has a major influence on the structure of the clusters. We employ agglomerative hierarchical clustering in conjunction with Ward's criteria (XXX reference). Individual entities with the highest similarity computed using the desired distance metrics are sequentially merged using agglomerative algorithms. We can possibly employ any clustering method that supports the given distance metric as input.

- _Characterization of clusters_

Depending on the distance measure utilized for the study, the cluster characterization technique will differ. Clusters that utilise intra-category distances are characterised using multi-dimensional scaling and parallel coordinate displays. For inter-category distances, the distribution across major granularities may be presented to ensure that the goal of similar shapes within clusters and distinct shapes across clusters is met. This technique may potentially make advantage of multi-dimensional scaling.


\noindent Multidimensional scaling (MDS) (XXX reference) refers to a family of methods that analyse a matrix of distances or dissimilarities to provide a representation of the data points in a reduced-dimension space. There are many kinds of MDS, but they all solve the same fundamental issue: Given a $n*n$ matrix of dissimilarities and a distance measure, identify a configuration of $n$ points $x_1, x_2, \dots, x_n$ in the reduced dimension space $R^q$ ($q<p$) where the distance between the points is near to the dissimilarity between the points. All techniques must determine the coordinates of the points as well as the space dimension, $q$. Metric and nonmetric MDS are the two main kinds of MDS. Metric MDS methods presume a functional connection between the interpoint distances and the supplied dissimilarities and assume that the data are quantitative. We use metric MDS.

\noindent Parallel coordinate plots (XXX reference) Parallel coordinates have been extensively used to display high-dimensional and multivariate data, allowing for the detection of patterns within the data via visual grouping.





<!-- - A random sample  of the original data is taken for clustering analysis and includes missing and noisy observations (detailed description in Appendix) -->

<!-- - All harmonies are computed for each customer in the sample. -->
<!-- Cyclic granularities which are clashes for all customers in the sample are removed. -->

<!-- - It is worth noting that a number of other solutions may be considered at the pre-processing stage of the method. We have considered a) Normal-Quantile Transform and b) Robust transformation. -->

<!-- - Two methods are considered for computing dissimilarity between two customers. The first one involves computing according to one granularity is computed as the sum of the JS distances between distribution of all the categories of the granularity. When we consider more than one granularity, we consider the sum of the  average distances for all the granularity so that the combined metric is also a distance. -->

<!-- - Given the scale of dissimilarity among the energy readings, the model chooses optimal number of clusters -->

<!-- - Once clusters have been allocated, the groups are explored visually. -->

<!-- - Results are reported and compared. -->

<!-- Two methods are used for computing distances between subjects and then hierarchical clustering algorithm is used. -->

<!-- The existing work on clustering probability distributions assumes we have an iid sample $f_1(v),\dots,f_n(v)$, where $f_i(v)$ denotes the probability distribution from observation $i$ over some random variable $v = \{v_t: t = 0, 1, 2, \dots, T-1\}$ observed across $T$ time points. In our work, we are using $i$ as denoting a customer and the underlying variable as the electricity demand. So $f_i(v)$ is the distribution of household $i$ and $v$ is electricity demand. -->

<!-- We want to cluster distributions of the form $f_{i,j,k}(v)$, where $i$ and $j$ denote  -->

<!-- Consider a harmony table consisting of many harmonies, each of the form $(A, B)$, such that $A = \{ a_j: j = 1, 2, \dots, J\}$ and $B = \{ b_k: k = 1, 2, \dots, K\}$. Each household consists of a $J*K$ distributions one harmony. We compute the distributional difference -->
<!-- between $(A, B)$ for the $s^{th}$ household using $wpd_{{s}}(A,B)$. -->
<!-- $wpd_{{s}}(A,B)$ denotes the normalized weighted-pairwise distributional distances between $(A, B)$ and is a feature which measures distributional difference between harmonies. If we have $H_{N_C}$ harmonies in the harmony table, then for each household we have a vector of $wpd_{{s}}$ of $H_{N_C}$ elements with each element corresponding to one harmony. We aim to have pockets of households showing similar periodic behavior by conidering $wpd$ vlaues for different harmonies and some time series features. The features should also characterize probability distributions of different household. -->


<!-- ### Notations -->

<!-- Consider an iid sample $f_1(v),\dots,f_n(v)$, where $f_i(v)$ denotes the probability distribution from observation $i$ over some random variable $v = \{v_t: t = 0, 1, 2, \dots, T-1\}$ observed across $T$ time points. In our work, we are using $i$ as denoting a household and the underlying variable as the electricity demand. Further consider a cyclic granularity of the form $B = \{ b_k: k = 1, 2, \dots, K\}$. Each customer consists of collection of probability distributions. -->


<!-- So $f_i(v)$ is the distribution of household $i$ and $v$ is electricity demand. We want to cluster distributions of the form $f_{i,j,k}(v)$, where $i$ and $j$ denote $i^{th}$ and $j^{th}$ customer respectively. -->



<!-- a harmony table consisting of many harmonies, each of the form $(A, B)$, such that $A = \{ a_j: j = 1, 2, \dots, J\}$ and $B = \{ b_k: k = 1, 2, \dots, K\}$.  -->




<!-- ### A single or pair of granularities together (change names) -->

<!-- The methodology can be summarized in the following steps: -->

<!-- - _Pre-processing step_ -->

<!-- Robust scaling method or NQT used for each customer. -->


<!-- - _NQT_ -->


<!-- - _Treatment to outliers_ -->






<!-- ### Many granularities together (change names) -->

<!-- The methodology can be summarized in the following steps: -->


<!-- 1. Compute quantiles of distributions across each category of the cyclic granularity -->
<!-- 2. Compute JS distance between households for each each category of the cyclic granularity -->
<!-- 3. Total distance between households computed as sum of JS distances for all hours -->
<!-- 4. Cluster using this distance with hierarchical clustering algorithm (method "Ward.D") -->

<!-- _Pro:_   -->
<!-- - distance metric makes sense to group different shapes together   -->
<!-- - simulation results look great on typical designs   -->
<!-- _Cons:_   -->
<!-- - Can only take one granularity at once   -->
<!-- - Clustering a big blob of points together whereas the aim is to groups these big blob into smaller ones   -->

<!-- ### Multiple-granularities -->

<!-- _Description:_   -->

<!-- Choose all significant granularities and compute wpd for all these granularities for all customers. Distance between customers is taken as the euclidean distances between them with the granularities being the variables and wpd being the value under each variable for which Euclidean distance needs to be measured.   -->
<!-- _Pro:_   -->
<!-- - Can only take many granularities at once -->
<!-- - can apply variable selection PCP and other interesting clustering techniques -->
<!-- - simulation results look great on typical designs -->
<!-- - splitting the data into similar sized groups   -->
<!-- _Cons:_   -->
<!-- - distance metric does not make sense to split the data into similar shaped clusters  -->



# Validation

To evaluate clustering approaches, we create data designs that replicate prototype behaviors that might be seen in electricity data contexts. We spiked several features in the data to see where one method works better than the other and where they might give us the same outcome or the effect of missing data and trends on the proposed methods. A continuous measured variable $y$ of length $T$ indexed by ${0, 1, \dots T-1}$. Three circular granularities $g1$, $g2$ and $g3$ are considered with $2$, $3$ and $5$ levels respectively. Categories of $g1$, $g2$ and $g3$ are represented by ${g10,g11}$, ${g20, g21, g22}$ and ${g30, g31, g32, g33, g34}$. These categories could be integers or some more meaningful labels. For example, the granularity "day-of-week" could be either represented by ${0, 1, 2, \dots, 6}$ or ${Mon, Tue, \dots, Sun}$. Each categories of $g1$, $g2$ and $g3$ could be assumed to be from same distribution, few from one and others from separate distributions, or all from different distributions, resulting in distinct data designs. We created independent replications $(R = \{25, 250, 500\})$ of all data designs to see if our proposed clustering approaches can detect distinct designs in various groups for small, medium and large number of series. A sample size of $T=\{300, 1000, 5000\}$ is used in all designs to test small, medium and large sized series. For all data designs, the data type is set to  be "continuous," and a gaussian setup for errors is assumed. The method could preform differently with different jumps between consecutive categories. So a difference of $diff = {1, 2, 5}$ for corresponding categories are also considered. The code for creating these designs can be found in the Supplementary section (link to github repo). The results for $T=300$ and $R=25$ is shown, that means we have $25$ time series each with length $300$. The rest of the results could be found in the supplementary paper.


## Data generating processes

An ARMA (p,q) process is used to generate series, where $p$ and $q$ are selected at random such that the series is stationary. The various designs on $g1$, $g2$, and $g3$ are introduced by adding matching designs to this series' innovations. The innovations are considered to have a normal distribution, although they follow the same pattern as the designs. To eliminate the effect of starting values, the first 500 observations in each series are discarded.


## Data designs  

**Three signifiant granularities**

<!-- Consider a case where all the three granularities $g1$, $g2$ and $g3$ would be responsible for making the designs distinct. That would mean, the pattern for each of $g1$, $g2$ and $g3$ will change for at least one design. We consider a situation with all the null cases corresponding to no difference in distribution across categories, that is, all categories follow the same distribution N(0,1). -->

Consider the scenario when all three granularities $g1$, $g2$, and $g3$ are responsible for distinguishing the designs. That means that for at least one among the to-be-grouped designs, the pattern for each of $g1$, $g2$, and $g3$ will change. We consider a _null_ case to represent that categories follow N(0,1) and _alternate_ case to represent different  distributions across categories (as in Table \ref{tab:}) that will lead to different designs (as in Table \ref{tab:}).



```{r tab-distribution}

# table_mayan <- tibble(granularity  = )


```


| granularity 	|  Alternate design 	|
|---	|---	|
| g1 	| g10 ~ N(0, 1), g11 ~ N(2, 1) 	|
| g2 	| g21 ~ N(2, 1), g22 ~ N(1, 1), g23 ~ N(0, 1) 	|
| g3 	| g31 ~ N(0, 1), g32 ~ N(1, 1), <br><br>g33 ~ N(2, 1), g34 ~ N(1, 1), g35 ~ N(0, 1) 	|

A combination of these null and alternate distributions are considered when building the designs as in the table \ref{tab:tab-design}. Figure \ref{#fig:plot-linear-3change} shows the linear and cyclic representation of $y$ under these five designs.


```{r tab-design}
design = c("design-1", "design-2", "design-3", "design-4", "design5")
g1 = c("null", "vary", "null", "null", "vary")
g2 = c("null", "null", "vary", "null", "vary")
g3 = c("null", "null", "null", "vary", "vary")
table <- tibble(design, g1, g2, g3) %>% kable()
table

```


```{r generate-design-3change}
generate_design <- function(t, mu1, mu2, mu3){
  
t <- seq(0, t, 1)
g1 <- t %%2
g2 <- t %%3
g3 <- t %%5

# null design
g1_dnull <- rep( rep(0, each = length(unique(g1))), length.out= length(t))
g2_dnull <- rep( rep(0, each = length(unique(g2))), length.out= length(t))
g3_dnull <- rep( rep(0, each = length(unique(g3))), length.out= length(t))
 
# mean changing across categories in varying ways

g1_dvary <- rep(mu1, length.out= length(t))
g2_dvary <- rep(mu2, length.out= length(t))
g3_dvary <- rep(mu3, length.out= length(t))


design1 = distributional::dist_normal(g1_dnull + g2_dnull + g3_dnull)
design2 = distributional::dist_normal(g1_dvary + g2_dnull + g3_dnull)
design3 = distributional::dist_normal(g1_dnull + g2_dvary + g3_dnull)
design4 = distributional::dist_normal(g1_dnull + g2_dnull + g3_dvary)
design5 = distributional::dist_normal(g1_dvary + g2_dvary + g3_dvary)

data_bind <- tibble::tibble(
index = t,
g1 = g1,
g2 = g2,
g3 = g3,
design1 = distributional::generate(design1, times = 1) %>% unlist(),
design2 = distributional::generate(design2, times = 1) %>% unlist(),
design3 = distributional::generate(design3, times = 1) %>% unlist(),
design4 = distributional::generate(design4, times = 1) %>% unlist(),
design5 = distributional::generate(design5, times = 1) %>% unlist()
) %>% 
  pivot_longer(-c(1, 2, 3, 4), names_to = "design", values_to = "sim_data")

data_bind
}

t = 300
mu1= c(0, 2)
mu2 = c(2, 1, 0)
mu3 = c(0, 1, 2, 1, 0)

data_bind <- generate_design(t, mu1, mu2, mu3)

```

```{r plot-linear-change , out.width="100%", fig.cap="The linear and cyclic representation of the time series variable y. It is not possible to comprehend these patterns across cyclic granularities g1, g2 and g3 or group similar series just by looking at the linear plots."}

# plot_linear_data <- function(data){
# ggplot(data,
#              aes(x = index, y = sim_data)) + 
#   geom_line() +
#   xlab("index")+
#   theme_bw() 
# }

p1 <- ggplot(data_bind,
             aes(x = index, y = sim_data)) + 
  geom_line() +
  xlab("index")+
  facet_wrap(~design, scales = "free_y",ncol =1) +
  theme_validation()

p2 <- ggplot(data_bind,
             aes(x = as.factor(g1), y = sim_data)) + 
  geom_boxplot(alpha =0.5) + xlab("g1") + 
  facet_wrap(~design, scales = "free_y", ncol = 1)+ stat_summary(
    fun = median,
    geom = 'line',
    aes(group = 1), size = 0.8, color = "blue") +
  theme_validation()



p3 <- ggplot(data_bind, aes(x = as.factor(g2), y = sim_data)) + geom_boxplot(alpha =0.5) + xlab("g2") + theme_bw() +
  facet_wrap(~design, scales = "free_y",ncol = 1)+ stat_summary(
    fun = median,
    geom = 'line',
    aes(group = 1), size = 0.8, color = "blue") + ylab("")+
  theme_validation()

p4 <- ggplot(data_bind, aes(x = as.factor(g3), y = sim_data)) + geom_boxplot(alpha =0.5) +
  xlab("g3") + theme_bw()+
  facet_wrap(~design, scales = "free_y", ncol = 1)+ stat_summary(
    fun = median,
    geom = 'line',
    aes(group = 1), size = 0.8, colosr = "blue")+ ylab("")+
  theme_validation()


(p1 + (p2 + p3 + p4)) *
  theme_validation() + plot_layout(widths = c(2, 1))
# 
# plot_cyclic_data( data_bind %>% filter(design=="design2"))
# plot_data( data_bind %>% filter(design=="design3"))
# plot_data( data_bind %>% filter(design=="design4"))
# plot_data( data_bind %>% filter(design=="design5"))
```


**Two significant granularities**

Consider a case where distribution of $y$ would vary across levels of $g2$ for all designs, across levels of $g1$ for few designs and $g3$ does not change across designs. So $g3$ is not responsible for distinguishing across designs. Figure \ref{#fig:plot-linear} shows the linear and cyclic representation of $y$. The first panel shows raw plot of $y$ in a linear scale and the second panel shows distribution of $y$ across cyclic granularities namely $g1$, $g2$ and $g3$. As could be seen from the plots, it is impossible to decipher from the raw time plot that the time series variable shows such pattern across different granularities.

**One signifiant granularity**

Consider a case where distribution of $y$ would vary across levels of $g2$ for all designs, across levels of $g1$ for few designs and $g3$ does not change across designs. So $g3$ is not responsible for distinguishing across designs. 

<!-- **Missing data** -->

<!-- **Trend** -->

<!-- **Outliers** -->

<!-- observations: -->

<!-- designs: -->



<!-- When we move from the linear to the cyclic world of temporal granularities, we can see patterns across different categories of the granularities which gets lost in the linear representation. Populations are modeled by a collection of these cyclic granularities. Each cyclic granularity might or might not have patterns across its categories. Each cluster is characterized by similar patterns across one or more of these cyclic granularities. -->


<!-- A small example is given to setup the problem. -->

<!-- We constructed the simulation parameters to represent common patterns in electricity data. 50, 200 and 500 time series were chosen for different simulation designs with different granularities and patterns changing across different granularities. -->
<!-- The data type is fixed to be "continuous".We generated independent replications of all combinations of the simulation parameters. -->

<!-- Consider a continuous time series variable $y$ of length $T$ indexed by ${0, 1, \dots T-1}$. Three circular granularities $g1$, $g2$ and $g3$ are considered with 2, 3 and 5 levels respectively. Categories of g1, g2 and g3 are represented by ${0,1}$, ${0, 1, 2}$ and ${0, 1, 2, 3, 4}$. These categories could be integers or some more meaningful labels. For example, the granularity "day-of-week" could be either represented by $\{0, 1, 2, \dots, 6\}$ or $\{Mon, Tue, \dots, Sun\}$. -->

<!-- Consider the case where distribution of $y$ would vary across levels of $g1$ and g2$ but will not vary across levels of $g3$ as described in Figure .Figure \ref shows four designs where $g3$ acts as a nuisance variable, and $g1$ and $g2$ varies in both  Design 2 and 3, and $g2$ varies in all designs. -->


<!-- Consider a case where distribution of $y$ would vary across levels of $g2$ -->
<!-- for all designs, across levels of $g1$ for few designs and $g3$ does not change across designs. Figure \ref{#fig:plot-linear} shows the linear and cyclic representation of $y$. The first panel shows raw plot of $y$ in a linear scale and the second panel shows distribution of $y$ across cyclic granularities namely $g1$, $g2$ and $g3$. As could be seen from the plots, it is impossible to decipher from the raw time plot that the time series variable shows such pattern across different granularities. -->


```{r change-var, echo=FALSE}
t = 300
n <- seq(0, t, 1)
g1 <- n %%2
g2 <- n %%3
g3 <- n %%5
mu11 = c(0, 5)
mu12 = c(3, 0)
mu21= c(2, 0, 0)
mu22 = c(0, 2, 0)
mu23 = c(0, 0, 2)
mu3 = c(0, 0, 0, 0, 0)
```

```{r generate-design} 
generate_design <- function(t, mu21, mu22, mu23){
  
t <- seq(0, t, 1)
g1 <- t %%2
g2 <- t %%3
g3 <- t %%5

# null design
g1_dnull <- rep( rep(0, each = length(unique(g1))), length.out= length(t))
g2_dnull <- rep( rep(0, each = length(unique(g2))), length.out= length(t))
g3_dnull <- rep( rep(0, each = length(unique(g3))), length.out= length(t))
 
# mean changing across categories in varying ways

g11_dvary <- rep(mu11, length.out= length(t))
g12_dvary <- rep(mu12, length.out= length(t))
g21_dvary <- rep(mu21, length.out= length(t))
g22_dvary <- rep(mu22, length.out= length(t))
g23_dvary <- rep(mu23, length.out= length(t))
g3_dvary <- rep(0, length.out= length(t))

design1 = distributional::dist_normal(g1_dnull + g2_dnull + g3_dnull)
design2 = distributional::dist_normal(g11_dvary + g21_dvary + g3_dnull)
design3 = distributional::dist_normal(g12_dvary + g22_dvary + g3_dnull)
design4 = distributional::dist_normal(g1_dnull + g23_dvary + g3_dnull)

data_bind <- tibble::tibble(
index = t,
g1 = g1,
g2 = g2,
g3 = g3,
design1 = distributional::generate(design1, times = 1) %>% unlist(),
design2 = distributional::generate(design2, times = 1) %>% unlist(),
design3 = distributional::generate(design3, times = 1) %>% unlist(),
design4 = distributional::generate(design4, times = 1) %>% unlist(),
#design5 = distributional::generate(design5, times = 1) %>% unlist()
) %>% 
  pivot_longer(-c(1, 2, 3, 4), names_to = "design", values_to = "sim_data")

data_bind
}

data_bind <- generate_design(t, mu21, mu22, mu23)

#data_bind

```


```{r plot-linear , out.width="100%", fig.cap="The linear and cyclic representation of the time series variable y. It is not possible to comprehend these patterns across cyclic granularities g1, g2 and g3 or group similar series just by looking at the linear plots."}

# plot_linear_data <- function(data){
# ggplot(data,
#              aes(x = index, y = sim_data)) + 
#   geom_line() +
#   xlab("index")+
#   theme_bw() 
# }

p1 <- ggplot(data_bind,
             aes(x = index, y = sim_data)) + 
  geom_line() +
  xlab("index")+
  facet_wrap(~design, scales = "free_y",ncol =1) +
  theme_validation()

p2 <- ggplot(data_bind,
             aes(x = as.factor(g1), y = sim_data)) + 
  geom_boxplot(alpha =0.5) + xlab("g1") + 
  facet_wrap(~design, scales = "free_y", ncol = 1)+ stat_summary(
    fun = median,
    geom = 'line',
    aes(group = 1), size = 0.8, color = "blue") +
  theme_validation()



p3 <- ggplot(data_bind, aes(x = as.factor(g2), y = sim_data)) + geom_boxplot(alpha =0.5) + xlab("g2") + theme_bw() +
  facet_wrap(~design, scales = "free_y",ncol = 1)+ stat_summary(
    fun = median,
    geom = 'line',
    aes(group = 1), size = 0.8, color = "blue") + ylab("")+
  theme_validation()

p4 <- ggplot(data_bind, aes(x = as.factor(g3), y = sim_data)) + geom_boxplot(alpha =0.5) +
  xlab("g3") + theme_bw()+
  facet_wrap(~design, scales = "free_y", ncol = 1)+ stat_summary(
    fun = median,
    geom = 'line',
    aes(group = 1), size = 0.8, color = "blue")+ ylab("")+
  theme_validation()


(p1 + (p2 + p3 + p4)) *
  theme_validation() + plot_layout(widths = c(2, 1))
# 
# plot_cyclic_data( data_bind %>% filter(design=="design2"))
# plot_data( data_bind %>% filter(design=="design3"))
# plot_data( data_bind %>% filter(design=="design4"))
# plot_data( data_bind %>% filter(design=="design5"))
```

A subset of many possible designs are shown in Figure \ref{fig: plot-linear }. For the parameter space (XXX unique  combinations shown in table YYY), 100, 500 independent replications of all possible combination of simulation parameters were generated. The clustering methodologies were run all these unique combinations and subsets of these to verify if the methodologies work as expected.


| Granularity type                                                   	| # Significant 	| # Replications 	|
|--------------------------------------------------------------------	|---------------	|----------------	|
| **Individual**  <br><br># obs: 300, 500, 2000  <br># clusters: 6/7 	| 1/2/3         	| 25, 100, 200   	|
| **Interaction**  <br><br># obs: 500, 2000  <br># clusters: 4       	| 1/2           	| 25, 100, 200   	|


## Results

All the methods were fitted to each data designs and results are reported through confusion matrices. With increasing difference between categories, it gets easier for the methods to correctly distinguish the designs. For difference=1, the performances are pretty bad for js-robust methods and wpd method. The performance starts getting better with increasing difference and get worse with increasing number of replications. Length of series do not show to have any effect on the performance of the methods. It does not depend on if time series is ar or arma.


<!-- - confusion matrix could be used for showing results if proper labeling is used -->

<!-- - write about features that we have spiked into the data set -->
<!-- - write about you incorporated noise -->
<!-- - What is the additional structure you can incorporate that will lead to failing of method1 and method2? -->
<!-- - And both gives the same result? Basically say when method 1 works better than method 2 and vice versa! -->

<!-- - -->




# Application


## Data source

The entire data is procured from CSIRO. A subset of this data is also available from [SGSC consumer trial data](https://data.gov.au/data/dataset/smart-grid-smart-city-customer-trial-data) is available through [Department of the Environment and Energy](https://data.gov.au/data/organization/doee). It consists of the following data sets. 
_1. CustomerData:_ 78720 customers with 62 variables about them 
_2. EUDMData:_ 300 billion half-hourly consumption level data  
_3. OffersData:_ Method of contact to customer to join SGSC customer trial, either door-to-door (D2D) or via Telesales  
_4. PEResponseData:_ Peak Events response customer wise  
_5. PETimesData:_ Peak Events time stamps  

Only _CustomerData_ and _EUDMData_ are relevant for the clustering goals of this paper. _EUDMData_ contains half-hourly general supply in KwH for 13,735 customers, resulting in 344,518,791 observations in total. `CustomerData` provides demographic data for 78,720 customers most of which are missing and not utilized for the purpose of this paper. To meet the requirements for anonymity preservation, the energy patterns could not be identified at the individual level, but rather by the geographical location of their residence information about their Local Government Area.

### Characteristics of raw data

In Figure \ref{fig:raw-data-50}, the time series of energy consumption is plotted along the y-axis against time from past to future for $50$ sampled households. Each of these series correspond to a single customer. For each customer, the energy consumption is available at fine temporal resolution (every 30 minutes) for a long period of time (~ 2 years) resulting in 27,000 (median) observations for each customer. Some customers' electricity use may be unavailable owing to power outages or improper recording, resulting in implied missing numbers in the database. For this data set it was found that out of 13735 customers in total, 8685 customers do not have any implicit missing observations, while the rest 5050 customers had missing values. With further exploration, it was found that there is no structure in the missingness, that is missing observations can occur at any time point (see Appendix). Moreover, the data for these customers are characterized by unequal length, different start and end dates.
Since our proposed methods consider probability distribution instead of raw data, both of these characteristics would not pose any threat to our methodology unless of course there is any structure or systematic patterns in them.  


\noindent It can be expected that energy consumption vary substantially between customers, which is a reflection of their varied  behavior owing to differences in profession, family size, geographical or physical characteristics. Since the linear time series plot has too many measurements all squeezed in this linear representation, it hinders us to discern any repetitive behavioral pattern for even one customers (let alone many customers together). In most cases, electricity data will have multiple seasonal patterns like daily, weekly or annual. We do not learn about these repetitive behaviors from the linear view. Hence we transition into looking at cyclic granularities, that can potentially provide more insight on their repetitive behavior.  


\noindent The aim of this section is to illustrate that the proposed methodology can be used to understand repetitive behavior for several customers together.


```{r raw-data-50, out.width = "100%", fig.cap="The raw data for 50 households are shown. It looks like there is a lot of missing values and unequal length of time series along with asynchronous periods for which data is observed. No insightful behavioral pattern could be discerned from this view other than when the customer is not at home."}

knitr::include_graphics("figs/raw_plot_cust.png") # look at smart-meter.R for the code
```


<!-- ### Missing Data -->

<!-- Electricity usage for some customers may become unavailable due to power outage or not recording their usage properly, thus resulting in implicit missing values in the database. It is interesting to explore where missing-ness occurs or if there is a relationship between the underlying missing patterns. We use the R package `tsibble` to do this.  -->

```{r missing-data, out.width="100%", eval = FALSE}
include_graphics("figs/missing-data.png")
```


<!-- - if there is any systematic missing patterns in the data -->
<!-- - this missing plot can go in the supplementary -->
<!-- - how to add missing values (should be added in data pre-processing) -->
<!-- - instance learning -->
<!-- - types of summary techniques to use ( -->
<!-- generally multivariate means and sd are used, I can't use that in a time series context, you can show across different granularity? -->
<!-- within-group sum of squares and between-group sum of squares -->

<!-- ) -->

<!-- 13735 customers in elec_ts -->
<!-- 8685 customers in elec_nogap -->
<!-- 5050 customers in count_na_df -->

<!-- Then is the graph of missing observations even interesting. -->
<!-- You can show two graphs, one to show that missingness do not have a pattern -->
<!-- another to show even if no missing, they start and end at different times. (A sample of 50 customers).  -->

<!-- \noindent A dataset of 100 SGSC homes has been used to lay out the structure to be used for analyzing the big dataset. The smaller dataset contains half-hourly kwh values form 2012 to 2014 and has asynchronous time series distributed evenly over the observation period (Figure \ref{fig:elec-raw}), similar to the bigger data set. Figure \ref{fig:count-gaps} can be used to interpret missingness in the data, where the customers are arranged from maximum to minimum missing. It looks like data is most missing before 2013 and for a particular date in 2014. -->



```{r miss-data}
empty_as_na <- function(x){
    if("factor" %in% class(x)) x <- as.character(x) ## since ifelse wont work with factors
    ifelse(as.character(x)!="", x, NA)
}

```


 <!-- - Handling autocorrelation: Autocorrelation in the time series is likely to get removed from considering probability distributions and cyclic granularities in the clustering algorithm. -->

<!-- At this stage, we need to define the aim of clustering as there could be various aims of clustering like between-cluster separation, within cluster homogeneity: low distances, within-cluster homogeneous distributional shape, good representation of data by centroids, little loss of information, high density without cluster gaps, uniform cluster sizes, stability and others. Finally, how distinct they are and how can we summarize the main features of the cluster would be discussed here. -->



## Prototype application

<!-- A clean data set is obtained by carefully choosing customers which shows similar shapes across one or more cyclic granularity. Since this is unlabeled data, there is no way to do external validation of our methodologies. Thus, we chose this way to see how well our methodology works in a cleaner data set as this one. -->


First, we select the customers which do not have any implicit missing values and filtered their data for the year 2013. From this set, we randomly sample a set of 600 customers. Then we removed customers for which any of the hod, moy and wkndwday is a clash. We further removed customers for which the deciles of the energy consumption is of the same value. Finally, we are left with 356 customers from which we wanted to do instance selection. Since this is unlabeled data, there is no way to do external validation of our methodologies. Thus, we chose this way to see how well our methodology works in a cleaner data set as this one to classify customers who have similar behavior across all significant granularities. Next we select 4 customers which are far apart from each other and 6 customers which are lying closest in distance to each of these 4 customers. These selection was done using the granularity hod. When we use our methodologies, it is based on all significant granularities and not only hod, but if hod is significant we can expect that the grouping would be similar to how we have initially chosen the set of customers. This is process is similar to instance-wise classfication methods.By this instancewise discriminative learning, instances can reasonably distribute in the representation space, which reveals their similarities. In
order to further improve visual representations, we propose a
dual-level progressive similar instance selection (DPSIS) method
to build a bridge from instance to class by selecting similar
instances (neighbors) for each instance (anchor) and treating the
anchor and its neighbors as the same class.




Fig \ref{fig:data-heatmap-hod}, \ref{fig:} and \ref{fig:} shows the distribution of 24 customers across hour-of-day, month-of-year and wknd-wday respectively. Every row in \ref{fig:} shows different shapes across hour-of-day and across columns show similar shapes for each row. We use our methodology to see if the customers are allocated to same group have similar shapes across one or more significant granularity.



```{r data-pick}

quantile_prob_graph <- c(0.25, 0.5, 0.75)

wkndwday <- read_rds(here("data/dist_gran_wkndwday_356cust_robust.rds")) %>% broom::tidy()

moy <- read_rds(here("data/dist_gran_moy_356cust.rds")) %>% broom::tidy()

hod <- read_rds(here("data/dist_gran_hod_356cust_robust.rds")) %>% broom::tidy()

distance <- wkndwday %>% 
  left_join(moy, by = c("item1", "item2")) %>% 
  left_join(hod, by = c("item1", "item2")) %>% 
  rename("wkndwday" ="distance.x",
         "moy" = "distance.y",
         "hod" = "distance") %>%
  mutate(item1 = as.integer(as.character(item1)),
         item2 = as.integer(as.character(item2))) 

total_distance <- distance %>% 
  mutate(total = hod)
  #mutate(total = wkndwday/2 + moy/12 + hod/24) 

  
# 8454221 10420689
data_pick_one <- total_distance %>% filter(item1 %in% 
                            c(8454221)) %>% 
  #group_by(item1) %>%
  arrange(total) %>% 
  head(5) %>% 
  mutate(item1 = as.integer(as.character(item1)),
         item2 = as.integer(as.character(item2)))

data_pick_ch <-  total_distance %>% filter(item1 %in% 
                            c(9393696)) %>% group_by(item1) %>% arrange(total) %>% head(5) %>% 
  mutate(item1 = as.integer(as.character(item1)),
         item2 = as.integer(as.character(item2)))



#11013154  8181071 11048034
data_pick_two <- total_distance %>% filter(item1 %in% 
                            c(8181071)) %>% group_by(item1) %>% arrange(total) %>% head(5)%>% 
  mutate(item1 = as.integer(as.character(item1)),
         item2 = as.integer(as.character(item2)))


data_pick_three <- total_distance %>% filter(item1 %in% 
                            c(11013154)) %>%
  #group_by(item1) %>%
  arrange(total) %>% head(5)%>% 
  mutate(item1 = as.integer(as.character(item1)),
         item2 = as.integer(as.character(item2)))

# data_pick_three <- total_distance %>% filter(item1 %in%
#                             c(11013154)) %>% group_by(item1) %>% arrange(total) %>% head(5)%>%
#   mutate(item1 = as.integer(as.character(item1)),
#          item2 = as.integer(as.character(item2)))

# 8653709
data_pick_four <- total_distance %>% filter(item1 %in%
                            c(8184707)) %>% 
  #group_by(item1) %>% 
  arrange(total) %>% head(5)%>%
  mutate(item1 = as.integer(as.character(item1)),
         item2 = as.integer(as.character(item2)))


# c(11013154, 8495194, 8627007,10109182,10677705,8952846)

data_pick_cust <- bind_rows(
unique(c(data_pick_one$item1,data_pick_one$item2)) %>% as_tibble(),    unique(c(data_pick_two$item1,data_pick_two$item2)) %>% as_tibble(), 
c(11013154, 9345642, 8328008, 8636035,8627007,8454235) %>% as_tibble(),
unique(c(data_pick_four$item1,data_pick_four$item2)) %>% as_tibble(),
.id = "design")

data_pick_cust$design <- paste("design", data_pick_cust$design, sep = "-")

data_pick <- read_rds(here::here("data/elec_nogap_2013_clean_356cust.rds"))%>%
  filter(customer_id %in% data_pick_cust$value) %>% 
  gracsr::scale_gran( method = "robust", response = "general_supply_kwh")


```


<!-- # ```{r} -->
<!-- # data_pick %>% ggplot() + geom_line(aes(x=reading_datetime, y = general_supply_kwh)) + -->
<!-- #   facet_wrap(~customer_id, ncol= 2) +  -->
<!-- # ``` -->


```{r clustering}
filtered_distance <- distance %>% filter(item1 %in% data_pick_cust$value) %>% 
  filter(item2 %in% data_pick_cust$value)


total_distance <- filtered_distance %>% 
mutate(total = wkndwday/2 + moy/12 + hod/24) 
#mutate(total = hod) 

total_distance_wide <- total_distance %>% pivot_wider(-c(2:5), 
                               names_from = item2,
                               values_from = total)
#total_distance_wide$`11013154` <- NA

rownames(total_distance_wide) <- total_distance_wide$item1

mds_data <- total_distance_wide %>% 
  mutate_all(~replace(., is.na(.), 0)) %>%
   tibble::rownames_to_column() %>%  
   select(-item1) %>% 
   pivot_longer(-rowname) %>% 
   pivot_wider(names_from=rowname, values_from=value) 

rownames(mds_data) <- total_distance_wide$item1

# group <- mds_data[-1] %>% clust_gran(kopt=4)
# 

df <- mds_data[-1] %>% as.matrix()
DM <- matrix(0, ncol(mds_data), ncol(mds_data))
DM[lower.tri(DM)] = df[lower.tri(df, diag=TRUE)]

f = as.dist(DM)

#f %>% hclust %>% cutree(k=4)


first_lot <- mds_data %>% names()
# because distance matrix is such  that 1:23 appears in columns and 2:24 appears in rows such that 1st customer is missing from row and last customer is missing from variables
id <- c(first_lot[-1], mds_data$name[nrow(mds_data)])

group <- f %>% hclust (method = "ward.D") %>% cutree(k=4)

cluster_result <- bind_cols(id = id, group = group) 

mds  = cmdscale(f)
rownames(mds) = id
colnames(mds) = c("mds1", "mds2")

# ggplot(data = mds %>% as_tibble() %>% mutate(group = cluster_result$group), 
#        aes(x = mds1, y = mds2, color = as.factor(group))) +
#   geom_point() +
#   geom_text(aes(label = id))

# cluster_result %>% filter(group==1)
# cluster_result %>% filter(group==2)
# cluster_result %>% filter(group==3)
# cluster_result %>% filter(group==4)

  #x = ~mds1.hod, y = ~ mds2.hod, text = ~customer_id
```
```{r heatmap-hod}

data_heatmap_hod <- quantile_gran(data_pick,
                                  "hour_day", quantile_prob_val = c(0.25, 0.5, 0.75)) %>% 
  pivot_wider(names_from = quantiles, values_from = quantiles_values) %>% 
  left_join(data_pick_cust, by = c("customer_id"="value"))
  
#data_heatmap_hod$customer_id = factor(data_heatmap_hod$customer_id, levels = data_pick_cust$value)
data_heatmap_hod$category <- factor(data_heatmap_hod$category, levels = 0:23)

```

```{r data-heatmap-hod, fig.cap="The distribution of electricity demand for the selected 24 customers across hour-of-day. The median is represented by a line and shaded region represents the area between 25^{th} and 75^{th} percentile. Each row represents a distinct shape of daily load, and all customers in the same row have similar daily profile. If they are represented by the same color, it means that they are in the same group as suggested by our proposed clustering algorithm."}

data_heatmap_hod %>% 
  mutate(customer_id = as.character(customer_id)) %>% 
  left_join(cluster_result, by = c("customer_id" = "id")) %>% 
  ggplot(aes(x = category)) + 
  geom_ribbon(aes(ymin = `25%`, 
                  ymax = `75%`,
                  group=customer_id,
                  fill = as.factor(group)),
              alpha = 0.5) +
  geom_line(aes(y = `50%`,
                group=customer_id, 
                color = as.factor(group)), size = 1) +
  facet_wrap(design~customer_id, 
             scales = "free_y", 
             labeller = "label_value",
             ncol = 6) +
    theme_bw() +
   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) +
    theme(strip.text = element_text(size = 10, margin = margin(b = 0, t = 0))) + xlab("hour-of-day") +
  ylab("demand (in Kwh)")  + 
  theme(panel.spacing =unit(0, "lines")) + 
  theme(axis.text.x = element_text(angle=90, hjust=1, size = 7)) + theme(legend.position = "bottom")+ 
  #theme() strip.text = element_text(size = 8, margin = margin(b = 0, t = 0)))+
  theme(plot.margin = margin(0, 0, 0, 0, "cm") ) +
  scale_x_discrete(breaks = seq(0,23, 4)) +
  scale_color_manual(values = c("#E69F00", "#009E73","#0072B2", "#D55E00")) +
  xlab("group")
```


```{r data-heatmap-hod-group, fig.cap="The distribution of electricity demand for the clusters across hour-of-day. The median is represented by a line and the shaded region represents the area between 25^{th} and 75^{th} percentile. The first cluster is characterised by high comsumption in early morning hours and late night hours. The second cluster should be the most common type of daily profile characterised by a smaller morning and higher evening peak. The third cluster corresponds to two equal peaks and the last one corresponds to only distinct evening peaks."}

data_group <- data_pick %>% 
  mutate(customer_id = as.character(customer_id)) %>% 
  left_join(cluster_result, by = c("customer_id" = "id"))

data_heatmap_hod_group <- quantile_gran(data_group,
                                  gran1="hour_day",
                                  quantile_prob_val = c(0.25, 0.5, 0.75),
                                  group="group") %>% 
  pivot_wider(names_from = quantiles, values_from = quantiles_values) 

  
data_heatmap_hod_group$category <- factor(data_heatmap_hod_group$category, levels = 0:23)

# data_heatmap_hod_group <- data_heatmap_hod_group %>%
#   mutate(group = case_when(group==1 ~"Group 2",
#                            group==2 ~"Group 4",
#                            group==3 ~"Group 1",
#                            group==4 ~"Group 3"))

# data_heatmap_hod_group$group <- factor(data_heatmap_hod_group$group, levels = c("Group 1", "Group 2", "Group 3", "Group 4"))

data_heatmap_hod_group$group <- paste("group", data_heatmap_hod_group$group, sep = "-")

data_heatmap_hod_group %>% 
  ggplot(aes(x = category)) + 
  geom_ribbon(aes(ymin = `25%`, 
                  ymax = `75%`,
                  group=group,
                  fill = as.factor(group)),
              alpha = 0.5) +
  geom_line(aes(y = `50%`,
                group=group, 
                color = as.factor(group)), size = 1)+
  facet_wrap(~group, 
             scales = "free_y",  nrow = 4) + 
              #labeller = labeller(xfacet = c(`1` = "Group 2", `2` = "Group 4",`3` = "Group 1",`4` = "Group 3"))
    theme(strip.text = element_text(size = 10, margin = margin(b = 0, t = 0))) + xlab("hour-of-day") + ylab("demand (in Kwh)") + 
  theme_bw() + 
  theme(panel.spacing =unit(0, "lines")) + theme(axis.text.x = element_text(angle=90, hjust=1, size = 7)) +
  scale_x_discrete(breaks = seq(1, 24, 3))+ 
  #theme(strip.text = element_text(size = 8, margin = margin(b = 0, t = 0)))+
  theme(plot.margin = margin(0, 0, 0, 0, "cm")) +
  scale_fill_manual(values = c("#0072B2", "#E69F00","#D55E00", "#009E73"))+
  #scale_color_manual(values = c("#0072B2", "#E69F00","#D55E00", "#009E73"))+
  theme(legend.position = "bottom")

# "#E69F00", "#009E73","#0072B2", "#D55E00"

# 
# data_pick_cust <- data_pick_cust %>% mutate(value = as.character(value))
# 
# data_pick_cust %>% left_join(cluster_result, by = c("value" = "id")) %>% group_by(group)
```

  
```{r data-heatmap-moy, fig.cap = "The distribution of electricity demand for the selected 24 customers across month-of-year. The median is represented by a line and shaded region represents the area between 25^{th} and 75^{th} percentile. The clustering leads to customers with similar shape across month-of-year with some exceptions. This is natural in a clus"}

data_heatmap_moy <- quantile_gran(data_pick, "month_year", quantile_prob_val = c(0.25, 0.5, 0.75)) %>% 
  pivot_wider(names_from = quantiles, values_from = quantiles_values) %>% 
  left_join(data_pick_cust, by = c("customer_id"="value"))
  
data_heatmap_moy$category <- factor(data_heatmap_moy$category, levels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))


data_heatmap_moy %>% 
  mutate(customer_id = as.character(customer_id)) %>% 
  left_join(cluster_result, by = c("customer_id" = "id")) %>% 
  ggplot(aes(x = category)) + 
  geom_ribbon(aes(ymin = `25%`, 
                  ymax = `75%`, group=customer_id), fill = "lightblue") +
  geom_line(aes(y = `50%`, group=customer_id, color = as.factor(group)), size = 1) +
  facet_wrap(design~customer_id, 
             scales = "free_y", 
             labeller = "label_value",
             ncol = 6) +
    theme(strip.text = element_text(size = 7, margin = margin(b = 0, t = 0))) + xlab("month-of-year") + ylab("demand (in Kwh)")  + theme_bw() + theme(panel.spacing =unit(0, "lines")) + theme(axis.text.x = element_text(angle=90, hjust=1, size = 7)) + theme(legend.position = "bottom")+ theme(
    strip.text = element_text(size = 8, margin = margin(b = 0, t = 0)))+
  theme(plot.margin = margin(0, 0, 0, 0, "cm") )
```


```{r data-heatmap-moy-group}

data_group <- data_pick %>% 
  mutate(customer_id = as.character(customer_id)) %>% 
  left_join(cluster_result, by = c("customer_id" = "id"))

data_heatmap_moy_group <- quantile_gran(data_group,
                                  gran1="month_year",
                                  quantile_prob_val = c(0.25, 0.5, 0.75),
                                  group="group") %>% 
  pivot_wider(names_from = quantiles, values_from = quantiles_values) 

data_heatmap_moy_group$category <- factor(data_heatmap_moy_group$category, levels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))

data_heatmap_moy_group %>% 
  ggplot(aes(x = category)) + 
  geom_ribbon(aes(ymin = `25%`, 
                  ymax = `75%`, group=group, fill = as.factor(group))) +
  geom_line(aes(y = `50%`, group=group)) +
  facet_wrap(~group, 
             scales = "free_y", 
             labeller = "label_value",
             nrow = 4) +
    theme(strip.text = element_text(size = 10, margin = margin(b = 0, t = 0))) + xlab("month-of-year") + ylab("demand (in Kwh)") + theme_bw() + theme(panel.spacing =unit(0, "lines")) + theme(axis.text.x = element_text(angle=90, hjust=1, size = 7)) + theme(
    strip.text = element_text(size = 8, margin = margin(b = 0, t = 0)))+
  theme(plot.margin = margin(0, 0, 0, 0, "cm") )

```
The clusters across month-of-year shows that cluster 2 uses electricity in summer.




```{r data-heatmap-wkndwday}

data_heatmap_wkndwday <- quantile_gran(data_pick, "wknd_wday", quantile_prob_val = c(0.25, 0.5, 0.75)) %>% 
  pivot_wider(names_from = quantiles, values_from = quantiles_values) %>% 
  left_join(data_pick_cust, by = c("customer_id"="value"))
  
data_heatmap_wkndwday$category <- factor(data_heatmap_wkndwday$category, levels = c("Weekday", "Weekend"))


data_heatmap_wkndwday %>% 
  mutate(customer_id = as.character(customer_id)) %>% 
  left_join(cluster_result, by = c("customer_id" = "id")) %>% 
  ggplot(aes(x = category)) + 
  geom_ribbon(aes(ymin = `25%`, 
                  ymax = `75%`, group=customer_id), fill = "lightblue") +
  geom_line(aes(y = `50%`, group=customer_id, color = as.factor(group)), size = 1) +
  facet_wrap(design~customer_id, 
             scales = "free_y", 
             labeller = "label_value",
             ncol = 6) +
    theme(strip.text = element_text(size = 7, margin = margin(b = 0, t = 0))) + xlab("wknd_wday") + ylab("demand (in Kwh)")  + theme_bw() + theme(panel.spacing =unit(0, "lines")) + theme(axis.text.x = element_text(angle=90, hjust=1, size = 7)) + theme(legend.position = "bottom")+ theme(
    strip.text = element_text(size = 8, margin = margin(b = 0, t = 0)))+
  theme(plot.margin = margin(0, 0, 0, 0, "cm") )


```

```{r data-heatmap-wkndwday-group}

data_group <- data_pick %>% 
  mutate(customer_id = as.character(customer_id)) %>% 
  left_join(cluster_result, by = c("customer_id" = "id"))

data_heatmap_wkndwday_group <- quantile_gran(data_group,
                                  gran1="wknd_wday",
                                  quantile_prob_val = c(0.25, 0.5, 0.75),
                                  group="group") %>% 
  pivot_wider(names_from = quantiles, values_from = quantiles_values) 

data_heatmap_wkndwday_group$category <- factor(data_heatmap_wkndwday_group$category, levels = c("Weekday", "Weekend"))


data_heatmap_wkndwday_group %>% 
  ggplot(aes(x = category)) + 
  geom_ribbon(aes(ymin = `25%`, 
                  ymax = `75%`, group=group, fill = as.factor(group))) +
  geom_line(aes(y = `50%`, group=group)) +
  facet_wrap(~group, 
             scales = "free_y", 
             labeller = "label_value",
             ncol = 4) +
    theme(strip.text = element_text(size = 10, margin = margin(b = 0, t = 0))) + xlab("wknd_wday") + ylab("demand (in Kwh)") + theme_bw() + theme(panel.spacing =unit(0, "lines")) + theme(axis.text.x = element_text(angle=90, hjust=1, size = 7)) + theme(
    strip.text = element_text(size = 8, margin = margin(b = 0, t = 0)))+
  theme(plot.margin = margin(0, 0, 0, 0, "cm") )

```





### Cluster validation

**Internal cluster validation** uses the internal information of the clustering process to evaluate the goodness of a clustering structure without reference to external information. It can be also used for estimating the number of clusters and the appropriate clustering algorithm without any external data.  

Generally most of the indices used for internal clustering validation combine compactness and separation measures.

<!-- _Distinction, repeatability, and robustness metrics_ -->

We will also determine if any identified clusters or patterns are indeed statistically meaningful in the sense that they actually exist and are not a random allocation. Hence, the robustness of this methodology is tested through simulations. This section will contain the data structure and detailed methodology to be employed for the cluster analysis. The cluster validation indexes like average silhouette width (ASW) is to be employed here to check how homogeneous these clusters are.

**External cluster validation** consists in comparing the results of a cluster analysis to an externally known result, such as externally provided class labels. It measures the extent to which cluster labels match externally supplied class labels. Since we know the “true” cluster number in advance, this approach is mainly used for selecting the right clustering algorithm for a specific data set.  

**Relative cluster validation** evaluates the clustering structure by varying different parameter values for the same algorithm (e.g.,: varying the number of clusters k). It’s generally used for determining the optimal number of clusters.  

Interval validation includes
_Compactness or cluster cohesion:_ Measures how close are the objects within the same cluster. A lower within-cluster variation is an indicator of a good compactness (i.e., a good clustering). The different indices for evaluating the compactness of clusters are base on distance measures such as the cluster-wise within average/median distances between observations.

_Separation:_ Measures how well-separated a cluster is from other clusters. The indices used as separation measures include:
distances between cluster centers
the pairwise minimum distances between objects in different clusters

_Connectivity:_ corresponds to what extent items are placed in the same cluster as their nearest neighbors in the data space. The connectivity has a value between 0 and infinity and should be minimized.


## Clustering results for 100 customers

## Clustering results for 5K customers

<!-- The robustness of this clustering method is provided through practical explanation of the formed clusters, visualizing how they relate to any weather, socio-economic or geographical conditions.  -->
<!-- Furthermore, we ensure that a cross-validation algorithm is employed leading to robust clusters.  -->
<!-- We will present how matching other data like geography, census and weather would help us in validating our clustering approach. -->

<!-- ### Geography -->

<!-- We will present how mapping the customers to their respective LGA's and reflect on the clusters obtained in the earlier section. The data is at SA2 and LGA levels. However, some of the LGAs in NSW changed between 2011 and 2016 and hence there would not be a one-to-one correspondence between the LGAs. -->

<!-- ### Census -->

<!-- We will present how mapping the customers to their respective income and population level reflect on the clusters obtained in the earlier section. The [ABS TableBuilder](https://www.abs.gov.au/websitedbs/censushome.nsf/home/tablebuilder) has census data from 2011 and 2016.  -->

<!-- ### Weather -->

<!-- Weather, notably temperature (and humidity) can be the main driver(s) for energy usage.  In NSW many households have electric heaters so their use can impact winter energy use and air-conditioners can impact summer energy use. Relevant weather data could be obtained from the [Bureau of Meteorology](www.bom.gov.au). Some weather stations have 30 minute (sometimes even smaller interval) weather data. Potentially, there could be lag effects of weather on energy usage which should be considered. -->

## Software implementation

The implementation for our framework is available in the R package [`gracsr`](https://github.com/Sayani07/gracsr) for ease of use in other applications.

## Discussion

This section will cover some drawback of this clustering method and potential extensions of this work.


<!-- <Earlier write up> -->
<!-- ## Introduction -->


<!-- <!-- Description of data available --> -->
<!-- The Smart Grid, Smart City (SGSC) project, which rolled out Australia's first commercial-scale smart grid was implemented across eight local government areas in New South Wales (NSW). Data from more than 13,000 household electricity smart meters is obtained as part of that project. It provides half-hourly energy usage and demographic data for Australia, as well as detailed information on appliance use, climate, retail and distributor product offers, and other related factors. The trials were based in Newcastle, New South Wales, but also covered areas in Sydney CBD, Newington, Ku-Ring-Gai, and the rural township of Scone. The load time series is asynchronous as it is observed for these households for unequal time lengths and consists of missing observations. -->

<!-- The massive amount of data generated in such projects could be overwhelming for analysis. Electricity utilities can utilize the consumption patterns of customers to develop targeted tariffs for individual groups and alleviate the problem of volatility in production by capitalizing on the flexibility of consumers. Beyea (2010) has pointed out, there has been little discussion or exploration of the full potential of these data bases and their benefits can reach beyond the original intentions for collecting these data. Thus, there is a scope to investigate and analyze these data in various ways for a greater understanding of consumption patterns and how they correlate with other economic, physical or geographical factors. In this work, we are interested to see how we can utilize this dataset to group different customers with similar periodic behavior. Towards this goal, this chapter aims to: (a) describe the contents of the data set in SGSC database that we can utilize, and (b) propose a clustering algorithm to group customers with similar periodic behaviors. The distance metric introduced in Chapter 2 will be the inputs for this cluster analysis. One of the advantages of using our  -->
<!-- approach is that the technique is based on probability distributions instead of raw data. Many clustering approaches are limited by the type of noisy, patchy, and unequal time-series common in residential data sets. Since the distance measure considered is based on differences in probability distribution of time series, it is likely to be less sensitive to missing or noisy data. -->



<!-- The following contributions are made through the following chapter: -->

<!--  * Present a cluster analysis of SGSC dataset to group households with similar periodic behavior -->
<!--  * Cluster validation by relating to external data -->






<!-- ## Introduction -->


<!-- In Australia, the federal government and fourteen partners, electricity distribution and transmission companies, technology companies, universities and CSIRO, invested AUD 490 million in the Smart Grid Smart City (SGSC) field trial. One of the largest commercial-scale smart grid technology assessment projects worldwide, SGSC is implemented across eight local government areas in New South Wales (NSW), covering 30,000 dwellings since 2009. -->
<!-- The SGSC database is publicly available through the Information Clearing House (ICH) (Ausgrid 2014). It is a massive database covering most physical aspects of the smart grid, as well as, aspects of users’ demographics. -->


<!-- Customer Sale and Half-Hour Power Consumption -->
<!-- and Generation Datasets -->

<!-- This is a big portion of the data from about 5900 households who accepted a -->
<!-- product out of about 24,000 offers made to the target population. Spatial resolution -->
<!-- down to anonymous individual dwellings [Customer Sales: Households: -->
<!-- Customer_Key] is also given based on {[Postcode], [Local Government Area], or -->
<!-- [Suburb Name]}. In order to make these datasets easily understandable, here we -->
<!-- present a new narrative approach to explain the available data fields and their -->
<!-- interdependencies in a brief yet logical and memorisable way as follows: -->

<!-- The [Half-Hour Consumption and Generation] data provide kWh readings of -->
<!-- the actual flow of energy. This includes the electricity supply imported from the -->
<!-- grid [General Supply] which is the main source of energy under any of the applied -->
<!-- tariffs, import during off-peak or for a control load [Off Peak], gross or net values of -->


<!-- domestic (often solar) electricity that is exported back to the grid [Generation]. The -->
<!-- [Peak Events] data field gives the details of every event [Event Key] including its -->
<!-- [Type], [Date], [Start Time], and [End Time]. Accordingly, the entire sequence of -->
<!-- user demand responses to any of the cost-reflective products is available in the -->
<!-- dataset to evaluate the project objectives as previously discussed. -->



<!-- Electricity smart meter technology is increasingly being deployed in residential and commercial buildings. For example, in the state of Victoria, Australia, it is a state government policy that all households have a smart meter fitted (Victoria State Government, 2015), which has resulted in 97% of the approximately 2.1 million households in Victoria having a smart meter installed in their home. These smart meters collect energy usage information at half-hourly intervals, resulting in over 35 billion half-hourly observations per year across all households in the state. Clustering electricity consumption patterns can enable electricity utilities to develop targeted tariffs for individual groups alleviating the problem of volatility in production by capitalizing on the flexibility of consumers. Clustering households using only smart meter consumption data should provide value in a societal setting by combining the findings and external data like weather conditions, socio-economic or other demographic factors of those households. -->

<!-- <!-- Description of data available --> -->
<!-- Data from more than 13,000 household electricity smart meters is obtained as part of the Smart Grid Smart City (SGSC) project (2010-2014). It provides one of the few linked sets of customer time of use (half-hour increments) and demographic data for Australia, as well as detailed information on appliance use, climate, retail and distributor product offers, and other related factors. The trials were based in Newcastle, New South Wales, but also covered areas in Sydney CBD, Newington, Ku-Ring-Gai, and the rural township of Scone. This data set includes electricity use interval readings for each customer for every 30 minutes and demographic data for Australia, as well as detailed information on appliance use, climate, peak events; peak events response and retail and distributor product offers. The load time series is asynchronous as it is observed for these households for unequal time lengths and consists of missing observations. -->


<!-- <!-- Background literature and shortcomings --> -->
<!-- The foundation for this study is Tureczek2017-pb, which conducts a systematic review of the current state of the art in smart meter data analytics. The paper evaluates approximately 2100 unique peer-reviewed papers and presents three main findings related to clustering methods, data and cluster validation. None of the 34 selected papers which focus on clustering consumption are based on Australian smart meter data. The clustering is frequently applied directly to the raw data without investigating time series features like auto correlation and periodicity. The application of smart meter data to cluster electricity consumption using K-Means is ubiquitous. But the omission of the time series structure or correlation in the analysis while employing K-Means leads to inefficient clusters. Principal Component Analysis or Self-Organizing Maps removes correlation structures and transitions the data to a reduced feature space, but it comes at a cost of interpretability of the final results. @Tureczek2018-ha has shown that the clever transformation of data before K-Means clustering can improve performance and enable K-Means. However, it does not explain the cluster composition by combining it with external data. We propose the following for the study of the Australian smart meter data. -->

<!-- Analysis for more than 13,000 household -->
<!-- electricity smart meters are included in this chapter. The following contributions are made through the following chapter: -->

<!-- - Presenting a cluster analysis of Australian residential electricity consumption data. -->

<!-- - Transformation and extraction of input data features enabling K-Means to account for other time series features in the clustering. This can easily be extended to include other data structures. -->

<!-- - Cluster validation by relating to external data -->


<!-- ## Preliminary exploratoration -->

<!-- This section familiarizes the 13,000 SGSC households through visualization and provide a detailed layout of data structures (like missing observations) and also the external data that needs to be utilized for validating the clustering process. -->

<!-- ## Common load clustering techniques of smart meter data -->

<!-- This section will discuss some literature on clustering techniques that were applied to smart meter customers in the past. -->

<!-- ## The proposed load-clustering technique -->

<!-- We employ a cluster analysis technique to obtain pockets of households showing similar periodic behavior by grouping probability distributions across a cyclic granularity. This clustering algorithm is adopted to remove or appropriately adjust for auto correlation and unequal length in the data. The method could be further extended by clustering probability distributions conditional on one or more cyclic granularities. Further, we elaborate on the effect of missing data and how it is handled in the clustering process. -->

<!-- ### Methodology, results, and discussions -->

<!-- This will contain the data structure and detailed methodology to be employed for the cluster analysis. -->

<!-- ### Distinction, repeatability, and robustness metrics -->

<!-- This section will cover the homogeneity and heterogeneity of the clusters, how distinct they are and summarize the main features of the cluster. -->

<!-- <!-- ## Comparison with traditional load clustering methods --> -->

<!-- <!-- This section will cover the homogeneity and heterogeneity of the clusters, how distinct they are and summarize the main features of the cluster. --> -->


<!-- ## Combining findings with external data -->

<!-- We will also determine if any identified clusters or patterns are indeed statistically meaningful in the sense that they actually exist and are not a random allocation. We aim to do this by providing a practical explanation of the formed clusters, visualizing how they relate to any weather, socio-economic or geographical conditions. Furthermore, we ensure that a cross-validation algorithm is employed leading to robust clusters.  -->
<!-- We will present how matching other data sources might help us in validating our clustering approach. -->

<!-- ### Geography -->

<!-- We will present how mapping the customers to their respective LGA's and reflect on the clusters obtained in the earlier section. -->

<!-- ### Census -->

<!-- We will present how mapping the customers to their respective income and population level reflect on the clusters obtained in the earlier section. -->


<!-- ## Conclusion -->

<!-- This section will cover some drawback of this clustering method and potential extensions of this work. -->

<!-- There are many cluster analysis methods that can produce quite different clusterings on the same dataset. Cluster validation is about the evaluation of the quality of a clustering; “relative cluster validation” is about using such criteria to compare clusterings. This can be used to select one of a set of clusterings from different methods, or from the same method ran with different parameters such as different numbers of clusters. There are many cluster validation indexes in the literature. Most of them attempt to measure the overall quality of a clustering by a single number, but this can be inappropriate. There are various different characteristics of a clustering that can be relevant in practice, depending on the aim of clustering, such as low within-cluster distances and high between-cluster separation. In this paper, a number of validation criteria will be introduced that refer to different desirable characteristics of a clustering, and that characterise a clustering in a multidimensional way. In specific applications the user may be interested in some of these criteria rather than others. A focus of the paper is on methodology to standardise the different characteristics so that users can aggregate them in a suitable way specifying weights for the various criteria that are relevant in the clustering application at hand. -->