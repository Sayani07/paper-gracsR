---
title:  Clustering time series based on probability distributions across temporal granularities

# to produce blinded version set to 1
blinded: 0

authors:
- name: Sayani Gupta
  affiliation: Department of Econometrics and Business Statistics, Monash University
  thanks: "Email: Sayani.Gupta@monash.edu"

- name: Rob J Hyndman
  affiliation: Department of Econometrics and Business Statistics, Monash University

- name: Dianne Cook
  affiliation: Department of Econometrics and Business Statistics, Monash University
  
keywords:
- data visualization
- statistical distributions
- time granularities
- calendar algebra
- periodic data
- grammar of graphics
- R

abstract: |
 With more and more time series data being collected at much finer temporal resolution, for a longer length of time, and for a larger number of individuals/entities, time series clustering research is getting a lot of traction. The sort of noisy, patchy, uneven, and asynchronous time series that is typical in many disciplines limits similarity searches among these lengthy time series. In this work, we suggest a method for overcoming these constraints by grouping time series based on probability distributions over cyclic temporal granularities. Cyclic granularities are temporal deconstructions of a time period into units such as hour-of-the-day, work-day/weekend, and so on, and can be helpful for detecting repeating patterns. Looking at probability distributions across cyclic granularities results in an approach that is robust to missing or noisy data, aids in dimension reduction, and ensures small pockets of similar repeated behaviours. The proposed method was tested using a collection of residential electricity customers. The simulated and empirical evidence demonstrates that our method is capable of producing meaningful clusters.
 
bibliography: [bibliography.bib]
preamble: >
  \setlength {\marginparwidth }{2cm}
  \usepackage{mathtools,amssymb,booktabs,longtable,todonotes,amsthm}
  \def\mod{~\text{mod}~}
output:
  bookdown::pdf_book:
    base_format: rticles::asa_article
    fig_height: 4
    fig_width: 6
    fig_caption: yes
    dev: "pdf" 
    keep_tex: yes
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache=TRUE, messages=FALSE, warning=FALSE)
# Make sure you have the latest version of rmarkdown and bookdown
#devtools::install_github("rstudio/rmarkdown")
#devtools::install_github("rstudio/bookdown")
library(ggplot2)
library(gravitas)
library(gracsr)
library(ggdendro)
library(dplyr)
library(readr)
library(visdat)
library(ggplot2)
library(tidyverse)
library(naniar)
library(here)
library(tsibble)
library(knitr)
library(patchwork)
library(GGally)
library(distributional)
library(viridis)
```

```{r external, include = FALSE, cache = FALSE}
knitr::read_chunk(here('script/smart_meter.R'))
```


```{r mytheme}
theme_validation <- function() {
  theme_bw() +
      theme(
    strip.text = element_text(size = 8, margin = margin(b = 0, t = 1)),
    plot.margin = margin(0, 0, 0, 0, "cm"),
    axis.title.y = element_blank(),
    #axis.text.x = element_blank(),
  axis.text.y = element_blank(),
  axis.ticks = element_blank(),
      legend.position = "bottom"
    )
}


theme_characterisation <- function() {
  
  theme_bw() + # seeting theme
    theme(strip.text = element_text(size = 10,
                                    margin = margin(b = 0, t = 0))) + # narrow facet space
   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) + # no axis ticks 
  theme(panel.spacing =unit(0, "lines")) +  # to ensure no gap between facets
    theme(axis.text.x = element_text(angle=90, hjust=1, size = 10)) + # rotate the x-axis text
    theme(legend.position = "bottom")+
  theme(plot.margin = margin(0, 0, 0, 0, "cm")) +
  theme(axis.text.x = element_text(size=5))
}
```


```{r mytheme-application}
theme_application <- function() {
  
  theme_light() + # setting theme
    #theme(strip.text = element_text(margin = margin(b = 0, t = 0))) + # narrow facet space
   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) + # no axis ticks 
  theme(panel.spacing =unit(0, "lines")) +  # to ensure no gap between facets
    #theme(axis.ticks.x = element_blank(),
     #   axis.text.x = element_blank()) + # no x-axis labels to further reduce the gap between facets
    #theme(axis.text.x = element_text(angle=90, hjust=1, size = 9)) + # rotate the x-axis text
  theme(plot.margin = margin(0, 0, 0, 0, "cm")) +
    theme(axis.text.x = element_text(angle=90, hjust=1, size = 10))+
      theme(plot.background = element_blank(), panel.grid = element_blank())+
  #theme(axis.text.x = element_text(size=5)) +
    theme(strip.background = element_blank(),
  strip.text.x = element_blank()) 
}
```

# Introduction

<!-- time series clustering and its challenges -->
Time-series clustering is the process of unsupervised partitioning of $n$ time-series data into $k$ ($k<n$) groups such that homogeneous time-series are grouped together based on a certain similarity measure. The time-series features, length of time-series, representation technique, and, of course, the purpose of clustering time-series all influence the suitable similarity measure or distance strategy to a meaningful level. The three primary methods to time series clustering (@liao2005clustering) are algorithms that operate directly with distances or raw data points in the time or frequency domain (distance-based), with features derived from raw data (feature-based), or indirectly with models constructed from raw data (model-based) (model-based). The efficacy of distance-based techniques is highly dependent on the distance measure utilised. Defining an appropriate distance measure for the raw time series may be a difficult task since it must take into account noise, variable lengths of time series, asynchronous time series, different scales, and missing data. Commonly used Distance-based similarity measures as suggested by a review of time series clustering approaches (@Aghabozorgi2015-ct) are Euclidean, Pearson's correlation coefficient and related distances, Dynamic Time Warping, Autocorrelation, Short time series distance, Piecewise regularisation, cross-correlation between time series, or a symmetric version of the Kullback–Liebler distances (@liao2007clustering). Euclidean distance and DTW are often used in time series clustering. When it comes to time-series clustering accuracy, the Euclidean distance beats DTW, but DTW has its own advantages (@corradini2001dynamic).Euclidean distance requires time series of equal length. while DTW can assist cluster time series of varying lengths (@ratanamahatana2005multimedia), only if there are no missing observations.

We are motivated by the residential smart meter data. These long time series are asynchronous, with varying time lengths for different houses and missing observations and characterised by noisy and patchy behavior that can quickly become overwhelming and hard to interpret, requiring summarizing the large number of customers into pockets of similar energy behavior. Choosing probability distributions instead of raw data seems to be a natural way to analyze these types of data sets. Hence this paper proposes a distance metric based on Jensen-Shannon distances between probability distributions across significant cyclic granularities. Cyclic temporal granularities, which are temporal deconstructions of a time period into units such as hour-of-the-day, work-day/weekend, can be useful for measuring repetitive patterns in large univariate time series data. Since cyclic granularities are considered instead of linear granularities, the resulting clusters are expected to group customers that have similar repetitive behaviors.  Below are some of the benefits of our method, which will be detailed in further depth in subsequent sections.

- Some clustering algorithms become problematic with the very high dimensionality of the time series resulting from the frequency at which they are recorded and the length of time for which they are observed. We can efficiently cluster long length time series by reducing dimensionality by characterising through probability distributions;

- By utilising Jensen-Shannon distances, we are evaluating the distance between two distributions rather than raw data, which is less susceptible to missing observations and outliers compared to other traditional distance measures;

- While most clustering algorithms produce clusters similar across just one temporal granularity, this technique takes a broader approach to the problem, attempting to group observations with similar forms across all key cyclic granularities. Because cyclic granularities are used rather than linear granularities, clustering would group consumers who exhibit similar repeating behaviour over many cyclic granularities where patterns are predicted to be important. 

- It is reasonable to define a time series based on its degree of trend and seasonality and to take these characteristics into account while clustering it. The change in data structure by considering probability distributions across cyclic granularities ensures there is no trend and seasonal fluctuations are handled separately. Thus there is no need to de-trend or de-seasonalize the data prior to performing the clustering method. For similar reasons, there is no need to exclude holiday or weekend routines.

<!-- . Often the objective is finding similarities in time (Fourier transforms, Piecewise aggregate approximation), in  shape (Dynamic time Warping (DTW)), in structure, based on global features. -->


<!-- However, the process of determining a similarity measure is challenging since time-series data are naturally noisy and include outliers and missing observations, and the length of time-series changes and can be asynchronous. -->


<!-- Time-series clustering is the most-used approach as an exploratory technique  for discovery of interesting patterns in time-series datasets. However, the process of finding similarity measure is complicated, because time-series data are naturally noisy and include outliers and shifts [18], at the other hand the length of time-series varies and the distance among them needs to be calculated. These common issues have made the -->
<!-- similarity measure a major challenge for data miners. -->


_Background and motivation_


<!-- Description of data available -->

        
<!-- significance of load clustering -->
Large spatio-temporal data sets, both from open and administrative sources, offer up a world of possibilities for research. <!--Beyea (2010) has pointed out, there has been little exploration of the full potential of these data bases and their benefits can reach beyond the original intentions for collecting these data.--> One such data sets for Australia is the Smart Grid, Smart City (SGSC) project (2010–2014) available through [Department of the Environment and Energy](https://data.gov.au/data/organization/doee)<!--and Data61 CSIRO-->. The project provides half-hourly data of over 13,000 household electricity smart meters distributed unevenly from October 2011 to March 2014. <!---The massive amount of data generated in such projects could be overwhelming for analysis.Raw data of these asynchronous time series can quickly become overwhelming and hard to interpret, requiring summarizing the large number of customers into pockets of similar energy behavior. Electricity utilities can utilize the smart meter usage patterns to develop targeted tariffs for individual groups and alleviate the problem of volatility in production by capitalizing on the flexibility of consumers. --><!-- Sources of variation in large data sets --><!-- The enormous quantity of data provides for greater individual level clarity and analysis. However, -->
<!-- due to the growing variety of consumers,  -->. Larger data sets include greater uncertainty about customer behavior due to growing variety of customers. Households vary in size, location, and amenities such as solar panels, central heating, and air conditioning. The behavioural patterns differ amongst customers due to many temporal dependencies. Some households use a dryer, while others dry their clothes on a line. Their weekly profile may reflect this. They may vary monthly, with some customers using more air conditioners or heaters than others, while having equivalent electrical equipment and weather circumstances. Some customers are night owls, while others are morning larks. Day-off energy use varies depending on whether customers stay home or go outside. Age, lifestyle, family composition, building attributes, weather, availability of diverse electrical equipment, among other factors, make the task of properly segmenting customers into comparable energy behaviour a fascinating one. This challenge is worsened when all we know about our consumers is their energy use history (@Ushakova2020-rl). To safeguard the customers' privacy, it is probable that such information is not accessible. Also, energy suppliers may not always update client information, such as property features, in a timely manner. Thus, there is a growing need to have research that examines how much energy usage heterogeneity can be found in smart meter data and what are some of the most common power consumption patterns, rather than explaining why consumption differs.


_Related work_

A multitude of papers have emerged around smart meter time series clustering for deepening our knowledge of consumption patterns. @Tureczek2017-pb conducted a systematic study of over $2100$ peer-reviewed papers on smart meter data analytics. None of the $34$ articles chosen for their emphasis use Australian smart meter data. The most often used algorithm is K-Means. Using K-Means without considering time series structure or correlation results in inefficient clusters. Principal Component Analysis (PCA) or Self-Organizing Maps (SOM) eliminate correlation patterns and decrease feature space, but lose interpretability. To reduce dimensionality, several studies use principal component analysis or factor analysis to pre-process smart-meter data before clustering (@Ndiaye2011-pf). Other algorithms utilised in the literature include k-means variants, hierarchical approaches, and greedy k-medoids. Time series data, such as smart metre data, are not well-suited to any of the techniques mentioned in @Tureczek2017-pb. Only one study [@ozawa2016determining] identified time series characteristics using Fourier transformation, which converts data from time to frequency and then uses K-Means to cluster by greatest frequency . @Motlagh2019-yj suggests that the time feature extraction islimited by the type of noisy, patchy, and unequal time-series common in residential datasets and addresses model-based clustering  by transforming the series into other other objects such as structure or set of parameters which can be more easily characterised and clustered. [@chicco2010renyi] addresses information theory-based clustering such as Shannon or Renyi entropy and its variations. @Melnykov2013-sp disucusses how outliers, noisy observations and scattered observations can complicate estimating mixture model parameters and hence the partitions.

\noindent Given the limitations of the similarity measures in dealing with large volumes of this complicated time series data, we present a similarity measure based on probability distributions that seems to be a more organic option for coping with time series data with aforementioned characteristics. The remainder of the paper is organized as follows: Section&nbsp;\ref{sec:methodology} provides the clustering methodology introducing the features and distance metrics. Section&nbsp;\ref{sec:validation} shows data designs to validate our methods and draw comparisons against several methods.Section&nbsp;\ref{sec:application} discusses the application of the method to a subset of the real data. Finally, we summarize our results and discuss possible future directions in Section&nbsp;\ref{sec:discussion}.



<!-- A typical clustering technique includes the following steps: (a) establishing distance (dissimilarity) and similarity through feature or model extraction and selection; and (b) selecting the clustering algorithm design. Distance measures could be time-domain based or frequency-domain (fast Fourier transform). -->



<!-- A model-based clustering works by transforming the series into other other objects such as structure or set of parameters which can be more easily characterised and clustered (@Motlagh2019-yj). [@chicco2010renyi] addresses information theory-based clustering such as Shannon or Renyi entropy and its variations. The essential temporal characteristics of the curves are defined or extracted using feature-based clustering. -->






<!-- This work -->
<!-- This is similar to a stochastic approach (@Motlagh2019-yj) to clustering, which proposes interpreting electricity demand as a random process and extracting time-series characteristics, or a model of the series, to enable unsupervised clustering. Unsupervised clustering is only as good as the features that are extracted/selected or the distance metrics that were utilized. Well-designed additional features may collect characteristics that default features cannot. Based on the underlying structure of the temporal data, this article offers new distance metric and features for clustering and applies them to actual smart-meter data. Firstly, the distance metric is based on probability distribution, which in our knowledge is the first attempt to cluster smart meter data using probability distributions. These recorded time series are asynchronous, with varying time lengths for different houses and missing observations. Taking probability distributions helps to deal with such data, while helping with dimension reduction in one hand but not losing too much information due to aggregation. Secondly, we recognise that most clustering algorithms only provide hourly energy profiles during the day, but this approach provides a wider approach to the issue, seeking to group consumers with similar shapes over all important cyclic granularities. Since cyclic granularities are considered instead of linear granularities, clustering would group customers that have similar repetitive behavior across more than one cyclic granularities across which patterns are expected to be significant.  -->


<!-- common similarity measures -->

<!-- electricity data structure -->

<!-- common similarity measures used there -->


<!-- lit review -->




# Clustering methodology {#sec:methodology}


<!-- In contrast to models, a feature-based strategy is used to explicitly define or automatically extract the curves’ key time features, for instance by application of PCA on the daily curves [ --


<!-- Most papers discussed in Tureczek2017-pb fail to accept smart meter readings as time series data, a data type which contains a temporal component. The omission of the essential time series features in the analysis leads to the application of methods that are not designed for handling temporal components. K-Means ignores autocorrelation, unless the input data is pre-processed. The clusters identified in the papers are validated by a variety of indices, with the most prevalent -->
<!-- being the cluster dispersion index (CDI) [22–24], the Davies–Bouldin index (DBI) [25,26] and the mean index adequacy (MIA) [8,13]. -->


<!-- The data set solely contains readings from smart meters and no information about the consumers' specific physical, geographical, or behavioural attributes. As a result, no attempt is made to explain why consumption varies. Instead, this work investigates how much energy usage heterogeneity can be found in smart meter data and what some of the most common electricity use patterns are. -->

<!-- Because of this importance, countless approaches to estimate time series similarity have been proposed.  -->

The proposed methodology aim to leverage the intrinsic temporal data structure hidden in time series data. The foundation of our method is unsupervised clustering algorithms based exclusively on the time-series data. The similarity measure is the most essential ingredient of time series clustering. The (dis) similarity measure in this paper focuses on looking at the (dis) similarity between underlying distributions that may have resulted in different patterns across different cyclic temporal granularities. It is worth noting that when studying these similarities, a variety of objectives may be pursued. One objective could be to group time series with similar shapes over all relevant cyclic granularities. In this scenario, the variation in customers within each group is in magnitude rather than shape, while the variation between groups is only in shape. There
are distance measures are used for shape-based clustering [Ding et al. 2008; Wang
et al. 2013] and many more but none of them look at the probability distributions while computing similarity. Moreover, most distance measures offer similar shape across just one dimension. For example, we often see "similar" daily energy profiles across hours of the day, but we suggest a broader approach to the problem, aiming to group consumers with similar distributional shape across all significant cyclic granularities. Another purpose of clustering could be to group customers that have similar differences in patterns across all major cyclic granularities, capturing similar jumps across categories regardless of the overall shape. For example, in the first goal, similar shapes across hours of the day will be grouped together, resulting in customers with similar behaviour across all hours of the day, whereas in the second goal, any similar big-enough jumps across hours of the day will be clubbed together, regardless of which hour of the day it is. Both of these objectives may be useful in a practical context and, depending on the data set, may or may not propose the same customer classification. Depending on the goal of clustering, the distance metric for defining similarity would be different. These distance metrics could be fed into a clustering algorithm to break large data sets into subgroups that can then be analyzed separately. These clusters may be commonly associated with real-world data segmentation. However, since the data is unlabeled a priori, more information is required to corroborate this. This section presents the work flow of the methodology: 


- _Data preparation_

@wang2020tsibble introduced the tidy "tsibble" data structure to support exploration and modeling of temporal data comprising of an index, optional key(s), and measured variables. <!-- An index is a variable with inherent ordering from past to present and a key is a set of variables that define observational units over time. A linear granularity is a mapping of the index set to subsets of the time domain. For example, if the index of a tsibble is days, then a linear granularity might be weeks, months or years.--> For each key variable, the raw smart meter data is a sequence that is indexed by time and comprises values of several measurement variables at each time point. This sequence, though, could be depicted in a variety of ways. A shuffling of the raw sequence could reflect the distribution of hourly consumption over a single day, while another could indicate consumption over a week or a year. These temporal deconstructions of a time period into units such as hour-of-day, work-day/weekend are called cyclic temporal granularities. All cyclic granularities can be expressed in terms of the index set and could be augmented with the initial tsibble structure (index, key, measurements). It is worthwhile to note that the data structure changes while transporting from linear to cyclic scale of time as multiple observations of the measured variable would correspond to each category of the cyclic granularities. In this paper, quantiles are chosen to characterize the distributions for each category of the cyclic granularity. So, each category of a cyclic granularity corresponds to a list of numbers which is essentially few chosen quantiles of the multiple observations.


- _Finding significant cyclic granularities or harmonies_

\noindent These cyclic granularities are useful for exploring repetitive patterns in time series data that get lost in the linear representation of time. It is advantageous to consider only those cyclic granularities across which there is a significant repetitive pattern for the majority of customers or noteworthy in an electricity-behavior context. In that case, when the customers are grouped, we can expect to observe some interesting patterns across the categories of the cyclic granularities considered. [@Gupta2021-hakear] proposes a way to select significant cyclic granularities and harmonies which is used for this paper.


- _Individual or combined categories of cyclic granularities as DGP_

The existing work on clustering probability distributions assumes we have an iid sample $f_1(v),\dots,f_n(v)$, where $f_i(v)$ denotes the distribution from observation $i$ over some random variable $v = \{v_t: t = 0, 1, 2, \dots, T-1\}$ observed across $T$ time points. In our work, we are using $i$ as denoting a customer and the underlying variable as the electricity demand. So $f_i(v)$ is the distribution of household $i$ and $v$ is electricity demand. In this work, instead of considering the probability distributions of the linear time series, we assume that the measured variables across different categories of any cyclic granularity are from different data generating processes.
Hence, we want to be able to cluster distributions of the form $f_{i,A,B \dots, {N_C}}(v)$, where $A, B$ represent the cyclic granularities under consideration such that $A = \{a_j: j=1, 2, \dots J\}$,  $B = \{b_k: k  = 1, 2, \dots K\}$ and so on. We consider individual category of a cyclic granularity ($A$) or combination of categories for interaction of cyclic granularities (for e.g. $A*B$) to have a distribution. For example, let us consider we have two cyclic  granularities of interest, $A = {0, 1, 2, \dots, 23}$ representing hour-of-day and  $B = \{Mon, Tue, Wed, \dots, Sun\}$ representing day-of-week. 
Each customer $i$ consist of a collection of probability distributions. In case individual granularities ($A$ or $B$) are considered, there are  $J = 24$ distributions of the form $f_{i,j}(v)$ or $K = 7$ distributions of the form $f_{i,k}(v)$ for each customer $i$. In case of interaction, $J*K=168$ distributions of the form $f_{i,j, k}(v)$ could be conceived for each customer $i$. 

\noindent As a result, a distance between collections of these univariate probability distributions is required. Depending on the objective of the problem, there could be many approaches to considering such distances. This paper considers two approaches, which are explained in the next segment.


- _Distance metrics_

Considering each individual or combined categories of cyclic granularities as a data generating process lead to a collection of conditional distributions for each customer $i$. The (dis) similarity between each pair of customers should be obtained by combining the distances between these collections of conditional distributions such that the resulting metric is a distance metric, which could be fed into the clustering algorithm. Two types of distance metric is considered:

<!-- The choice of distance measures is a critical step in clustering. It defines how the similarity of two elements (x, y) is calculated. -->

**JS-based distances**

This distance matrix considers two objects to be similar if every category of an individual cyclic granularity or combination of categories for interacting cyclic granularities have similar distributions. In this study, the distribution for each category is characterized using deciles and the distances between distributions are computed by using the Jensen-Shannon distance, which is symmetric and hence could be used as a distance measure. 


\noindent The total distance between two elements $x$ and $y$ is then defined as $$S^A_{x,y} = \sum_{j} D_{x,y}(A)$$ (sum of distances between each category $j$ of cyclic granularity A) or  $$S^{A*B}_{x,y} = \sum_j \sum_k D_{x,y}(A, B)$$ (sum of distances between each combination of categories $(j, k)$ of the harmony $(A, B)$. When combining distances from individual $L$ cyclic granularities $C_l$ with $n_l$ levels, $$S_{x, y} = \sum_lS^{C_l}_{x,y}/n_l$$ is used, which is also a distance metric being the sum of JS distances. 

**wpd-based distances**

Compute weighted pairwise distances ($wpd$) for all considered granularities for all objects. $wpd$ is designed to capture the maximum variation in the measured variable explained by an individual cyclic granularity or their interaction and is estimated by the maximum pairwise distances between consecutive categories normalized by appropriate parameters. A higher value of $wpd$ indicates that some interesting pattern is expected, whereas a lower value would indicate otherwise. 

\noindent Distance between elements is then taken as the euclidean distances between them with the granularities being the variables and $wpd$ being the value under each variable. Since Euclidean distance is chosen, the observations with high values of features ($wpd$ values) will be clustered together. The same holds true for observations with low values of features. Thus this distance matrix would be useful to group customers that have similar significance of patterns across different granularities.


<!-- Consider a harmony table consisting of many harmonies, each of the form $(A, B)$, such that $A = \{ a_j: j = 1, 2, \dots, J\}$ and $B = \{ b_k: k = 1, 2, \dots, K\}$. Each household consists of a $J*K$ distributions one harmony. We compute the distributional difference -->
<!-- between $(A, B)$ for the $s^{th}$ household using $wpd_{{s}}(A,B)$. -->
<!-- $wpd_{{s}}(A,B)$ denotes the normalized weighted-pairwise distributional distances between $(A, B)$ and is a feature which measures distributional difference between harmonies. If we have $H_{N_C}$ harmonies in the harmony table, then for each household we have a vector of $wpd_{{s}}$ of $H_{N_C}$ elements with each element corresponding to one harmony. We aim to have pockets of households showing similar periodic behavior by conidering $wpd$ vlaues for different harmonies and some time series features. The features should also characterize probability distributions of different household. -->


<!-- grouping probability distributions across a harmony. This clustering algorithm is adopted to remove or appropriately adjust for auto correlation and unequal length in the data. The method could be further extended by clustering probability distributions conditional on one or more cyclic granularities. The following are some of the advantages of our proposed method. -->



- _Pre-processing steps_

<!--Handling trend, seasonality, non-stationarity and auto-correlation:** Trend and seasonality are fundamental characteristics of time series data, and it is reasonable to define a time series according to its degree of trend and seasonality. These characteristics of the time series are lost or handled independently by considering probability distributions (trend is lost) across categories of cyclic granularities (by independently modeling all seasonal fluctuations), and so there is no need to de-trend or de-seasonalize the data before conducting the clustering method. There is no need to omit holiday or weekend patterns for similar reasons-->


Practically most problems will have a very skewed distribution, it is often helpful to bring them to a normal-like shape before clustering. Two data transformation techniques are employed for the JS-based methods and NQT is built-in transformation used for computation of $wpd$, which forms the basis of wpd-based distances.  
    
_Robust scaling_ Standardizing is a common scaling method that subtracts the mean from values and divides by the standard deviation, resulting in a conventional Gaussian probability distribution for an input variable (zero mean and unit variance). If the input variable includes outlier values, standardisation may become skewed or prejudiced. To address this, robust scaling methods could be utilized  (value – median) / (p75 – p25)) which results in a variable with a zero mean and median, as well as a standard deviation of one, while the outliers are still there with the same relative connections to other values.
    
_Normal-Quantile transform_  First as a data pre-processing step to make all assymetrical real world variables more symmetric, we perform a quantile-normal transform on the data. This makes sure that the CDF of the resulting variable in Gaussian. The original data is ranked in ascending order and the probabilities $P(Y<=y(i)) = i/(n+1)$ are attached to $y(i)$, in terms of their ranking order. A NQT based transformation is applied by computing from a standard normal distribution a variable $\eta(i)$, which corresponds to the same probability $P(\eta< \eta(i)) = i/n+1$. By doing this, the new variables $\eta(i)$ will be marginally distributed according to standard Normal, N(0,1). NQT will transform the positively and negatively skewed distribution to a similar bell-shaped. From the transformed distribution, it is difficult to understand that raw distribution was of which shape. Also, multimodality gets hidden or magnitude get reversed with NQT. But deciles from the distribution will move in a similar manner as the raw distribution and hence the final distance matrix seem to be unaffected. Hence, this could be used. 


- _Clustering algorithm_

In the analysis of energy smart meter data, K-Means or hierarchical clustering are often employed. These are simple and effective techniques that work well in a range of scenarios. For clustering, both employ a distance measure, and the distance measure chosen has a major influence on the structure of the clusters. We employ agglomerative hierarchical clustering in conjunction with Ward's criteria (XXX reference). The pair of clusters with minimum between-cluster distance are are sequentially merged in this using this agglomerative algorithms. A good comprehensive list of algorithms can be found in @.Xu2015-ja. We can possibly employ any clustering method that supports the given distance metric as input.


- _Characterization of clusters_

Characterization of clusters both statistically and qualitatively is an important stage of a cluster analysis. A potential way is to look at the findings from all the groups in graphs, and enhance our qualitative descriptions of the groupings. @Cook2007-qe provides several ways to characterize clusters.


(a) Parallel coordinate plot: Parallel coordinate plot (@wegman1990hyperdimensional) are widely used to display high-dimensional and multivariate data, allowing visual grouping to detect patterns. In a Parallel Coordinates Plot, each variable has its own axis, which are all parallel. Each axis is connected by a series of lines. That is, each line is made up of connected points on each axis. The order of the axes can affect how the reader interprets the data. This is because adjacent variables are more easily perceived than non-adjacent variables and rearranging the axes can reveal patterns or correlations between variables. Scattered plots of the p variables are arranged in a scatterplot matrix. It's a neat way to show multiple relationships at once, and it allows us to compare all the plots at once. 

(b) Scatterplot matrix: The scatter plot matrix (draftsman's plot) is a matrix that comprises pairwise scatter plots of the p variables.Pairwise scatter plots are excellent for determining relationships between variables and determining which factors have contributed the most to clustering.

(c) Plotting cluster statistics: For larger problems, parallel coordinate plots may become cluttered and difficult to read, therefore we may opt to display cluster statistics instead. (@Dasu_undated-gw)

(c) MDS, PCA and t-SNE: While all of the techniques examine a matrix of distances or dissimilarities to give a representation of the data points in a reduced-dimension space, their goals are not the same. The principal component analysis (Johnson & Wichern 2002) attempts to retain data variance. Multidimensional scaling (@borg2005modern) seeks to maintain the distances between pairs of data points, with an emphasis on pairings of distant points in the original space. t-SNE, on the other hand, is concerned with preserving neighbourhood data points. Close data points in high-dimensional space will be condensed in the t-SNE embeddings.

(d) Tour: A tour (@wickham2011tourr) is a collection of interpolated linear projections of multivariate data into a lower-dimensional space. The sequence is seen as a dynamic visualization, enabling the viewer to observe the shadows cast by the high-dimensional data in a lower-dimensional view.
<!-- and also provides examples on how one might spin and brush using a tour, to build up a cluster analysis manually. -->

Depending on the distance measure utilized for the study, the cluster characterization technique will differ. Clusters that utilize wpd-based distances are characterised using multi-dimensional scaling and parallel coordinate displays. For JS-based distances, the distribution across major granularities may be presented to ensure that the goal of similar shapes within clusters and distinct shapes across clusters is met. This technique may potentially make advantage of multi-dimensional scaling.


<!-- - A random sample  of the original data is taken for clustering analysis and includes missing and noisy observations (detailed description in Appendix) -->

<!-- - All harmonies are computed for each customer in the sample. -->
<!-- Cyclic granularities which are clashes for all customers in the sample are removed. -->

<!-- - It is worth noting that a number of other solutions may be considered at the pre-processing stage of the method. We have considered a) Normal-Quantile Transform and b) Robust transformation. -->

<!-- - Two methods are considered for computing dissimilarity between two customers. The first one involves computing according to one granularity is computed as the sum of the JS distances between distribution of all the categories of the granularity. When we consider more than one granularity, we consider the sum of the  average distances for all the granularity so that the combined metric is also a distance. -->

<!-- - Given the scale of dissimilarity among the energy readings, the model chooses optimal number of clusters -->

<!-- - Once clusters have been allocated, the groups are explored visually. -->

<!-- - Results are reported and compared. -->

<!-- Two methods are used for computing distances between subjects and then hierarchical clustering algorithm is used. -->

<!-- The existing work on clustering probability distributions assumes we have an iid sample $f_1(v),\dots,f_n(v)$, where $f_i(v)$ denotes the probability distribution from observation $i$ over some random variable $v = \{v_t: t = 0, 1, 2, \dots, T-1\}$ observed across $T$ time points. In our work, we are using $i$ as denoting a customer and the underlying variable as the electricity demand. So $f_i(v)$ is the distribution of household $i$ and $v$ is electricity demand. -->

<!-- We want to cluster distributions of the form $f_{i,j,k}(v)$, where $i$ and $j$ denote  -->

<!-- Consider a harmony table consisting of many harmonies, each of the form $(A, B)$, such that $A = \{ a_j: j = 1, 2, \dots, J\}$ and $B = \{ b_k: k = 1, 2, \dots, K\}$. Each household consists of a $J*K$ distributions one harmony. We compute the distributional difference -->
<!-- between $(A, B)$ for the $s^{th}$ household using $wpd_{{s}}(A,B)$. -->
<!-- $wpd_{{s}}(A,B)$ denotes the normalized weighted-pairwise distributional distances between $(A, B)$ and is a feature which measures distributional difference between harmonies. If we have $H_{N_C}$ harmonies in the harmony table, then for each household we have a vector of $wpd_{{s}}$ of $H_{N_C}$ elements with each element corresponding to one harmony. We aim to have pockets of households showing similar periodic behavior by conidering $wpd$ vlaues for different harmonies and some time series features. The features should also characterize probability distributions of different household. -->


<!-- ### Notations -->

<!-- Consider an iid sample $f_1(v),\dots,f_n(v)$, where $f_i(v)$ denotes the probability distribution from observation $i$ over some random variable $v = \{v_t: t = 0, 1, 2, \dots, T-1\}$ observed across $T$ time points. In our work, we are using $i$ as denoting a household and the underlying variable as the electricity demand. Further consider a cyclic granularity of the form $B = \{ b_k: k = 1, 2, \dots, K\}$. Each customer consists of collection of probability distributions. -->


<!-- So $f_i(v)$ is the distribution of household $i$ and $v$ is electricity demand. We want to cluster distributions of the form $f_{i,j,k}(v)$, where $i$ and $j$ denote $i^{th}$ and $j^{th}$ customer respectively. -->



<!-- a harmony table consisting of many harmonies, each of the form $(A, B)$, such that $A = \{ a_j: j = 1, 2, \dots, J\}$ and $B = \{ b_k: k = 1, 2, \dots, K\}$.  -->




<!-- ### A single or pair of granularities together (change names) -->

<!-- The methodology can be summarized in the following steps: -->

<!-- - _Pre-processing step_ -->

<!-- Robust scaling method or NQT used for each customer. -->


<!-- - _NQT_ -->


<!-- - _Treatment to outliers_ -->






<!-- ### Many granularities together (change names) -->

<!-- The methodology can be summarized in the following steps: -->


<!-- 1. Compute quantiles of distributions across each category of the cyclic granularity -->
<!-- 2. Compute JS distance between households for each each category of the cyclic granularity -->
<!-- 3. Total distance between households computed as sum of JS distances for all hours -->
<!-- 4. Cluster using this distance with hierarchical clustering algorithm (method "Ward.D") -->

<!-- _Pro:_   -->
<!-- - distance metric makes sense to group different shapes together   -->
<!-- - simulation results look great on typical designs   -->
<!-- _Cons:_   -->
<!-- - Can only take one granularity at once   -->
<!-- - Clustering a big blob of points together whereas the aim is to groups these big blob into smaller ones   -->

<!-- ### Multiple-granularities -->

<!-- _Description:_   -->

<!-- Choose all significant granularities and compute wpd for all these granularities for all customers. Distance between customers is taken as the euclidean distances between them with the granularities being the variables and wpd being the value under each variable for which Euclidean distance needs to be measured.   -->
<!-- _Pro:_   -->
<!-- - Can only take many granularities at once -->
<!-- - can apply variable selection PCP and other interesting clustering techniques -->
<!-- - simulation results look great on typical designs -->
<!-- - splitting the data into similar sized groups   -->
<!-- _Cons:_   -->
<!-- - distance metric does not make sense to split the data into similar shaped clusters  -->

# Validation {#sec:validation}

To validate the clustering approaches, we create data designs that replicate prototype behaviors that might be seen in electricity data contexts. We spiked several attributes in the data to see where one method works better than the other and where they might give us the same outcome or the effect of missing data <!---and trends--> on the proposed methods. Three circular granularities $g1$, $g2$ and $g3$ are considered with categories denoted by ${g10,g11}$, ${g20, g21, g22}$ and ${g30, g31, g32, g33, g34}$ and levels $l_{g_1}=2$, $l_{g_2}=3$ and $l_{g_3}=5$. These categories could be integers or some more meaningful labels. For example, the granularity "day-of-week" could be either represented by ${0, 1, 2, \dots, 6}$ or ${Mon, Tue, \dots, Sun}$. Here categories of $g1$, $g2$ and $g3$ are represented by  $\{0, 1\}$, $\{0, 1, 2\}$ and $\{0, 1, 2, 3, 4\}$ respectively. A continuous measured variable $v$ of length $T$ indexed by $\{0, 1, \dots T-1\}$ is simulated such that it follows the structure across $g1$, $g2$ and $g3$. We created independent replications $R = \{25, 250, 500\}$ of all data designs to see if our proposed clustering approaches can detect distinct designs in various groups for small, medium and large number of series. A sample size of $T=\{300, 1000, 5000\}$ is used in all designs to test small, medium and large sized series. The methods could preform differently with different jumps between consecutive categories. So a mean difference of $diff = \{1, 2, 5\}$ for corresponding categories are also considered. The performance of the methods can vary with different number of significant granularities. So scenarios with all, few and just one significant granularities are considered. The code for creating these designs and the detailed results can be found in the Supplementary section (link to github repo). 


<!-- The results for $T=300$ and $R=25$ is shown, that means we have $25$ time series each with length $300$. The rest of the results could be found in the supplementary paper. -->


## Data generating processes

<!-- An ARMA (p,q) process is used to generate series, where $p$ and $q$ are selected at random such that the series is stationary. The various designs on $g1$, $g2$, and $g3$ are introduced by adding matching designs to this series' innovations. The innovations are considered to have a normal distribution, although they follow the same pattern as the designs. To eliminate the effect of starting values, the first 500 observations in each series are discarded. -->

Each category or combination of categories from $g1$, $g2$ and $g3$ are assumed to come from the same distribution, a subset of them from the same distribution, a subset of them from separate distributions, or all from different distributions, resulting in various data designs. As the methods ignore the linear progression of time, there is little value in adding time dependency in the data generating process. It is often reasonable to construct a time series using properties such as trend, seasonality, and auto-correlation. However, when examining distributions across categories of cyclic granularities, these time series features are lost or addressed independently by considering seasonal fluctuations through cyclic granularities. Because the time span during which an entity is observed in order to ascertain its behavior is not very long,
the behavior of the entity will not change drastically and hence the time series can be assumed to remain stationary throughout the observation period. If the observation period is very long (for e.g more than 3 years), property, physical or geographical attributes might change leading to a non-stationary time series. But such a scenario is not considered here and the resulting clusters are assumed to be time invariant in the observation period. The data type is set to be "continuous," and the setup is assumed to be Gaussian. When the distribution of a granularity is "fixed", it means distributions across categories do not vary and are considered to be from N (0,1). The mean of different categories are altered in the "varying" designs, leading to varying distributions across categories.


<!-- An ARMA (p,q) process is used to generate series, where $p$ and $q$ are selected at random such that the series is stationary. The various designs on $g1$, $g2$, and $g3$ are introduced by adding matching designs to this series' innovations. The innovations are considered to have a normal distribution, although they follow the same pattern as the designs. To eliminate the effect of starting values, the first 500 observations in each series are discarded. -->


## Data designs  

### Individual granularities


  _Scenario (a): All signifiant granularities_

<!-- Consider a case where all the three granularities $g1$, $g2$ and $g3$ would be responsible for making the designs distinct. That would mean, the pattern for each of $g1$, $g2$ and $g3$ will change for at least one design. We consider a situation with all the null cases corresponding to no difference in distribution across categories, that is, all categories follow the same distribution N(0,1). -->

\noindent Consider the scenario when all three granularities $g1$, $g2$, and $g3$ are responsible for distinguishing the designs. This implies that the patterns across each granularity will change significantly for at least one among the to-be-grouped designs. We consider different distributions across categories (as in Table \ref{tab:tab-dist-design} top) that will lead to different designs (as in Table \ref{tab:tab-dist-design} below). Figure \ref{fig:plot-3gran-new} shows the linear and cyclic representation of the simulated variable under these five designs. As could be seen from the plot, it is impossible to decipher the structural difference in the time series variable just by looking at the linear view. The difference in structure becomes quite clear when we see the distribution across cyclic granularities. Hence, for the consequent scenarios, only graphical displays across cyclic granularities are provided to emphasize the difference in structure.

_Scenario (b): Few significant granularities_

\noindent This is the case where one granularity will remain the same across all designs. We consider the case where the distribution of $v$ would vary across levels of $g2$ for all designs, across levels of $g1$ for few designs and $g3$ does not change across designs. So $g3$ is not responsible for distinguishing across designs. Figure
\ref{#fig:plot-linear}(left) shows the considered design. 

_(c) One signifiant granularity_

\noindent Here only one granularity is responsible for distinguishing the designs. Designs change significantly only for the granularity $g3$. Figure \ref{fig:plot-1gran}(right) shows this.


```{r tab-distribution}
```


```{r tab-design}
```


```{r tab-dist-design}
```



```{r generate-design-3change}

```

```{r plot-3gran-new, out.width="100%", fig.cap="The linear (left) and cyclic (right) representation of the measured variable is shown. In this scenario, all of $g1$, $g2$ and $g3$ changes across at least one design. Also, it is not possible to comprehend these patterns across cyclic granularities or group similar series just by looking at the linear plots."}

```


```{r generate-design-new}

```


```{r generate-design-2gran-data}

```


```{r generate-design-2gran-plot}

```


```{r generate-design-1gran-data}

```


```{r generate-design-1gran-plot}

```


```{r gran2and1-clubbed, fig.cap=" For the left scenario $g1$, $g2$ would change across atleast one design but $g3$ change remains same across all design. For the right one, only $g3$ changes across different designs."}
#gran2_change  + gran1_change + plot_layout(widths = c(1,1))
#ggpubr::ggarrange(gran1_change, gran2_change, labels = c("a", "b"))
```

### Interaction of granularities

The proposed methods could be extended when two granularities of interest interact and we are interested to group subjects based on the interaction of the two granularities. For example, consider a group having a different weekday, weekend behavior in summer months, but not across winter. This type of joint behavior across granularities wknd-wday and month-of-year can be discovered by examining the distribution across combination of categories for different interacting granularities. Hence, in this scenario, we consider combination of categories to be generated from different distributions. For simplicity, consider a case with just two interacting granularities $g1$ and $g2$ of interest. As opposed to the last case, where we could examine distributions across $l_{g_1} + l_{g_2} = 5$ individual categories, with interaction, we need to examine the distribution of $l_{g_1}*l_{g_2}=6$ combination of categories. Consider $4$ designs in Figure \ref{fig:interaction-gran} where different distributions are assumed for different combination of categories resulting in different designs. Design-1 has no change in distributions across $g1$ or $g2$, while Design-2 and Design-3 change across only $g1$ and $g2$ respectively. Design-4 changes across categories of both $g1$ and $g2$. Design-3 and Design-4 looks similar according to their relative difference between consecutive categories, but Design-4 also changes across facets, unlike Design-3 where all facets look the same.


```{r interaction-gran, fig.cap = "Design-1 (a) has no change in distributions across different categories of $g1$ or $g2$, while Design-2 (b) and Design-3 (c) change across only $g1$ and $g2$ respectively. Design-4 (d) changes across categories of both $g1$ and $g2$."}

```



<!-- When two granularities of interest interact, the connection between a granularity and the measured variable is determined by the value of the other interacting granularity. This happens when the effects of the two granularities on the measured variable are not additive. For simplicity, consider a case with just two interacting granularities $g1$ and $g2$ of interest. As opposed to the last case, where we could play with the distribution of $5$ individual categories, with interaction we can play with the distribution of $6$ combination of categories. Consider $4$ designs in Figure \ref{fig:} where different distributions are assumed for different designs to get some distinction across designs. For example, in application, think about the scenario when customers need to grouped basis their joint behavior across hour-of-day and month-of-year. -->

<!-- | Granularity type                                                   	| # Significant 	| # Replications 	| -->
<!-- |--------------------------------------------------------------------	|---------------	|----------------	| -->
<!-- | **Individual**  <br><br># obs: 300, 500, 2000  <br># clusters: 4/5 	| 1/2/3         	| 25, 100, 200   	| -->
<!-- | **Interaction**  <br><br># obs: 500, 2000  <br># clusters: 4       	| 2           	| 25, 100, 200   	| -->


## Results

All the methods were fitted to each data designs and results are reported through confusion matrices. With increasing difference between categories, it gets easier for the methods to correctly distinguish the designs. For $mean_diff=1$, the performances are pretty bad for js-robust methods and wpd method for lower nT. Although, with the kind of residential load datasets, a full year of load is the minimal requirement to capture expected variations in winter and summer profiles, for example. It is likely that $nT$ would be at least $1000$ with half-hourly data, even if data is only available just for a month. The performance is promising except when the number of observations for a customer is really small. For smaller difference between categories, it is expected that method js-nqt would preform better than the other two.
<!-- A confusion table can come alive with linked brushing, so that mismatches and agreements between methods can be explored. -->


```{r, eval=FALSE}
# code in append_3gran_change.R
table <- read_rds("data/append_3gran_change.rds")
table %>% kable()
```


<!-- starts getting better with increasing difference and get worse with increasing number of replications. Length of series do not show to have any effect on the performance of the methods. It does not depend on if time series is ar or arma. -->


<!-- - confusion matrix could be used for showing results if proper labeling is used -->

<!-- - write about features that we have spiked into the data set -->
<!-- - write about you incorporated noise -->
<!-- - What is the additional structure you can incorporate that will lead to failing of method1 and method2? -->
<!-- - And both gives the same result? Basically say when method 1 works better than method 2 and vice versa! -->

<!-- - -->




# Application {#sec:application}

The use of our methodology is illustrated on smart meter energy usage for a sample of customers from [SGSC consumer trial data](https://data.gov.au/data/dataset/smart-grid-smart-city-customer-trial-data) which was available through [Department of the Environment and Energy](https://data.gov.au/data/organization/doee) and Data61 CSIRO. It contains half-hourly general supply in KwH for 13,735 customers, resulting in 344,518,791 observations in total. It also provides demographic data for these customers most of which are missing and not utilized for the purpose of this paper. To maintain anonymity, the energy patterns could not be recognised at the person level, but rather by the geographical location of their dwelling and information about their Local Government Area.


<!-- ## Data source -->

<!-- The entire data is procured from CSIRO. A subset of this data is also available from [SGSC consumer trial data](https://data.gov.au/data/dataset/smart-grid-smart-city-customer-trial-data) is available through [Department of the Environment and Energy](https://data.gov.au/data/organization/doee). It consists of the following data sets.  -->
<!-- _1. CustomerData:_ 78720 customers with 62 variables about them  -->
<!-- _2. EUDMData:_ 300 billion half-hourly consumption level data   -->
<!-- _3. OffersData:_ Method of contact to customer to join SGSC customer trial, either door-to-door (D2D) or via Telesales   -->
<!-- _4. PEResponseData:_ Peak Events response customer wise   -->
<!-- _5. PETimesData:_ Peak Events time stamps   -->

<!-- Only _CustomerData_ and _EUDMData_ are relevant for the clustering goals of this paper. _EUDMData_ contains half-hourly general supply in KwH for 13,735 customers, resulting in 344,518,791 observations in total. `CustomerData` provides demographic data for 78,720 customers most of which are missing and not utilized for the purpose of this paper. To meet the requirements for anonymity preservation, the energy patterns could not be identified at the individual level, but rather by the geographical location of their residence information about their Local Government Area. -->

<!-- ## Characteristics of raw data -->

In Figure \ref{fig:raw-data-50}, the time series of energy consumption is plotted along the y-axis against time from past to future for $50$ sampled households. Each of these series correspond to a single customer. For each customer, the energy consumption is available at fine temporal resolution (every 30 minutes) for a long period of time (~ 2 years). Some customers' electricity use may be unavailable owing to power outages or improper recording, resulting in implied missing numbers in the database. For this data set it was found that out of 13,735 customers in total, 8,685 customers do not have any implicit missing observations, while the rest 5,050 customers had missing values. With further exploration, it was found that there is no structure in the missing-ness, that is missing observations can occur at any time point (see Appendix). Moreover, the data for these customers are characterized by unequal length, different start and end dates. Since our proposed methods consider probability distribution instead of raw data, both of these characteristics would not pose any threat to our methodology unless of course there is any structure or systematic patterns in them.  


\noindent It can be expected that energy consumption vary substantially between customers, which is a reflection of their varied  behavior owing to differences in profession, family size, geographical or physical characteristics. Since the linear time series plot has too many measurements all squeezed in this linear representation, it hinders us to discern any repetitive behavioral pattern for even one customers (let alone many customers together). In most cases, electricity data will have multiple seasonal patterns like daily, weekly or annual. We do not learn about these repetitive behaviors from the linear view. Hence we transition into looking at cyclic granularities, that can potentially provide more insight on their repetitive behavior.    




<!-- ### Missing Data -->

<!-- Electricity usage for some customers may become unavailable due to power outage or not recording their usage properly, thus resulting in implicit missing values in the database. It is interesting to explore where missing-ness occurs or if there is a relationship between the underlying missing patterns. We use the R package `tsibble` to do this.  -->

```{r missing-data, out.width="100%", eval = FALSE}
include_graphics("figs/missing-data.png")
```


<!-- - if there is any systematic missing patterns in the data -->
<!-- - this missing plot can go in the supplementary -->
<!-- - how to add missing values (should be added in data pre-processing) -->
<!-- - instance learning -->
<!-- - types of summary techniques to use ( -->
<!-- generally multivariate means and sd are used, I can't use that in a time series context, you can show across different granularity? -->
<!-- within-group sum of squares and between-group sum of squares -->

<!-- ) -->

<!-- 13735 customers in elec_ts -->
<!-- 8685 customers in elec_nogap -->
<!-- 5050 customers in count_na_df -->

<!-- Then is the graph of missing observations even interesting. -->
<!-- You can show two graphs, one to show that missingness do not have a pattern -->
<!-- another to show even if no missing, they start and end at different times. (A sample of 50 customers).  -->

<!-- \noindent A dataset of 100 SGSC homes has been used to lay out the structure to be used for analyzing the big dataset. The smaller dataset contains half-hourly kwh values form 2012 to 2014 and has asynchronous time series distributed evenly over the observation period (Figure \ref{fig:elec-raw}), similar to the bigger data set. Figure \ref{fig:count-gaps} can be used to interpret missingness in the data, where the customers are arranged from maximum to minimum missing. It looks like data is most missing before 2013 and for a particular date in 2014. -->



```{r miss-data}
empty_as_na <- function(x){
    if("factor" %in% class(x)) x <- as.character(x) ## since ifelse wont work with factors
    ifelse(as.character(x)!="", x, NA)
}

```


 <!-- - Handling autocorrelation: Autocorrelation in the time series is likely to get removed from considering probability distributions and cyclic granularities in the clustering algorithm. -->

<!-- At this stage, we need to define the aim of clustering as there could be various aims of clustering like between-cluster separation, within cluster homogeneity: low distances, within-cluster homogeneous distributional shape, good representation of data by centroids, little loss of information, high density without cluster gaps, uniform cluster sizes, stability and others. Finally, how distinct they are and how can we summarize the main features of the cluster would be discussed here. -->



## Prototype selection

<!-- A clean data set is obtained by carefully choosing customers which shows similar shapes across one or more cyclic granularity. Since this is unlabeled data, there is no way to do external validation of our methodologies. Thus, we chose this way to see how well our methodology works in a cleaner data set as this one. -->


<!-- Why instance selection -->
In supervised learning, a training set containing previously known information is used to categorize new occurrences. Acceptable classification rates may be obtained by discarding instances which are not helpful for classification; this process is known as instance selection (@olvera2010review). This is similar to subsetting the population along all dimensions of importance such that the sampled data is representative of the main characteristics of the underlying distribution. Instance selection in unsupervised learning has received limited attention in the literature, but could serve as an useful way to sample evaluation data set to measure the performance of a model or method. One such procedure is suggested in @Fan2021-bq that selects similar instances (neighbors) for each instance (anchor) and treats the anchor and its neighbors as the same class. In this section, a similar idea is used to select customers with prototype behaviors
that serves as evaluation data sets for our proposed methodology.


_Pre-processing steps_

P1. We randomly select a sample of $600$ customers which do not have any implicit missing values and filtered their data for the year 2013.

P2. Obtain $wpd$ for all cyclic granularities considered for these customers. It was found that `hod` (hour-of-day), `moy` (month-of-year) and `wkndwday` (weeknd/weekday) are coming out to be significant for most customers. This implies that for most customers, there is some interesting pattern across these three granularities.

P3. Remove customers from this list for which data in any category for the significant granularities are empty. For example, in this data set, if a customer do not have data for an entire month, they have been removed as their monthly behavior could not be studied in that case.

P4. Remove customers for which all the deciles of the energy consumption is zero. These are the customers whose consumption remain mostly flat and is expected to have no interesting repetitive patterns that is our interest of study.

Finally, we are left with $356$ customers from which we run our prototype search. There are different methods of going ahead with this. For example, one approach could be to use any Non-linear dimensionality reduction technique like MDS or PCA and project the data in a 2-dimensional space. One can then look at few "anchor" customers which are far apart in the 2D space and pick few neighbors for each of the anchor customers. Paradoxically, the curse of dimensionality inverts for dimension reduction, resulting in an excessive amount of observations near the center of the distribution. This affects visualizations made on low-dimensional projections. [Ursula's paper] explains why points tend to be away from the center in the high-dimensional space, but crowd the center in low-dimensional projections, it is helpful to consider the projected volume relative to high-dimensional volume. 




```{r}
quantile_prob_graph <- c(0.25, 0.5, 0.75)

# data_pick_one <- c(8618759, 8291696, 10357256, 8290374) %>% as_tibble %>% set_names("customer_id")
# data_pick_two <- c(9044864, 8642053, 10534367, 9021526,11162275) %>% as_tibble %>% set_names("customer_id")
# data_pick_three <- c(8221762, 8273636, 10359424, 8232822)%>% as_tibble %>% set_names("customer_id")


data_pick_one <- c(8541744, 9355808, 8603880, 8619309, 10542667) %>% as_tibble %>% set_names("customer_id")
#data_pick_two <- c(8688242, 8643837, 8184707, 10534355, 8684420) %>% as_tibble%>% set_names("customer_id")
data_pick_three <- c(9792072, 8589936, 8454235, 10692366, 8603828)%>% as_tibble%>% set_names("customer_id")
data_pick_four <- c(8618759, 8291696, 10357256, 8290374) %>% as_tibble %>% set_names("customer_id")
data_pick_five <- c(9044864, 8642053, 10534367, 9021526,11162275) %>% as_tibble %>% set_names("customer_id")
data_pick_six <- c(8221762, 8273636, 10359424, 8232822, 11450499)%>% as_tibble %>% set_names("customer_id")


```

```{r assemble}
data_pick_cust <- bind_rows(
data_pick_one, 
#data_pick_two, 
data_pick_three,
data_pick_four,
data_pick_five, 
data_pick_six,
.id = "design") %>% 
  mutate(customer_id = as.character(customer_id))

```



```{r}
data_pick <- read_rds(here::here("data/elec_nogap_2013_clean_356cust.rds")) %>%
  mutate(customer_id = as.character(customer_id)) %>% 
  dplyr::filter(customer_id %in% data_pick_cust$customer_id) %>% 
  gracsr::scale_gran( method = "nqt",
                      response = "general_supply_kwh")
```


```{r clustering}
hod <- suppressMessages(data_pick %>% 
  dist_gran(gran1 = "hour_day", response = "general_supply_kwh"))

moy <- suppressMessages(data_pick %>% 
  dist_gran(gran1 = "month_year", response = "general_supply_kwh"))

wkndwday <- suppressMessages(data_pick %>% 
  dist_gran(gran1 = "wknd_wday", response = "general_supply_kwh"))

distance <- wkndwday/2 + moy/12 + hod/24

f = as.dist(distance)

cluster_result <- suppressMessages(f %>% 
  clust_gran(kopt = 5)) %>% 
  rename("customer_id" = "id")
```


```{r hod-data}
data_hod <- quantile_gran(data_pick,
                                  "hour_day",
                                  quantile_prob_val = quantile_prob_graph) %>% 
  pivot_wider(names_from = quantiles,
              values_from = quantiles_values) %>% 
  left_join(data_pick_cust, by = c("customer_id")) %>% 
  left_join(cluster_result, by = c("customer_id"))
  
#data_heatmap_hod$customer_id = factor(data_heatmap_hod$customer_id, levels = data_pick_cust$value)
data_hod$category <- factor(data_hod$category, levels = 0:23)

# lev <- data_hod %>% distinct(customer_id, design) %>% arrange(design) %>% pull(customer_id)
```


```{r hod-ind-design}

# data_hod$customer_id <- factor(data_hod$customer_id, levels = lev)

hod_ind_design <- data_hod %>% 
  left_join(cluster_result,by = c("customer_id", "group")) %>% 
  ggplot(aes(x = category)) + 
  geom_ribbon(aes(ymin = `25%`, 
                  ymax = `75%`,
                  group=customer_id, 
                  fill = design),
              alpha = 0.5) +
  geom_line(aes(y = `50%`,
                group=customer_id,
                color = design), 
            size = 1) +
  facet_wrap(design~customer_id, 
             scales = "free_y",
             ncol=6) + 
 theme_application() +
  xlab("hour-of-day") +
  scale_fill_viridis_d()+
  scale_color_viridis_d()+
  scale_x_discrete(breaks = seq(0, 23, 3))
```


```{r hod-ind-group}
hod_ind_group <- data_hod %>% 
  left_join(cluster_result) %>% 
  ggplot(aes(x = category)) + 
  geom_ribbon(aes(ymin = `25%`, 
                  ymax = `75%`,
                  group=customer_id, fill = as.factor(group)),
              alpha = 0.5) +
  geom_line(aes(y = `50%`,
                group=customer_id, color = as.factor(group)), size = 1) +
  facet_wrap(design~customer_id, 
             scales = "free_y",
              ncol=6) + theme_application()+
  scale_fill_manual(values = c("#E69F00", "#009E73","#0072B2", "#D55E00", "grey", "black"))+
  scale_color_manual(values = c("#E69F00", "#009E73","#0072B2", "#D55E00", "grey", "black")) +    xlab("hour-of-day")  +
  scale_x_discrete(breaks = seq(0, 23, 3))
# + scale_y_continuous(breaks = NULL) 
# 
```

```{r hod-combined, fig.cap = "The energy demand distribution for the prototype designs (left) and clustering (right) is given for each hour of the day. Customer design grouping and clustering strategies complement one other well. For a one-to-one comparison, both sides have the same order of consumers. Design-4 has a low morning peak and a strong evening peak. Because other members of group-1 have higher late-night usage, group-2 adopts this design and assigns customer B to it rather than group-1."}
(hod_ind_design + hod_ind_group)* theme(legend.position = "bottom")

```



```{r data-heatmap-moy-design}

data_moy <- quantile_gran(data_pick,
                                  "month_year", 
                                  quantile_prob_val = quantile_prob_graph) %>% 
  pivot_wider(names_from = quantiles, 
              values_from = quantiles_values) %>% 
  left_join(data_pick_cust, by = c("customer_id"))
  
data_moy$category <- factor(data_moy$category, levels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))

moy_ind_design <- data_moy %>% 
  left_join(cluster_result, by = c("customer_id")) %>% 
  ggplot(aes(x = category)) + 
  geom_ribbon(aes(ymin = `25%`, 
                  ymax = `75%`, 
                  group=customer_id, fill = as.factor(design)), alpha = 0.5) +
  geom_line(aes(y = `50%`, group=customer_id, color = as.factor(design)), size = 1) +
  facet_wrap(design~customer_id, 
             scales = "free_y", 
             ncol = 6) +
    ylab("demand (in Kwh)") +
    xlab("month-of-year")  +
  theme_application() +
  scale_fill_viridis_d(direction = 1)+
  scale_color_viridis_d(direction = 1)
```


```{r data-heatmap-moy-group-ind}

moy_ind_group <- data_moy %>% 
  left_join(cluster_result, by = c("customer_id")) %>% 
  ggplot(aes(x = category)) + 
  geom_ribbon(aes(ymin = `25%`, 
                  ymax = `75%`, 
                  group=customer_id, fill = as.factor(group)), alpha = 0.5) +
  geom_line(aes(y = `50%`, group=customer_id, color = as.factor(group)), size = 1) +
  facet_wrap(design~customer_id, 
             scales = "free_y", 
             ncol = 6) +
    ylab("demand (in Kwh)") +
    xlab("month-of-year")  +
  theme_application() +
  scale_fill_manual(values = c("#E69F00", "#009E73","#0072B2", "#D55E00", "grey", "black"))+
  scale_color_manual(values = c("#E69F00", "#009E73","#0072B2", "#D55E00", "grey", "black"))
```


```{r moy-combined, fig.cap = "The energy demand distribution for the prototype designs (left) and clustering (right) are shown. The customer grouping by design and the clustering approach largely match, except for some exceptions. For example, in group 3, consumers are categorised by greater usage in the middle of the year, which corresponds to winter in Australia. This is a better grouping in terms of similar month-of-year pattern than originally proposed by the prototype design."}

(moy_ind_design+moy_ind_group)*theme(legend.position = "bottom")
```  

```{r data-heatmap-wkndwday-group2}

data_wkndwday <- data_pick  %>%
  left_join(cluster_result)%>% create_gran("wknd_wday")  %>% 
  left_join(data_pick_cust, by = c("customer_id"))

ylim1 = boxplot.stats(data_wkndwday$general_supply_kwh)$stats[c(1, 5)]

wkndwday_ind_design <- data_wkndwday%>% 
  ggplot(aes(x=wknd_wday, y = general_supply_kwh)) +
  #lvplot::geom_lv(aes(fill = as.factor(design), 
   #                   color = as.factor(design)), k=5, alpha = 0.5) +
  geom_boxplot(aes(fill = as.factor(group), color = as.factor(group)),alpha = 0.5)+
  #geom_boxplot(outlier.shape = NA) + 
  coord_cartesian(ylim = ylim1*1.05)+
  facet_wrap(design~customer_id, 
             scales = "free_y", 
             labeller = "label_value",
              ncol = 24)  +
  theme_application() +
  scale_fill_viridis_d(direction = 1)+
  scale_color_viridis_d(direction = 1)
  

```


```{r}
wkndwday_ind_group <- data_wkndwday%>% 
  ggplot(aes(x=wknd_wday, y = general_supply_kwh)) +
  #lvplot::geom_lv(aes(fill = as.factor(group), 
          #            color = as.factor(group)), k=5, alpha = 0.5) +
  geom_boxplot(aes(fill = as.factor(group), color = as.factor(group)),alpha = 0.5)+
  #geom_boxplot(outlier.shape = NA) + 
  coord_cartesian(ylim = ylim1*1.05)+
  facet_wrap(design~customer_id, 
             scales = "free_y", 
             labeller = "label_value",
             ncol = 24)  +
  theme_application() +
  scale_fill_manual(values = c("#E69F00", "#009E73","#0072B2", "#D55E00",  "grey", "black"))+
  scale_color_manual(values = c("#E69F00", "#009E73","#0072B2", "#D55E00", "grey", "black"))

(wkndwday_ind_design / wkndwday_ind_group)* theme(legend.position = "none")
```

##  JS-based clustering

The $24$ prototypes are clustered using the methodology described in \ref{Sec:methodology}. The distribution of electricity demand for the selected $24$ customers across hour-of-day and month-of-year are shown in the right panel of Figures \ref{fig:hod-combined} and \ref{fig:moy-combined} respectively. The median is shown by a line, and the shaded region shows the area between the  $25^{th}$ and $75^{th}$. Customers are placed in the same order on both sides for both figures to simplify one-to-one comparison. All customers with the same color represent the same design (left) or cluster (right). Overall, the clustering method has been able to capture the shapes in each designs quite well, irrespective of the fact that the designs are only selected based on the $50^{th}$ percentile with Designs $1$, $2$, $3$, $4$, and $5$ correspond to groups $4$, $5$, $1$, $3$, $2$ and, in most situations.Because our method uses distribution across hod, moy, and wknd-wday, there may be some mismatches from the designs; nonetheless, visualising individual customers can help us determine whether the grouping by clustering is better than the planned grouping through designs. In a few circumstances, for example, as shown in Figures \ref{fig:hod-combined} and \ref{fig:moy-combined}, the clustering approach outperforms the design. The plotting scales are not displayed since we want to emphasize comparable shapes rather than scales. A customer in the cluster may have low daily or total energy usage, but their behavior may be quite similar to a customer with high usage. \noindent 

<!-- The third panel of Figure \ref{fig:combined} shows that the wknd-wday groups exhibit no significant changes across clusters, indicating that they may be a nuisance variable for these consumers. -->


Characterization of clusters both statistically and qualitatively is an important stage of a cluster analysis. A potential way is to look at the findings from all the groups in graphs, and enhance our qualitative descriptions of the groupings. Figure \ref{fig:combined} shows the distribution of the summarized groups and help us to characterise each of the clusters. All of these may be validated with further information about the customer.

_Group 1:_ This group a strong early morning and late night hours. These consumers may be flexible students or elderly retirees who are night owls.
Their day time usage has high variability. They have heaters on in the winter but consume less energy in the summer. Winter usage may also be due to increased usage of heaters at night when they are up, but typically very less variability in other months.

_Group 2:_ They appear to work 9-5, get up and perform morning activities from 7-10, and then go. Evenings are busier than mornings as people return home to cook supper and perform other activities. During the fall (March-June), these consumers' average energy use declines, before rising again during the winter. However, energy behavior varies more in the winter than in the summer and a bit in the fall months.

_Group 3:_ Presence of children or stay-at-home parents is indicated by the group's almost equivalent morning, afternoon and evening median use and variability across all hours. They have low usage for winter months (or maybe due to use of gas heaters, it is not reflected in electricity usage).
It seems that the users in this group are more concerned about the comfort and quality of life than the cost of electricity as we see their electricity demand peaking up in the spring and summer.

_Group 4:_ These users have an opposite monthly profile to Group 3, with more usage during summer months (probably due to increased use of air conditioners) and demand decreasing across the year with least demand in winter months. The users in this cluster are sensitive to high temperature, but insensitive to lower temperature or their heater usage is not reflected in their electricity usage.

_Group 5:_ Presence of children or stay-at-home parents is indicated by Group-4's almost equivalent morning, afternoon and evening profile. This group is more typical in their monthly energy behavior with more energy usage in winter and moderately decreasing for fall and spring months. The pattern in the variability is similar to that of the median.


<!-- _Group 5:_ Presence of children or stay-at-home parents is indicated by Group-4's almost equivalent morning, afternoon and evening profile. They have a flat monthly profile except for winter months, with higher median usage and variability than typically seen in other seasons. -->

<!-- It seems that the users in this group are more concerned about the comfort and quality of life than the cost of electricity as opposed to Group 1 and 3, who do not consume more electricity in the summer. -->

<!-- insensitive to high temperature but have their heaters on in the winter months May-Aug. -->

<!-- _Group 2:_  This is the group that rushes out of the house in the morning to get to work. They only return at night and do all activities at night, so there is no morning peak. In addition, the users in this cluster are insensitive to high temperature, but sensitive to lower temperature. -->

<!-- _Group 3:_  This group a strong early morning and late night hours. These consumers may be flexible students or elderly retirees who are night owls. They have heaters on in the winter but consume less energy in the summer. Winter usage may also be due to increased usage of heaters at night when they are up. -->

<!-- _Group 4:_ Presence of children or stay-at-home parents is indicated by Group-4's almost equivalent morning, afternoon and evening profile. They have a flat monthly profile, indicating the usage across all months are similar. It seems that the users in this group are more concerned about the comfort and quality of life than the cost of electricity as opposed to Group 1 and 3, who do not consume more electricity in the summer. -->


```{r data-heatmap-hod-group-new}
legend_title <- "group"

data_group <- data_pick  %>% 
  left_join(cluster_result, by = c("customer_id"))

data_heatmap_hod_group <- quantile_gran(data_group,
                                  gran1="hour_day",
                                  quantile_prob_val = c(0.25, 0.5, 0.75),
                                  group="group") %>% 
  pivot_wider(names_from = quantiles, values_from = quantiles_values) 

  
data_heatmap_hod_group$category <- factor(data_heatmap_hod_group$category, levels = 0:23)

# data_heatmap_hod_group <- data_heatmap_hod_group %>%
#   mutate(group = case_when(group==1 ~"Group 2",
#                            group==2 ~"Group 4",
#                            group==3 ~"Group 1",
#                            group==4 ~"Group 3"))

# data_heatmap_hod_group$group <- factor(data_heatmap_hod_group$group, levels = c("Group 1", "Group 2", "Group 3", "Group 4"))

data_heatmap_hod_group$group <- paste("group", data_heatmap_hod_group$group, sep = "-")

hod_group <- data_heatmap_hod_group %>% 
  ggplot(aes(x = category)) + 
  geom_ribbon(aes(ymin = `25%`, 
                  ymax = `75%`,
                  group=group,
                  fill = as.factor(group), alpha = 0.5),
              alpha = 0.5) +
  geom_line(aes(y = `50%`,
                group=group, 
                color = as.factor(group)), size = 1)+
  facet_wrap(~group, 
             scales = "free_y",  
             nrow = 5) + 
              #labeller = labeller(xfacet = c(`1` = "Group 2", `2` = "Group 4",`3` = "Group 1",`4` = "Group 3"))
    theme(strip.text = element_text(size = 10, margin = margin(b = 0, t = 0))) + xlab("hour-of-day") + 
  ylab("demand (in Kwh)") + 
  theme_bw() +
   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  theme(panel.spacing =unit(0, "lines")) + 
  theme(axis.text.x = element_text(angle=90, hjust=1, size = 7)) +
  scale_x_discrete(breaks = seq(1, 24, 3))+ 
  #theme(strip.text = element_text(size = 8, margin = margin(b = 0, t = 0)))+
  theme(plot.margin = margin(0, 0, 0, 0, "cm")) +
   scale_color_manual(legend_title, values = c("#E69F00", "#009E73","#0072B2", "#D55E00", "grey")) +
  scale_fill_manual(legend_title, values = c("#E69F00", "#009E73","#0072B2", "#D55E00", "grey")) + 
  theme(legend.position = "bottom") 

```

```{r data-heatmap-moy-group-new}
data_heatmap_moy_group <- quantile_gran(data_group,
                                  gran1="month_year",
                                  quantile_prob_val = c(0.25, 0.5, 0.75),
                                  group="group") %>% 
  pivot_wider(names_from = quantiles, values_from = quantiles_values) 

data_heatmap_moy_group$category <- factor(data_heatmap_moy_group$category, levels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))


data_heatmap_moy_group$group <- paste("group", data_heatmap_moy_group$group, sep = "-")


moy_group <- data_heatmap_moy_group %>% 
  ggplot(aes(x = category)) + 
  geom_ribbon(aes(ymin = `25%`, 
                  ymax = `75%`, group=group, fill = as.factor(group)), alpha = 0.5) +
  geom_line(aes(y = `50%`, group=group, color = as.factor(group)), size = 1 ) +
  facet_wrap(~group, 
             scales = "free_y", 
             labeller = "label_value",
             nrow = 5) +
    theme(strip.text = element_text(size = 10, margin = margin(b = 0, t = 0))) + xlab("month-of-year") + 
  ylab("demand (in Kwh)") +
  theme_bw() +
   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) + 
  theme(panel.spacing =unit(0, "lines")) + 
  theme(plot.margin = margin(0, 0, 0, 0, "cm"))  +
   scale_color_manual(legend_title, values = c("#E69F00", "#009E73","#0072B2", "#D55E00", "grey")) +
  scale_fill_manual(legend_title, values = c("#E69F00", "#009E73","#0072B2", "#D55E00", "grey")) + 
  theme(legend.position = "bottom") +
  theme(panel.spacing =unit(0, "lines")) + 
  theme(axis.text.x = element_text(angle=90, hjust=1, size = 7))
```


```{r validation}
data_validation <- (hod/24) %>% broom::tidy() %>% 
  rename("hod" = "distance") %>% 
  left_join((moy/12) %>% 
              broom::tidy(), 
            by = c("item1", "item2")) %>% 
  rename("moy" = "distance") %>% 
  left_join((wkndwday/2) %>% 
              broom::tidy(), 
            by = c("item1", "item2"))%>% 
  rename("wkndwday" = "distance") %>% 
  left_join(cluster_result, by = c("item1" = "customer_id")) %>% 
  rename("group_item1" = "group") %>% 
  left_join(cluster_result, by = c("item2" = "customer_id")) %>%  
  rename("group_item2" = "group") %>% 
  pivot_longer(3:5,names_to="gran",
               values_to = "distance")
```


```{r validation-data}

hod_cat <- suppressMessages(
data_pick %>% 
  dist_gran_cat(gran1 = "hour_day", response = "general_supply_kwh")) %>% 
  mutate(gran = "hod")

moy_cat <- suppressMessages(data_pick %>% 
  dist_gran_cat(gran1 = "month_year", response = "general_supply_kwh"))%>%
  mutate(gran = "moy")

wkndwday_cat <- suppressMessages(data_pick %>% 
  dist_gran_cat(gran1 = "wknd_wday", response = "general_supply_kwh"))%>% 
  mutate(gran = "wnwd")


data_validation_cat <- bind_rows(hod_cat, moy_cat, wkndwday_cat)


gran_cat_dist <- data_validation_cat %>% 
  left_join(cluster_result, by = c("customer_from" = "customer_id")) %>% 
  rename("group_from" = "group") %>% 
  left_join(cluster_result, by = c("customer_to" = "customer_id")) %>%  
  rename("group_to" = "group") %>% 
  group_by(gran,
           category,
           group_from, 
           group_to) %>% 
  summarise(sum = sum(distance),.groups = 'drop') %>% 
  pivot_wider(names_from = group_to, values_from = sum) %>% 
  mutate(distance = sum(`1`, `2`, `3`, `4`, `5`)) %>% 
  select(-(4:8)) %>% 
  arrange(-distance) 

```



```{r hod-bubble}
gran_cat_hod <-   gran_cat_dist %>% 
  filter(gran=="hod")

gran_cat_hod$category <- factor(gran_cat_hod$category, levels = 0:23)

group_total <- gran_cat_hod %>% 
  ungroup() %>% 
    group_by(group_from) %>% 
    summarise(group_total = sum(distance),.groups = 'drop')


gran_cat_hod %>% 
ungroup %>% 
    left_join(group_total, by = "group_from") %>% 
    mutate(cat_contibution = distance*100/group_total) %>% 
    group_by(group_from) %>% 
    arrange(-distance) %>% slice(c(1:4)) %>% 
      ggplot(aes(group_from, category, color = as.factor(group_from))) +
      geom_point(aes(size = distance)) +
  scale_color_manual(values = c("#E69F00", "#009E73","#0072B2", "#D55E00", "grey", "black"))+ theme_light()
```

```{r hod-cont}
data_pcp <- gran_cat_hod %>% 
    ungroup %>% 
    left_join(group_total, by = "group_from") %>% 
    mutate(cat_contibution = distance*100/group_total) %>% 
    group_by(group_from) %>% 
    arrange(-distance) %>% 
  select(group_from, category, cat_contibution) %>% 
  pivot_wider(names_from = "category", values_from = "cat_contibution") %>% mutate(group_from = as.factor(group_from))
  

parcoord <- GGally::ggparcoord(data_pcp ,
                   columns = 2:ncol(data_pcp),
                   groupColumn = "group_from",
                   showPoints = TRUE, 
                   alphaLines = 1,
                   order = "anyClass",
                   scale = "globalminmax"
) + 
  ggplot2::theme(
    plot.title = ggplot2::element_text(size=10)
  )+
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 10)) +
  theme(legend.position = "bottom") +
  xlab("") +
  ylab("wpd")+
  scale_color_manual(values = c("#E69F00", "#009E73","#0072B2", "#D55E00", "grey", "black")) + theme_bw()
   parcoord 
```



```{r moy-bubble, eval = FALSE}
gran_cat_moy <-   gran_cat_dist %>% 
  filter(gran=="moy")

gran_cat_moy$category <- factor(gran_cat_moy$category, levels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))

group_total <- gran_cat_moy %>% 
  ungroup() %>% 
    group_by(group_from) %>% 
    summarise(group_total = sum(distance),.groups = 'drop')


gran_cat_moy %>% 
ungroup %>% 
left_join(group_total, by = "group_from") %>% 
mutate(cat_contibution = distance*100/group_total) %>% 
group_by(group_from) %>% 
arrange(-distance) %>% slice(c(1:3)) %>% 
    ggplot(aes(group_from, category, color = as.factor(group_from))) +
      geom_point(aes(size = distance))+
  scale_color_manual(values = c("#E69F00", "#009E73","#0072B2", "#D55E00", "grey", "black"))+ theme_light()

```

```{r moy-contr, eval = FALSE}    
    
data_pcp <- gran_cat_moy %>% 
ungroup %>% 
left_join(group_total, by = "group_from") %>% 
mutate(cat_contibution = distance*100/group_total) %>% 
group_by(group_from) %>% 
arrange(-distance) %>% 
  select(group_from, category, cat_contibution) %>% 
  pivot_wider(names_from = "category", values_from = "cat_contibution") %>% mutate(group_from = as.factor(group_from))


parcoord <- GGally::ggparcoord(data_pcp ,
                   columns = 2:ncol(data_pcp),
                   groupColumn = "group_from",
                   showPoints = TRUE, 
                   alphaLines = 1,
                   order = "anyClass",
                   scale = "globalminmax"
) + 
  ggplot2::theme(
    plot.title = ggplot2::element_text(size=10)
  )+
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 10)) +
  theme(legend.position = "bottom") +
  xlab("") +
  ylab("wpd")+
  scale_color_manual(values = c("#E69F00", "#009E73","#0072B2", "#D55E00", "grey", "black")) + theme_bw()
    parcoord 


```





```{r wkndwday-bubble, eval = FALSE}
gran_cat_wkndwday <-   gran_cat_dist %>% 
  filter(gran=="wnwd")

gran_cat_wkndwday$category <- factor(gran_cat_wkndwday$category, levels = c("Weekday", "Weekend"))

group_total <- gran_cat_wkndwday %>% 
  ungroup() %>% 
    group_by(group_from) %>% 
    summarise(group_total = sum(distance),.groups = 'drop')

gran_cat_wkndwday %>% 
ungroup %>% 
left_join(group_total, by = "group_from") %>% 
mutate(cat_contibution = distance*100/group_total) %>% 
group_by(group_from) %>% 
arrange(-distance) %>% slice(c(1)) %>% 
 ggplot(aes(group_from, category, color = as.factor(group_from))) +
 geom_point(aes(size = distance))+
 scale_color_manual(values = c("#E69F00", "#009E73","#0072B2", "#D55E00", "grey", "black"))+ theme_light()
```



```{r data-heatmap-hod-group}
legend_title <- "group"

data_group <- data_pick  %>% 
  left_join(cluster_result, by = c("customer_id"))

data_heatmap_hod_group <- quantile_gran(data_group,
                                  gran1="hour_day",
                                  quantile_prob_val = c(0.25, 0.5, 0.75),
                                  group="group") %>% 
  pivot_wider(names_from = quantiles, values_from = quantiles_values) 

  
data_heatmap_hod_group$category <- factor(data_heatmap_hod_group$category, levels = 0:23)

data_heatmap_hod_group$group <- paste("group", data_heatmap_hod_group$group, sep = "-")

hod_group <- data_heatmap_hod_group %>% 
  ggplot(aes(x = category)) + 
  geom_ribbon(aes(ymin = `25%`, 
                  ymax = `75%`,
                  group=group,
                  fill = as.factor(group), alpha = 0.5),
              alpha = 0.5) +
  geom_line(aes(y = `50%`,
                group=group, 
                color = as.factor(group)), size = 1)+
  facet_wrap(~group, 
             scales = "free_y",  
             ncol = 5) + 
              #labeller = labeller(xfacet = c(`1` = "Group 2", `2` = "Group 4",`3` = "Group 1",`4` = "Group 3"))
    theme(strip.text = element_text(size = 10, margin = margin(b = 0, t = 0))) + xlab("hour-of-day") + 
  ylab("demand (in Kwh)") + 
  theme_bw() +
   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  theme(panel.spacing =unit(0, "lines")) + 
  theme(axis.text.x = element_text(angle=90, hjust=1, size = 7)) +
  scale_x_discrete(breaks = seq(1, 24, 3))+ 
  #theme(strip.text = element_text(size = 8, margin = margin(b = 0, t = 0)))+
  theme(plot.margin = margin(0, 0, 0, 0, "cm")) +
   scale_color_manual(legend_title, values = c("#E69F00", "#009E73","#0072B2", "#D55E00", "grey")) +
  scale_fill_manual(legend_title, values = c("#E69F00",
  "#009E73","#0072B2", "#D55E00", "grey")) + 
  theme(legend.position = "bottom") 

```

```{r data-heatmap-moy-group}
data_heatmap_moy_group <- quantile_gran(data_group,
                                  gran1="month_year",
                                  quantile_prob_val = c(0.25, 0.5, 0.75),
                                  group="group") %>% 
  pivot_wider(names_from = quantiles, values_from = quantiles_values) 

data_heatmap_moy_group$category <- factor(data_heatmap_moy_group$category, levels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))


data_heatmap_moy_group$group <- paste("group", data_heatmap_moy_group$group, sep = "-")


moy_group <- data_heatmap_moy_group %>% 
  ggplot(aes(x = category)) + 
  geom_ribbon(aes(ymin = `25%`, 
                  ymax = `75%`, group=group, fill = as.factor(group)), alpha = 0.5) +
  geom_line(aes(y = `50%`, group=group, color = as.factor(group)), size = 1 ) +
  facet_wrap(~group, 
             scales = "free_y", 
             labeller = "label_value",
             ncol = 5) +
    theme(strip.text = element_text(size = 10, margin = margin(b = 0, t = 0))) + xlab("month-of-year") + 
  ylab("demand (in Kwh)") +
  theme_bw() +
   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) + 
  theme(panel.spacing =unit(0, "lines")) + 
  theme(plot.margin = margin(0, 0, 0, 0, "cm"))  +
   scale_color_manual(legend_title, values = c("#E69F00", "#009E73","#0072B2", "#D55E00", "grey")) +
  scale_fill_manual(legend_title, values = c("#E69F00", "#009E73","#0072B2", "#D55E00", "grey")) + 
  theme(legend.position = "bottom") +
  theme(panel.spacing =unit(0, "lines")) + 
  theme(axis.text.x = element_text(angle=90, hjust=1, size = 7))
```


```{r combined-groups, fig.cap = The distribution of electricity demand for the clusters across hour-of-day, month-of-year and wknd-wday. The median is represented by a line and the shaded region represents the area between $25^{th}$ and $75^{th}$ percentile. Group 1 and 3 have a stronger hour-of-day pattern, while group 2, 3, 4 have a month-of-year pattern. For wknd-wday differences across different groups are not distinct suggesting that it might not be that important a variable to distinguish different clusters."}
combined <- (hod_group/moy_group) & theme(legend.position = "bottom")
combined + plot_layout(guides = "collect")
```

```{r}
wkndwday_data <- data_group %>% create_gran("wknd_wday") %>% 
  create_gran("hour_day")

ylim1 = boxplot.stats(wkndwday_data$general_supply_kwh)$stats[c(1, 5)]

wkndwday_group <- wkndwday_data%>% 
  ggplot(aes(x=hour_day, y = general_supply_kwh)) +
  #lvplot::geom_lv(aes(fill = as.factor(group)), k=5) +
  geom_boxplot(aes(fill = as.factor(group), color = as.factor(group)),alpha = 0.5)+
  #geom_boxplot(outlier.size = 1) + 
  coord_cartesian(ylim = ylim1*1.05)+
  #ggridges::geom_density_ridges2(aes(x = general_supply_kwh, y = wknd_wday,fill = as.factor(group))) + coord_flip() +
#geom_boxplot(aes(fill = as.factor(group))) +
  #scale_fill_lv() +
 xlab("wknd-wday") + 
  ylab("demand (in Kwh)") +
   facet_grid(wknd_wday~group, 
             scales = "free_y", 
             labeller = "label_both") + 
  theme_bw() +
   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) + 
  theme(panel.spacing =unit(0, "lines")) + 
  theme(plot.margin = margin(0, 0, 0, 0, "cm"))  +
  scale_fill_manual(legend_title, values = c("#E69F00", "#009E73","#0072B2", "#D55E00", "grey")) +
  scale_color_manual(legend_title, values = c("#E69F00", "#009E73","#0072B2", "#D55E00", "grey")) +
  theme(legend.position = "bottom") +
  theme(panel.spacing =unit(0, "lines")) + 
  theme(axis.text.x = element_text(angle=90, hjust=1, size = 7))
wkndwday_group
```


##  wpd-based clustering


```{r ggpairs, fig.cap = "The ggpairs plot of electricity demand against the three granularities hod, moy and wkndwday are shown. The correlation between any two granularities is not so high so as to pose a threat to the clustering methodology."}

elec_600_wpd <- read_rds(here::here("data/algo2-cust600-wpd-rawdata.rds"))

elec_pick <- elec_600_wpd %>% 
  filter(customer_id %in% data_pick_cust$customer_id)

elec_pick_wide <- elec_pick %>% pivot_wider(-c(1, 2), names_from = "x_variable", values_from = wpd)

```




```{r, fig.cap=" A ggpairsplot and parallel coordinate plot are used to depict each of the 24 customers. The ggpairs plot four distinct clusters across the month-of-year, which are less prominent across the hour-of-day and wknd-wday. The parallel coordinate plot ranks the variables in order of importance, indicating that the month-of-year is the most important in identifying clusters, whereas wkdn-wday is the least significant and has the least variability among the three variables.", message=FALSE, warning=FALSE}


scaled_var <- elec_pick_wide

f <- elec_pick_wide[-1] %>% dist() 


group <- f%>% hclust (method = "ward.D") %>% cutree(k=5)


cluster_result_wpd <- bind_cols(id = elec_pick_wide$customer_id, group = group) 

data_pcp <- scaled_var %>% 
  #bind_cols(customer_id =  elec_pick_wide$customer_id) %>%
  left_join(cluster_result_wpd , by = c("customer_id" = "id")) %>% 
  select(customer_id, group, everything()) %>% 
  mutate(group = as.factor(group))


pairsplot <- ggpairs(data_pcp, 
                     columns = 3:5,
                     aes(fill=group, 
                     color = group)) +
  scale_fill_viridis_d(direction = -1) +
  scale_color_viridis_d(direction = -1) +
  theme_light()
  

parcoord <- GGally::ggparcoord(data_pcp ,
                   columns = 3:ncol(data_pcp),
                   groupColumn = "group",
                   showPoints = FALSE, 
                   alphaLines = 0.8,
                   order = "anyClass",
                   scale = "globalminmax"
) + 
  ggplot2::theme(
    plot.title = ggplot2::element_text(size=10)
  )+
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 10)) +
  theme(legend.position = "bottom") +
  xlab("") +
  ylab("wpd") + scale_fill_viridis_d(direction = -1) +
  scale_color_viridis_d(direction = -1) + theme_light()

pairsplot

```

```{r parcoord, fig.cap = "A parallel coordinate plot with the three significant cyclic granularities used for wpd-based clustering. The variables are sorted according to their separation across classes (rather than their overall variation between classes). This means that moy is the most important variable in distinguishing the designs followed by hod and wkndwday. It can be observed that cluster 3 and 4 are distinguished by moy while 1 and 2 are distinguished by hod. Here, wkndwday is still acting as the nuisance variable."}
parcoord
```


A parallel coordinate plot with the three significant cyclic granularities used for wpd-based clustering. The variables are sorted according to their separation across classes (rather than their overall variation between classes). This means that moy is the most important variable in distinguishing the designs followed by hod and wkndwday. It can be observed that clusters are well separated by month-of-year, while hour-of-day and weekend-weekday are not useful distinguishing the clusters produced with this clustering method. There is only one customer who has significant wpd across weekend-weekday and stands out from the rest of the customers. 3 and 4 are distinguished by moy while 1 and 2 are distinguished by hod. Here, wkndwday is still acting as the nuisance variable. The ggpairs plot four distinct clusters across the month-of-year, which are less prominent across the hour-of-day and wknd-wday. The parallel coordinate plot ranks the variables in order of importance, indicating that the month-of-year is the most important in identifying clusters, whereas wkdn-wday is the least significant and has the least variability among the three variables.


<!-- \noindent We discover 4 qualitative clusters of varying shapes in the distribution of all consumers in the first panel of Figure \ref{fig:combined}. Group-1 includes consumers who work 9-5, get up and conduct morning activities from 7-10am, and then depart. Then they return home in the evening to cook supper and perform other activities, giving the evening a greater peak than the morning. Group 2 is the group that rushes out of the house in the morning to get to work. They only return at night and do all activities at night, so there is no morning peak. The third one has a strong early morning and late night hours. These consumers may be flexible students or elderly retirees who are night owls. Presence of children or stay-at-home parents is indicated by Group-4's almost equivalent morning, afternoon and evening profile. All of this may be validated with further information about the customer.  -->

<!-- \noindent The second panel of Figure \ref{fig:combined} shows that month-of-year qualitative clusters are not as distinguishable as hour-of-day. Group 2 is the most distinct and uses the most power during the summer, possibly owing to the use of air conditioners. Group 4 has a flat profile, indicating no significant month-to-month changes. Groups 1 and 3 have heaters on in the winter but consume less energy in the summer. Since gas is not available in all of NSW LGAs, it is possible that customers' heater usage is recorded in electricity rather than gas. -->

<!-- \noindent The third panel of Figure \ref{fig:combined} shows that the wknd-wday groups exhibit no significant changes across clusters, indicating that they may be a nuisance variable for these consumers. -->

<!-- \noindent The plotting scales are not displayed since we want to emphasise comparable shapes rather than scales. A customer in the cluster may have low daily or total energy usage, but their behaviour may be quite similar to a customer with high usage. That places them in the same group. -->



# Discussion {#sec:discussion}

We propose different clustering methodology for grouping noisy, patchy time series data available at a fine temporal scale. Depending on the aim of clustering, they produce different clustering. The clustering is done based on probability distributions of the time series variable measured across several cyclic granularirties. There is issue with scaling it up to many customers as anomalies need to be removed before such classification would be useful.


<!-- This section will cover some drawback of this clustering method and potential extensions of this work. -->

<!-- There are many cluster analysis methods that can produce quite different clusterings on the same dataset. Cluster validation is about the evaluation of the quality of a clustering; “relative cluster validation” is about using such criteria to compare clusterings. This can be used to select one of a set of clusterings from different methods, or from the same method ran with different parameters such as different numbers of clusters. There are many cluster validation indexes in the literature. Most of them attempt to measure the overall quality of a clustering by a single number, but this can be inappropriate. There are various different characteristics of a clustering that can be relevant in practice, depending on the aim of clustering, such as low within-cluster distances and high between-cluster separation. In this paper, a number of validation criteria will be introduced that refer to different desirable characteristics of a clustering, and that characterise a clustering in a multidimensional way. In specific applications the user may be interested in some of these criteria rather than others. A focus of the paper is on methodology to standardise the different characteristics so that users can aggregate them in a suitable way specifying weights for the various criteria that are relevant in the clustering application at hand. -->

```{r}
library(ochRe)
  scale_colour_ochre(
    palette="nolan_ned")
```

