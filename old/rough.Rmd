---
title: "rough"
author: "Sayani Gupta"
date: "08/06/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
library(tidyverse)
```

# Clustering Australian Smart Grid Smart City data for grouping periodically behaving customers


## Introduction


<!-- Description of data available -->
The Smart Grid, Smart City (SGSC) project, which rolled out Australia's first commercial-scale smart grid was implemented across eight local government areas in New South Wales (NSW). Data from more than 13,000 household electricity smart meters is obtained as part of that project. It provides half-hourly energy usage and demographic data for Australia, as well as detailed information on appliance use, climate, retail and distributor product offers, and other related factors. The trials were based in Newcastle, New South Wales, but also covered areas in Sydney CBD, Newington, Ku-Ring-Gai, and the rural township of Scone. The load time series is asynchronous as it is observed for these households for unequal time lengths and consists of missing observations.

The massive amount of data generated in such projects could be overwhelming for analysis. Electricity utilities can utilize the consumption patterns of customers to develop targeted tariffs for individual groups and alleviate the problem of volatility in production by capitalizing on the flexibility of consumers. Beyea (2010) has pointed out, there has been little discussion or exploration of the full potential of these data bases and their benefits can reach beyond the original intentions for collecting these data. Thus, there is a scope to investigate and analyze these data in various ways for a greater understanding of consumption patterns and how they correlate with other economic, physical or geographical factors. In this work, we are interested to see how we can utilize this dataset to group different customers with similar periodic behavior. Towards this goal, this chapter aims to: (a) describe the contents of the data set in SGSC database that we can utilize, and (b) propose a clustering algorithm to group customers with similar periodic behaviors. The distance metric introduced in Chapter 2 will be the inputs for this cluster analysis. One of the advantages of using our 
approach is that the technique is based on probability distributions instead of raw data. Many clustering approaches are limited by the type of noisy, patchy, and unequal time-series common in residential data sets. Since the distance measure considered is based on differences in probability distribution of time series, it is likely to be less sensitive to missing or noisy data.



The following contributions are made through the following chapter:

 * Present a cluster analysis of SGSC dataset to group households with similar periodic behavior
 * Cluster validation by relating to external data


<!-- Electricity smart meter technology is increasingly being deployed in residential and commercial buildings. For example, in the state of Victoria, Australia, it is a state government policy that all households have a smart meter fitted (Victoria State Government, 2015), which has resulted in 97% of the approximately 2.1 million households in Victoria having a smart meter installed in their home. These smart meters collect energy usage information at half-hourly intervals, resulting in over 35 billion half-hourly observations per year across all households in the state. Clustering electricity consumption patterns can enable electricity utilities to develop targeted tariffs for individual groups alleviating the problem of volatility in production by capitalizing on the flexibility of consumers. -->

<!-- Clustering households using only smart meter consumption data could also provide value in a societal setting by combining the findings and external data like weather conditions, socio-economic or other demographic factors of those households. -->

<!-- Background literature and shortcomings -->


## Preliminary exploratoration

This section familiarizes the 13,000 SGSC households through visualization and provide a detailed layout of data structures (like missing observations/number of customers/number of observations) and also the external data that needs to be utilized for validating the clustering process. The [ABS TableBuilder](https://www.abs.gov.au/websitedbs/censushome.nsf/home/tablebuilder) has census data from 2011 and 2016. The data is at SA2 and LGA levels. However, some of the LGAs in NSW changed between 2011 and 2016 and hence there would not be a one-to-one correspondence between the LGAs Weather, notably temperature (and humidity) can be the main driver(s) for energy usage.  In NSW many households have electric heaters so their use can impact winter energy use and air-conditioners can impact summer energy use. Relevant weather data could be obtained from the [Bureau of Meteorology](www.bom.gov.au). Some weather stations have 30 minute (sometimes even smaller interval) weather data. Potentially, there could be lag effects of weather on energy usage which should be considered.

A dataset of 100 SGSC homes has been used to lay out the structure to be used for analyzing the big dataset. The smaller dataset contains half-hourly kwh values form 2012 to 2014 and has asynchronous time series distributed evenly over the observation period (Figure \ref{fig:elec-raw}), similar to the bigger data set. Figure \ref{fig:count-gaps} can be used to interpret missingness in the data, where the customers are arranged from maximum to minimum missing. It looks like data is most missing before 2013 and for a particular date in 2014.


```{r load-elec, message = FALSE, warning=FALSE}

```

```{r elec-raw, fig.height = 8, fig.cap="The raw data for 50 households are shown. It looks like there is a lot of missing values and unequal length of time series along with asynchronous periods for which data is observed."}

```


```{r elec-gaps, warning=FALSE}

```


```{r count-gaps, fig.height = 8, fig.cap="The missing data pattern for 100 households are shown. It looks like most missingness happens before 2013 and for a particular data in 2014."}

```

## Common load clustering techniques of smart meter data

The foundation for this study is Tureczek2017-pb, which conducts a systematic review of the current state of the art in smart meter data analytics, which evaluates approximately 2100 peer-reviewed papers and summarizes the main findings. None of the 34 selected papers which focus on clustering consumption are based on Australian smart meter data. The clustering is frequently applied directly to the raw data without scrutinizing for auto correlation and periodicity. The algorithm most ubiquitously employed is K-Means. But the omission of the time series structure or correlation in the analysis while employing K-Means leads to inefficient clusters. Principal Component Analysis or Self-Organizing Maps removes correlation structures and transitions the data to a reduced feature space, but it comes at a cost of interpretability of the final results. @Tureczek2018-ha has shown that a transformation of data to incorporate autocorrelation before K-Means clustering can improve performance and enable K-Means to deliver smaller clusters with less within-cluster variance. However, it does not explain the cluster composition by combining it with external data. Some papers present pre-processing of the smart-meter data before clustering through principal component analysis or factor analysis for dimensionality reduction or self-organizing maps for 2-Dimensional representation of the data (@Ndiaye2011-pf). Other algorithms used in the literature include k-means variations, hierarchical methods and k-medoids based on a greedy algorithm have been designed to select typical periods in the time series. As the methods are often situation specific, 
it makes sense to compare them on the performance rather than any standard performance metric. A type of clustering based on information theory such as Shannon or Renyi entropy and their variants are addressed in , which differs from typical methods adopted for electricity consumer classification, based on the Euclidean distance notion. @Motlagh2019-yj presents strategy to address the problems on patchy, and unequal time-series common in residential data sets by converting load time series into map models. Most time-series clustering models are limited to handling time domain with same start and end date and time. Most of the solutions to handle this like longest common subsequence, dynamic time warping are prone to computational limit with increased length of the series.

<!-- In contrast to models, a feature-based strategy is used to explicitly define or automatically extract the curves’ key time features, for instance by application of PCA on the daily curves [ -->




<!-- Most papers discussed in Tureczek2017-pb fail to accept smart meter readings as time series data, a data type which contains a temporal component. The omission of the essential time series features in the analysis leads to the application of methods that are not designed for handling temporal components. K-Means ignores autocorrelation, unless the input data is pre-processed. The clusters identified in the papers are validated by a variety of indices, with the most prevalent -->
<!-- being the cluster dispersion index (CDI) [22–24], the Davies–Bouldin index (DBI) [25,26] and the mean index adequacy (MIA) [8,13]. -->


## The proposed load-clustering technique

The existing work on clustering probability distributions assumes we have an iid sample $f_1(v),\dots,f_n(v)$, where $f_s(v)$ denotes the probability distribution from observation $s$ over some random variable $v = \{v_t: t = 0, 1, 2, \dots, T-1\}$ observed across $T$ time points. In our work, we are using $s$ as denoting a household and the underlying variable as the electricity demand. So $f_s(v)$ is the distribution of household $s$ and $v$ is electricity demand. Consider two cyclic granularities $A$ and $B$, such that $A = \{ a_j: j = 1, 2, \dots, J\}$ and $B = \{ b_k: k = 1, 2, \dots, K\}$. We still want the households in clusters, but each household consists of a collection of probability distributions. For  example, we would have $J*K$ distributions per household. So we need a distance between collections of univariate probability distributions. One possibility is to consider the normalized weighted pairwise distance as the difference between various households for the variable $(A, B)$. Let $wpd_{norm_{s,t}}(A,B)$ denotes the normalized pairwise distances between subjects/elements $s$ and $t$ for the harmony pair $(A, B)$. $wpd_{norm_{s,t}}(A,B)$ could be thought to be a dimensionality reduction procedure in time series data mining. This feature, which is considered as a measure to determine the strength of any periodic pattern are then inputs to a clustering algorithm that can then discern similarity and differences between the time series.

We employ a hierarchical cluster analysis algorithm to obtain pockets of households showing similar periodic behavior by grouping probability distributions across a harmony. This clustering algorithm is adopted to remove or appropriately adjust for auto correlation and unequal length in the data. The method could be further extended by clustering probability distributions conditional on one or more cyclic granularities. The following are some of the advantages of our proposed method.

<!-- Trend and seasonality are common features of time series, and it is natural to characterize a time series by its degree of trend and seasonality. By considering the probability distributions through the use of $wpd_{norm_{s,t}}(A,B)$, these features of the time series are lost and hence there is no need to de-trend or de-seasonalize the data before performing the clustering algorithm. -->


 - Dimension reduction: If each $P_{i, j, k}$ be considered to be a point in the space, key $i$ would have $mp$ dimensions as opposed to $n_i$ dimensions in case of considering raw data. Hence for a large number of observations ($n_i>>mp$), this approach benefits by transitioning to a lower dimension.
 
 - Avoid loss of information due to aggregation: This approach ensures key characteristic information of the data is not lost due to averaging or aggregation measures in an attempt to transition to a lower dimension. Hence, this approach could be thought to somehow balance the drawback of considering raw data or aggregated data.

- Robustness to outliers: This approach could be adapted to be robust to outliers and extreme behaviors by trimming the tails of the probability distributions.

- Non-synchronized observed time periods: Considering probability distribution would imply the clustering process can handle keys that are observed over periods of time that are overlapping but don't necessarily coincide.

- Similar periodic behavior: Since cyclic granularities are considered instead of linear granularities, clustering would group keys that have similar behavior across these cyclic granularities. This implies they will be grouped according to their periodic behavior and not on the linear stretch of time over which they are observed.

The methodology is again applied to the 100 households and some preliminary results are presented. Figure \ref{fig:mds} show the multidimensional scaling of the 100 households with colors representing groups presented by hierarchical clustering method. The clusters have very different size and are far apart with no overlapping. They should be distinct from each other in terms of their periodic patterns. Figure \ref{fig:cyclic-cluster} shows the cyclic presentation of time where it could be viewed in details how these clusters are different with respect to the periodic pattern (day-of-week, hour-of-day). Distribution of energy demand across day-of-week and hour-of-day for three groups clustered through the proposed methodology is shown. For group-1, the morning peak looks very sharp, for group-2 the morning and evening peaks are rounded and spread over few hours, for group-3 peaks are less rounded. They differ in their relationship between 75th and 90th percentile. Also, the 75th and 90th percentile of group 2 are quite close, implying the behavior of this group is more regular.

 <!-- - Handling autocorrelation: Autocorrelation in the time series is likely to get removed from considering probability distributions and cyclic granularities in the clustering algorithm. -->


```{r,warning=FALSE, message=FALSE,eval = FALSE}
library(readr)
library(here)
library(dplyr)
library(tsibble)
library(gravitas)
library(purrr)


set.seed(7777)
all_data <- read_rds(here("../paper-hakear/paper/sim_table/all_data_scalar.rds"))
sm_cust_data <- read_rds(here("../paper-hakear/paper/data/sm_cust_data.rds"))

sm_list <- sm_cust_data %>%
  filter(customer_id == "10006414") %>% 
  create_gran("day_week") %>% 
  tibble() %>% 
  ungroup() %>% 
  group_split(day_week)


p <- seq(0.01, 0.99,0.01)

dist_list <- purrr::map(sm_list, function(x){
    map(p, function(y){
       quantile(x$general_supply_kwh, probs = y)
    }) %>% unlist()
}) %>% bind_rows(.id = "gran_id")
# 
# lag <- dist_list[2,][-1] %>% t() %>% 
#   tibble() %>% 
#   lag() %>% acf()
  
```


```{r mds, warning=FALSE, message=FALSE, fig.cap="Multidimensional scaling of the 100 households is shown along with the groups formed by the hierarchical clustering.", eval = FALSE}
set.seed(7777)
sm_unique_cust <- sm_cust_data %>% distinct(customer_id)
sm_cust_samp <- sm_cust_data %>% 
  #filter(customer_id %in% sm_30) %>% # for demo show 30 customers
  #filter(year(reading_datetime) == 2012) %>% #for demo just show one column
  as_tsibble(index = reading_datetime,
             key = customer_id) %>% 
  create_gran("day_week") %>% 
  create_gran("hour_day")

#all_data <- write_rds(all_data, "paper/sim_table/all_data.rds")

facet_variable = "day_week"
x_variable = "hour_day"

one_harmony_data <- all_data %>%
  #filter(customer_id %in% sm_30) %>% # for demo show 30 customers
  filter(facet_variable == "day_week",
         x_variable == "hour_day") %>% 
  mutate(customer_id = as.character(customer_id))

mds<- one_harmony_data %>%
  select(customer_id, facet_variable, x_variable, wpd_norm) %>% 
  dist() %>%          
  cmdscale() %>%
  as_tibble()

colnames(mds) <- c("Dim.1", "Dim.2")
rownames(mds) <- unique(one_harmony_data$customer_id)
mds <- mds %>%
  mutate(customer_id = as.character(unique(one_harmony_data$customer_id))) %>%
  select((customer_id)
         , Dim.1, Dim.2)

# ggpubr::ggscatter(mds,
#                   x = "Dim.1",
#                   y = "Dim.2", 
#                   label = rownames(mds),
#                   size = 1,
#                   repel = TRUE) 

# clustering as well

d = one_harmony_data %>%
  select(customer_id, facet_variable, x_variable, wpd_norm) %>% 
  dist()

hc = stats::hclust(d,method="complete")
#plot(hc)
groups<-cutree(hc, k=3)

all_data_cluster <- cbind(one_harmony_data, groups) %>% 
  left_join(mds, by = "customer_id") %>% 
  mutate(groups = as.factor(groups))


# q2 <- ggpubr::ggscatter(all_data_cluster,
#                   x = "Dim.1",
#                   y = "Dim.2", 
#                   label = rownames(mds),
#                   color = "groups",
#                   size = 1,
#                   repel = TRUE)

q2 <- all_data_cluster %>% 
  ggplot() +
  geom_point(aes(x = Dim.1, y = Dim.2, color = groups)) +
  scale_color_viridis_d()


 q2
```

```{r linear-cluster}
group1 <- all_data_cluster %>% 
  filter(groups == 1)

group2 <- all_data_cluster %>% 
  filter(groups == 2)

group3 <- all_data_cluster %>% 
  filter(groups == 3)


g1 <- sm_cust_samp %>% 
  filter(customer_id %in% group1$customer_id) %>% 
   ggplot(aes(x=reading_datetime, 
             y= general_supply_kwh), alpha = 0.5) + 
  geom_line() +
  scale_fill_gradient2() +
  facet_wrap(~customer_id, nrow = 1) +
  theme(strip.text.x = element_blank()) +
  theme_void() +
  ylab("group 1")

g2 <- sm_cust_samp %>% 
  filter(customer_id %in% group2$customer_id) %>% 
   ggplot(aes(x=reading_datetime, 
             y= general_supply_kwh), alpha = 0.5) + 
  geom_line() +
  scale_fill_gradient2() +
  facet_wrap(~customer_id, nrow= 1) +
  theme(strip.text.x = element_blank()) +
  theme_void() +
  ylab("group 2")


g3 <- sm_cust_samp %>% 
  filter(customer_id %in% group3$customer_id) %>% 
   ggplot(aes(x=reading_datetime, 
             y= general_supply_kwh), alpha = 0.5) + 
  geom_line() +
  scale_fill_gradient2() +
  theme(strip.text.x = element_blank()) +
  theme_void() +
  ylab("group 3")


# g1/g2 + g3
  
```


```{r cyclic-cluster, warnings = FALSE, message = FALSE, fig.cap = "Distribution of energy demand across day-of-week and hour-of-day for three groups clustered through the proposed methodology. For group-1, the morning peak looks very sharp, for group-2 the morning and evening peaks are rounded and spread over few hours, for group-3 peaks are less rounded. They differ in their relationship between 75th and 90th percentile.", eval = FALSE}
cg1 <- sm_cust_samp %>% 
  filter(customer_id %in% group1$customer_id) %>% 
  prob_plot("day_week", "hour_day", response = "general_supply_kwh",
            plot_type = "quantile", symmetric = TRUE, quantile_prob = c(0.1, 0.25, 0.5, 0.75, 0.9)) +
  scale_color_brewer(palette = "RdPu") + ggtitle("group 1") +
  scale_x_discrete(breaks = seq(0, 23, 5))


cg2 <- sm_cust_samp %>% 
  filter(customer_id %in% group2$customer_id) %>% 
  prob_plot("day_week", "hour_day", response = "general_supply_kwh",
            plot_type = "quantile", symmetric = TRUE, quantile_prob = c(0.1, 0.25, 0.5, 0.75, 0.9)) +
  scale_color_brewer(palette = "RdPu") + ggtitle("group 2") +
  scale_x_discrete(breaks = seq(0, 23, 5))


cg3 <- sm_cust_samp %>% 
  filter(customer_id %in% group3$customer_id) %>% 
  prob_plot("day_week", "hour_day", response = "general_supply_kwh",
            plot_type = "quantile", symmetric = TRUE, quantile_prob = c(0.1, 0.25, 0.5, 0.75, 0.9)) +
  scale_color_brewer(palette = "RdPu") + ggtitle("group 3") +
  scale_x_discrete(breaks = seq(0, 23, 5))

cg1 
cg2
cg3

```


###  Distinction, repeatability, and robustness metrics

We will also determine if any identified clusters or patterns are indeed statistically meaningful in the sense that they actually exist and are not a random allocation. Hence, the robustness of this methodology is tested through simulations. This section will contain the data structure and detailed methodology to be employed for the cluster analysis. The cluster validation indexes like average silhouette width (ASW) is to be employed here to check how homogeneous these clusters are. At this stage, we need to define the aim of clustering as there could be various aims of clustering like between-cluster separation, within cluster homogeneity: low distances, within-cluster homogeneous distributional shape, good representation of data by centroids, little loss of information, high density without cluster gaps, uniform cluster sizes, stability and others. Finally, how distinct they are and how can we summarize the main features of the cluster would be discussed here.


## Combining findings with external data

The robustness of this clustering method is provided through practical explanation of the formed clusters, visualizing how they relate to any weather, socio-economic or geographical conditions. 
<!-- Furthermore, we ensure that a cross-validation algorithm is employed leading to robust clusters.  -->
<!-- We will present how matching other data like geography, census and weather would help us in validating our clustering approach. -->

<!-- ### Geography -->

<!-- We will present how mapping the customers to their respective LGA's and reflect on the clusters obtained in the earlier section. The data is at SA2 and LGA levels. However, some of the LGAs in NSW changed between 2011 and 2016 and hence there would not be a one-to-one correspondence between the LGAs. -->

<!-- ### Census -->

<!-- We will present how mapping the customers to their respective income and population level reflect on the clusters obtained in the earlier section. The [ABS TableBuilder](https://www.abs.gov.au/websitedbs/censushome.nsf/home/tablebuilder) has census data from 2011 and 2016.  -->

<!-- ### Weather -->

<!-- Weather, notably temperature (and humidity) can be the main driver(s) for energy usage.  In NSW many households have electric heaters so their use can impact winter energy use and air-conditioners can impact summer energy use. Relevant weather data could be obtained from the [Bureau of Meteorology](www.bom.gov.au). Some weather stations have 30 minute (sometimes even smaller interval) weather data. Potentially, there could be lag effects of weather on energy usage which should be considered. -->

## Conclusion

This section will cover some drawback of this clustering method and potential extensions of this work.


# R packages

The implementation for our framework for the first paper is available in the R package [`gravitas`](https://github.com/Sayani07/gravitas) and that for the second paper is made available in the package [`hakear`](https://github.com/Sayani07/hakear) for ease of use in other applications.


# Discussion

The final chapter of my thesis will consist of the concluding remarks of the whole thesis and possible future extensions.


```{r data_structure_clust, out.width = "100%", eval=FALSE}
knitr::include_graphics("figures/data_structure_clust.png")
```


<!-- - Study of Australian smart meter data -->

<!-- <!-- Motivation and background --> 
<!-- Electricity smart meter technology is increasingly being deployed in residential and commercial buildings. For example, in the state of Victoria, Australia, it is a state government policy that all households have a smart meter fitted (Victoria State Government, 2015), which has resulted in 97% of the approximately 2.1 million households in Victoria having a smart meter installed in their home. These smart meters collect energy usage information at half-hourly intervals, resulting in over 35 billion half-hourly observations per year across all households in the state. Clustering electricity consumption patterns can enable electricity utilities to develop targeted tariffs for individual groups alleviating the problem of volatility in production by capitalizing on the flexibility of consumers. Clustering households using only smart meter consumption data should provide value in a societal setting by combining the findings and external data like weather conditions, socio-economic or other demographic factors of those households. -->

<!-- <!-- Description of data available --> 
<!-- Data from more than 13,000 household electricity smart meters is obtained as part of the Smart Grid Smart City (SGSC) project (2010-2014). It provides one of the few linked sets of customer time of use (half-hour increments) and demographic data for Australia, as well as detailed information on appliance use, climate, retail and distributor product offers, and other related factors. The trials were based in Newcastle, New South Wales, but also covered areas in Sydney CBD, Newington, Ku-Ring-Gai, and the rural township of Scone. This data set includes electricity use interval readings for each customer for every 30 minutes and demographic data for Australia, as well as detailed information on appliance use, climate, peak events; peak events response and retail and distributor product offers. The load time series is asynchronous as it is observed for these households for unequal time lengths and consists of missing observations. -->


<!-- <!-- Background literature and shortcomings --> 
<!-- The foundation for this study is Tureczek2017-pb, which conducts a systematic review of the current state of the art in smart meter data analytics. The paper evaluates approximately 2100 unique peer-reviewed papers and presents three main findings related to clustering methods, data and cluster validation. None of the 34 selected papers which focus on clustering consumption are based on Australian smart meter data. The clustering is frequently applied directly to the raw data without investigating time series features like auto correlation and periodicity. The application of smart meter data to cluster electricity consumption using K-Means is ubiquitous. But the omission of the time series structure or correlation in the analysis while employing K-Means leads to inefficient clusters. Principal Component Analysis or Self-Organizing Maps removes correlation structures and transitions the data to a reduced feature space, but it comes at a cost of interpretability of the final results. @Tureczek2018-ha has shown that the clever transformation of data before K-Means clustering can improve performance and enable K-Means. However, it does not explain the cluster composition by combining it with external data. -->

<!-- We propose the following for the study of the Australian smart meter data: -->

<!--  - *Preliminary exploratory visualization and summarisation*: Be familiar with an overall view of these 13,000 SGSC households through visualization and provide a detailed layout of data structures (like missing observations) and also the external data that needs to be utilized in the clustering phase. -->


<!--  - *Cluster analysis*: We employ a cluster analysis technique to obtain pockets of households showing similar periodic behavior by grouping probability distributions across a cyclic granularity. This clustering algorithm is adopted to remove or appropriately adjust for auto correlation and unequal length in the data. The method could be further extended by clustering probability distributions conditional on one or more cyclic granularities. Further, we elaborate on the effect of missing data and how it is handled in the clustering process. We will also determine if any identified clusters or patterns are indeed statistically meaningful in the sense that they actually exist and are not a random allocation. We aim to do this by providing a practical explanation of the formed clusters, visualizing how they relate to any weather, socio-economic or geographical conditions. Furthermore, we ensure that a cross-validation algorithm is employed leading to robust clusters.  -->


<!-- NOT USED IN MID_CAN -->

<!-- ## Clustering households with similar periodic behavior -->

<!-- The pool of all customers’ load time series will have different periods of time with overlaps. Any feature based clustering approach would trim the load curves to obtain equal length and synchronized date and time for all curves. These approaches are limited by the type of noisy, patchy, and unequal time-series common in residential data sets (@). In this project, we explore a clustering strategy to alleviate these limitations by looking at the distribution of load time series across different non-linear time granularities that can be readily clustered. Clustering could be employed for distribution of any harmonies (or all non-linear granularities) taken together to form clusters with similar periodic behavior across these harmonies (or all non-linear granularities). @ suggested carrying out cross validation by bootstrapping to form same clusters of homes on randomly selected days within a certain period. The strategy employed here when complimented with some cross validation would be a way to form same clusters of homes who show similar periodic behavior on randomly selected harmonies within a certain period. -->


# Timeline