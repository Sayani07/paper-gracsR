---
title: Clustering probability distributions across bivariate cyclic temporal granularities

# to produce blinded version set to 1
blinded: 0

authors:
- name: Sayani Gupta
  affiliation: Department of Econometrics and Business Statistics, Monash University
  thanks: "Email: Sayani.Gupta@monash.edu"

- name: Rob J Hyndman
  affiliation: Department of Econometrics and Business Statistics, Monash University

- name: Dianne Cook
  affiliation: Department of Econometrics and Business Statistics, Monash University
  
keywords:
- data visualization
- statistical distributions
- time granularities
- calendar algebra
- periodicties
- grammar of graphics
- R

abstract: |
 Clustering elements based on behavior across time granularities 
bibliography: [bibliography.bib]
preamble: >
  \setlength {\marginparwidth }{2cm}
  \usepackage{mathtools,amssymb,booktabs,longtable,todonotes,amsthm}
  \def\mod{~\text{mod}~}
output:
  bookdown::pdf_book:
    base_format: rticles::asa_article
    fig_height: 4
    fig_width: 6
    fig_caption: yes
    dev: "pdf" 
    keep_tex: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache=TRUE, messages=FALSE, warning=FALSE)
# Make sure you have the latest version of rmarkdown and bookdown
#devtools::install_github("rstudio/rmarkdown")
#devtools::install_github("rstudio/bookdown")
library(ggplot2)
library(gravitas)
library(gracsR)
library(ggdendro)
library(dplyr)
```

# Introduction


# Conventional distance matrices


# A new distance measure

<!-- We will use the following notations and symbols. Let -->
<!-- x be a continuous measurement variable which can have one of q -->
<!-- values contained in the set, $X = {x_1; x_2, \dots, x_q}$, where q is the quantile vector correposding to probability vector p. Consider n observations whose measurements of the value -->
<!-- of x are: $A = {a_1, a_2, \dots, a_n}$ where $a_j \in X$ . The distribution of of the set A along measurement x is H(x; A) which is an -->
<!-- ordered list (or b-dimensional vector) consisting of the -->
<!-- number of occurrences of the discrete values of x among -->
<!-- the ai. As we are interestedonly in comparing the histograms of the same measurement x, H(A) will be used -->
<!-- in place of H(x; A) without loss of generality. If Hi(A), -->

Consider two cyclic granularities $A$ and $B$, such that $A = \{ a_j: j = 1, 2, \dots, J\}$ and $B = \{ b_k: k = 1, 2, \dots, K\}$ with $A$ placed across x-axis and $B$ across facets. Let $v = \{v_t: t = 0, 1, 2, \dots, T-1\}$ be a continuous variable observed across $T$ time points. Let $wpd_{norm_{s,t}}(A,B)$ be the normalised pairwise distance between subjects/elements $s$ and $t$ for the harmony pair $(A, B)$.


, then we can define $S_{i,j} = \sum_{c_1} \sum_{c_2} D_{i,j}(c_1,c_2)$.




The proofs are attempted assuming sum is taken. Can we take maximum or minumum distances? That would finally imply we want to cluster households those are having similar extreme behavior, as we would be minimising the maximum distances through clustering.

\noindent Now since JS is a metric, non-negativity, reflexivity and triangle inequality holds for it. Thus,   
-  $D_{x, x} \geq 0$ with equality only if $x=y$  
-  $D(x, y) = D(y, x)$  
-  $D(x, y) + D(x, z) \geq D(y, z)$  

## Metric property

### Non-negative property

$S_{i,j}$ is nothing but the sum of $D(i,j)$ for all levels of $c_1$ and $c_2$ and each $D(i,j)$ has non-negativity property. Therefore, $S_{i,j}$ also has the non-negativity by definition.

### Reflexivity property

Since $D(i,i)$ = 0 is true, $\sum_{i} D(i, i) = 0$. Due to the non-negativity, 0 is the minimum bound of the measure: $S_{i,j} \geq 0$. Therefore, $S(i, i) = \sum_{i} D(i, i)= 0$.

### Commutative property

To prove: $S(x, y) = S(y, x)$  

### Triangle inequality property

Since, $D(i, j) + D(j, k) \geq D(i, k)$  
Therefore, $\sum_{c_1} \sum_{c_2}D(i, j) + \sum_{c_1} \sum_{c_2}D(i, j) \geq \sum_{c_1} \sum_{c_2}D(i, k)$   
or, $S(i, j) + S(j, k) \geq S(i, k)$  




### 

## Normalization

## Comparison with other distances


# Algorithm

## Simulations

```{r set_common_input}
.data = smart_meter10
gran1 = "day_week"
gran2 = "hour_day"
response = "general_supply_kwh"
clust_var = "customer_id"
```

### 2 subjects 5 simulations

```{r ncust2_niter5}
conditional_dendro(.data, gran1,
                       gran2, response,
                       clust_var, niter = 5,
                       ncust = 2)
```

### 5 subjects 5 simulations

```{r ncust5_niter5}
conditional_dendro(.data, gran1,
                       gran2, response,
                       clust_var, niter = 5,
                       ncust = 5)
```

### 10 subjects 5 simulations

```{r ncust10_niter5}
conditional_dendro(.data, gran1,
                       gran2, response,
                       clust_var, niter = 5,
                       ncust = 10)
```

### 10 subjects only

```{r}
conditional_dendro(.data, gran1,
                       gran2, response,
                       clust_var, niter = 1,
                       ncust = 10)
```



# Visualization of clusters

## dendogram and heat maps
## Multidimensional scaling

Let’s say we were given only the distances between objects (i.e. their similarities) — and not their locations? You could still create a map — but it would involve a fair amount of geometry, and some logical deductions. Kruskal & Wish (1978) — the authors of one of the first multidimensional scaling books — state that this type of logic problem is ideal for multidimensional scaling. You’re basically given a set of differences, and the goal is to create a map that will also tell you what the original distances where and where they were located.


https://www.statisticshowto.com/multidimensional-scaling/

- multidimensional scaling  and how points in mds space maps to points in your original data space using gravitas 
- tourr, cmds, prcomp  
- read about ggobi, tsne (topology over geometry)  
- intuition - elliptical and spherical spheres only when we have distinct clusters in model based   clustering  
- Wald's linkage, single linkage and complete linkage
Read ggobi and Di's machine learning course to get intuition  
- heat map and log scale on the color to have a distinction and map it to the dendogram  


<!-- #### -- start from paper-hakear -->

<!-- ## Smart meter data of Australia {#sec:smartmeter} -->

<!-- # how data looks -->

Clustering of electricity customers based on their consumption behavior facilitate effective market segmentation and hence better management and pricing.
Electricity demand time series are the primary source of information of customers' consumption behavior.
Due to technological developments, smart meters could provide large quantities of measurements on energy usage at more and more finer time intervals. Due to this extreme dimensionality along time of the demand data, it is a challenge to have clustering methods which could reduce the dimensionality to produce distinct clusters. One of the potential solutions is time series feature extraction. However, it is limited by the type of noisy, patchy and unequal time series that is very common in data sets of any residential customers. The idea is to choose similarity measures, which do not lose the core characteristic information of demand across different time deconstructions in aggregation processes and is robust against asynchronous or incomplete time series.

We use the data from one of the customer trials [@smart-meter] conducted as part of the Smart Grid Smart City project in Newcastle, New South Wales and some parts of Sydney. The data has customer wise data on energy consumption for every half hour from February 2012 to March 2014. <!--It would be interesting to explore the energy consumption distribution for these customers and gain insights on their energy behavior which are lost either due to aggregation or looking only at coarser temporal units.--> The idea here is to
use our similarity measure and show improved cluster distinction as compared to baseline feature based approaches. A data set of 100 homes is used to evaluate both approaches. This is different to deterministic segmentation which attempts to characterise load as a function of a fixed number of parameters, such as occupancy or types of appliances, and use those parameters for classification. We only use their load patterns across time deconstructions to define the similarity measure.

<!-- Write about common load clustering methods -->


### Multidimensional scaling and cluster analysis

Cluster analysis and multidimensional scaling (MDS) methods are used to explore the temporal structure of various households, their interrelations and determine similar behaving clusters of households in this study.
MDS methods are used to produce a lower dimensional display space where each household is represented by a point and the distance between these points give the measure of similarity between these households. On the other hand, clustering techniques are used to provide information about the clustering structure of the households. Households that are in the same cluster can be considered to be more similar, while households are more dissimilar in their energy behavior across one or few temporal deconstructions.

$wpd_norm$ is considered as the measure of similarity or dissimilarity while using MDS and clustering techniques. $wpd_norm$ could be constructed for each pair of harmony for each household and then similarity could be measured across single or all harmonies together. 


```{r mds}
set.seed(7777)
library(gravitas)
library(tsibble)
library(lubridate)
library(readr)

all_data <- read_rds(here("paper/sim_table/all_data.rds"))
sm_cust_data <- read_rds(here("paper/data/sm_cust_data.rds")) %>% 
  as_tsibble(index = reading_datetime,
             key = customer_id)

sm_unique_cust <- sm_cust_data %>% distinct(customer_id)

#sm_30 <- sample(sm_unique_cust$customer_id, 30)
sm_cust_samp <- sm_cust_data %>% 
  #filter(customer_id %in% sm_30) %>% # for demo show 30 customers
  filter(year(reading_datetime) == 2012)  %>% 
  create_gran("day_week") %>% 
  create_gran("hour_day")

#all_data <- write_rds(all_data, "paper/sim_table/all_data.rds")

facet_variable = "day_week"
x_variable = "hour_day"

one_harmony_data <- all_data %>%
  #filter(customer_id %in% sm_30) %>% # for demo show 30 customers
  filter(facet_variable == "day_week",
                    x_variable == "hour_day") %>% 
  mutate(customer_id = as.character(customer_id))

mds<- one_harmony_data %>%
  select(customer_id, facet_variable, x_variable, wpd_norm) %>% 
  dist() %>%          
  cmdscale() %>%
  as_tibble()

colnames(mds) <- c("Dim.1", "Dim.2")
rownames(mds) <- unique(one_harmony_data$customer_id)
mds <- mds %>%
  mutate(customer_id = as.character(unique(one_harmony_data$customer_id))) %>%
  select((customer_id)
         , Dim.1, Dim.2)
```


```{r clustering}
# K-means clustering
clust <- kmeans(mds, 3)$cluster %>%
  as.factor()
mds <- mds %>%
  mutate(groups = clust)


# Plot and color by groups
q1 <- ggpubr::ggscatter(mds, x = "Dim.1", y = "Dim.2", 
          label = rownames(mds),
          color = "groups",
          palette = "jco",
          size = 1, 
          ellipse = TRUE,
          ellipse.type = "convex",
          repel = TRUE)

```

- check transformation and scalar approaches are same
  
```{r scalar-approach}
set.seed(7777)
all_data <- read_rds(here("paper/sim_table/all_data_scalar.rds"))
sm_cust_data <- read_rds(here("paper/data/sm_cust_data.rds"))
set.seed(7777)
sm_unique_cust <- sm_cust_data %>% distinct(customer_id)
sm_cust_samp <- sm_cust_data %>% 
  #filter(customer_id %in% sm_30) %>% # for demo show 30 customers
  #filter(year(reading_datetime) == 2012) %>% #for demo just show one column
  as_tsibble(index = reading_datetime,
             key = customer_id) %>% 
  create_gran("day_week") %>% 
  create_gran("hour_day")

#all_data <- write_rds(all_data, "paper/sim_table/all_data.rds")

facet_variable = "day_week"
x_variable = "hour_day"

one_harmony_data <- all_data %>%
  #filter(customer_id %in% sm_30) %>% # for demo show 30 customers
  filter(facet_variable == "day_week",
         x_variable == "hour_day") %>% 
  mutate(customer_id = as.character(customer_id))

mds<- one_harmony_data %>%
  select(customer_id, facet_variable, x_variable, wpd_norm) %>% 
  dist() %>%          
  cmdscale() %>%
  as_tibble()

colnames(mds) <- c("Dim.1", "Dim.2")
rownames(mds) <- unique(one_harmony_data$customer_id)
mds <- mds %>%
  mutate(customer_id = as.character(unique(one_harmony_data$customer_id))) %>%
  select((customer_id)
         , Dim.1, Dim.2)

ggpubr::ggscatter(mds,
                  x = "Dim.1",
                  y = "Dim.2", 
                  label = rownames(mds),
                  size = 1,
                  repel = TRUE) 

# clustering as well

d = one_harmony_data %>%
  select(customer_id, facet_variable, x_variable, wpd_norm) %>% 
  dist()

hc = stats::hclust(d,method="complete")
#plot(hc)
groups<-cutree(hc, k=3)

all_data_cluster <- cbind(one_harmony_data, groups) %>% 
  left_join(mds, by = "customer_id") %>% 
  mutate(groups = as.factor(groups))



q2 <- ggpubr::ggscatter(all_data_cluster,
                  x = "Dim.1",
                  y = "Dim.2", 
                  label = rownames(mds),
                  color = "groups",
                  size = 1,
                  repel = TRUE) 


q1 + q2
```
  
  
  
10018272 is a customer from cluster 3, for which hour_day/wknd-wday comes after wknd-wday/day-month as opposed to 10006414 (cluster 1) for whom the direction is reversed.

```{r, echo = FALSE}

10006414


all_data %>% 
  filter(customer_id ==  10006414) %>% 
  arrange(-wpd_norm)

sm_cust_data %>% 
  filter(customer_id ==  10006414) %>% 
  prob_plot("hour_day",
            "wknd_wday",
            plot_type = "quantile",
            quantile_prob = c(0.5),
            symmetric = FALSE,
            response = "general_supply_kwh")


sm_cust_data %>% 
  filter(customer_id ==  10006414) %>% 
  prob_plot("hour_day",
            "day_week",
            plot_type = "quantile",
            quantile_prob = c(0.5),
            symmetric = FALSE,
            response = "general_supply_kwh")




all_data %>% 
  filter(customer_id ==  10007340) %>% 
  arrange(-wpd_norm)

sm_cust_data %>% 
  filter(customer_id ==  10007340) %>% 
  prob_plot("day_month",
            "day_week",
            plot_type = "quantile",
            quantile_prob = c(0.5),
            symmetric = FALSE,
            response = "general_supply_kwh")


sm_cust_data %>% 
  filter(customer_id ==  10007340) %>% 
  prob_plot("hour_day",
            "day_month",
            plot_type = "quantile",
            quantile_prob = c(0.5),
            symmetric = FALSE,
            response = "general_supply_kwh")


all_data %>% 
  filter(customer_id ==  10018272) %>% 
  arrange(-wpd_norm)


```

  
  
```{r}
all_data_scalar <- read_rds(here("paper/sim_table/all_data_scalar.rds"))
all_data <- read_rds(here("paper/sim_table/all_data.rds"))

merged_data <- left_join(all_data_scalar,all_data, by = c("customer_id",
                                                          "facet_variable",
                                                          "x_variable")) %>% 
  rename("wpd_scalar"  = "wpd_norm.x",
         "wpd_perm" = "wpd_norm.y")

merged_data %>% 
  mutate(pair = paste(facet_variable, x_variable, sep = "-")) %>% 
  ggplot() +
  geom_point(aes(x = wpd_scalar, y = wpd_perm, color = pair, group = pair))
  #geom_point(aes(x = wpd_scalar, y = wpd_perm, alpha = 0.05))

```
  

```{r linear}
raw_data <- sm_cust_data %>% 
  left_join(all_data_cluster, by = "customer_id") 

p1 <- raw_data %>% 
  filter(groups==1) %>% 
  ggplot() + 
  geom_line(aes(x = reading_datetime,
            y = general_supply_kwh)
  ) + ggtitle("Group 1")


p2 <- raw_data %>% 
  filter(groups==2) %>% 
  ggplot() + 
  geom_line(aes(x = reading_datetime,
            y = general_supply_kwh)
  ) + ggtitle("Group 2")

p3 <- raw_data %>% 
  filter(groups==3) %>% 
  ggplot() + 
  geom_line(aes(x = reading_datetime,
            y = general_supply_kwh)
  ) + ggtitle("Group 3")
library(patchwork)
p1/p2/p3

```


```{r gravitas}
raw_data <- sm_cust_data %>% 
  left_join(all_data_cluster, by = "customer_id") 

p1 <- raw_data %>% 
  filter(groups==1) %>% 
  prob_plot("day_week",
            "hour_day",
            response = "general_supply_kwh",
            plot_type = "quantile",
            symmetric = FALSE
)  + ggtitle("Group 1")

p2 <- raw_data %>% 
  filter(groups==2) %>% 
  prob_plot("day_week",
            "hour_day",
            response = "general_supply_kwh",
            plot_type = "quantile",
            symmetric = FALSE
)  + ggtitle("Group 2")

p3 <- raw_data %>% 
  filter(groups==3) %>% 
  prob_plot("day_week",
            "hour_day",
            response = "general_supply_kwh",
            plot_type = "quantile",
            symmetric = FALSE
)  + ggtitle("Group 3")


library(patchwork)
p1
p2
p3

```



```{r hour-day-day-month, eval = FALSE}

set.seed(7777)
library(gravitas)
library(tsibble)
library(lubridate)
library(readr)

all_data <- read_rds("sim_table/all_data.rds")
sm_cust_data <- read_rds("data/sm_cust_data.rds")
sm_unique_cust <- sm_cust_data %>% distinct(customer_id)

#sm_30 <- sample(sm_unique_cust$customer_id, 30)
sm_cust_samp <- sm_cust_data %>% 
  #filter(customer_id %in% sm_30) %>% # for demo show 30 customers
  filter(year(reading_datetime) == 2012) %>% #for demo just show one column
  as_tsibble(index = reading_datetime,
             key = customer_id) %>% 
  create_gran("wknd_wday") %>% 
  create_gran("week_month")

#all_data <- write_rds(all_data, "paper/sim_table/all_data.rds")

facet_variable = "wknd_wday"
x_variable = "week_month"

one_harmony_data <- all_data %>%
  #filter(customer_id %in% sm_30) %>% # for demo show 30 customers
  filter(facet_variable == "wknd_wday",
                    x_variable == "week_month") %>% 
  mutate(customer_id = as.character(customer_id))

mds<- one_harmony_data %>%
  select(customer_id, facet_variable, x_variable, wpd_norm) %>% 
  dist() %>%          
  cmdscale() %>%
  as_tibble()

colnames(mds) <- c("Dim.1", "Dim.2")
rownames(mds) <- unique(one_harmony_data$customer_id)
mds <- mds %>%
  mutate(customer_id = as.character(unique(one_harmony_data$customer_id))) %>%
  select((customer_id)
         , Dim.1, Dim.2)


clust <- kmeans(mds, 3)$cluster %>%
  as.factor()
mds <- mds %>%
  mutate(groups = clust)


# Plot and color by groups
ggpubr::ggscatter(mds, x = "Dim.1", y = "Dim.2", 
          label = rownames(mds),
          color = "groups",
          palette = "jco",
          size = 1, 
          ellipse = TRUE,
          ellipse.type = "convex",
          repel = TRUE)

```


### Advantage over taditional feature based clustering methods

Since probability distribution of the load is considered across different cyclic granularity, there is no discarding of valuable data that might result from bringing all customers to the same time horizon.
Moreover, since our metric is based on probability distribution, it will not be sensitive to outlier days and hence the clustering process will not be biased.
Moreover, feature extraction applies to all days for all customers at once and therefore does not support parallel computing. Our metric is computed for each household separately and can even run for each each harmonies separately and thus fits well into the parallel computing framework.


### Write the method in notations

-  notation of time series for each customer
-  wpd
-  harmonies
-  cluster


### Distinction, repeatability, and robustness metrics



### Putting similar households on linear scale 


<!-- ### end from paper-hakear -->






# How robust are your clusters 

bootstrap and other resampling techniques

# Application
## Smart meter data

## Cricket data

# Conclusion


